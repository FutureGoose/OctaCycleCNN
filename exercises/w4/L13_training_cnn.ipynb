{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to mean 0, std 1\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "training_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding='same')  \n",
    "        self.pool = nn.MaxPool2d(2, 2)                                    \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding='same') \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128) \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.flatten = nn.Flatten()                                     \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))        # Output: 32 x 28 x 28\n",
    "        x = self.pool(x)                    # Output: 32 x 14 x 14\n",
    "        x = self.relu(self.conv2(x))        # Output: 64 x 14 x 14\n",
    "        x = self.pool(x)                    # Output: 64 x 7 x 7\n",
    "        x = self.flatten(x)                 # Output: 64 * 7 * 7 = 3136\n",
    "        x = self.relu(self.fc1(x))          # Fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)                     # notera att vi faktiskt EJ behöver explicit ange softmax aktivering här på output layer\n",
    "                                            # anledningen är att vi i nästa skede använder CrossEntropyLoss, och den har faktiskt\n",
    "                                            # softmax-aktivering inbakad i sig. Med andra ord sköter CrossEntropyLoss den sista aktiveringen åt oss.\n",
    "                                                                                  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model = model.to(device)  # Move to device\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "\n",
    "model = CNN()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "batch_train_losses = []\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_features, y_true = batch\n",
    "        input_features, y_true = input_features.to(device), y_true.to(device)\n",
    "        \n",
    "        y_pred = model(input_features)  \n",
    "        loss = loss_function(y_pred, y_true)  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        batch_train_losses.append(batch_loss)\n",
    "    \n",
    "    \n",
    "    epoch_average_loss = np.average(batch_train_losses[-len(train_dataloader):])\n",
    "    epoch_train_losses.append(epoch_average_loss)\n",
    "    print(f\"Training loss: {epoch_average_loss}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # Evaluation section\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    with torch.no_grad(): \n",
    "      \n",
    "        x_test, y_test = next(iter(test_dataloader))\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "        \n",
    "        y_pred = model(x_test)\n",
    "        test_loss = loss_function(y_pred, y_test).item()\n",
    "        epoch_test_losses.append(test_loss)\n",
    "        print(f\"Test loss: {test_loss}\")\n",
    "        \n",
    "        # Evaluation accuracy\n",
    "        _, predicted_class = torch.max(y_pred, dim=1)\n",
    "        accuracy = (predicted_class == y_test).float().mean().item()\n",
    "        epoch_test_accuracies.append(accuracy)\n",
    "        print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(epoch_train_losses[:-2], label='Train Loss')\n",
    "plt.plot(epoch_test_losses[:-2], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Plot test accuracies\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(epoch_test_accuracies[:-2], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Uppgifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)**\n",
    "\n",
    "Kolla CNN-modellen vi definierade. Förstår du input och output size i varje lager?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)**\n",
    "\n",
    "Träna nätverket med loopen ovan.\n",
    "\n",
    "Det är ett relativt litet nätverk, särskilt fully conntected delen, om man jämför med vad vi använde när vi tränade på detta dataset förra lektionen.\n",
    "Är det lätt att få bättre prestanda nu jämfört med när vi använde exklusivt fully connected layers, trots att vi har en relativt litet CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)**\n",
    "\n",
    "Hur många parametrar har nätverket totalt? Hur många per lager?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)**\n",
    "\n",
    "Experimentera själv med att ändra i arkitekturen och träna om! Kan du nå bättre resultat på test-settet?\n",
    "\n",
    "Tips: Kan också vara en bra idé att implementera confusion matrix här för att evaluera prestanda per klass också, snarare än bara se till total accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
