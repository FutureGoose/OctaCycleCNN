{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduktion\n",
    "\n",
    " \n",
    "Inom AI-utveckling handlar allt i grund och botten om att hitta minsta värden för vissa funktioner. \n",
    "\n",
    "För det ändamålet gick vi under senaste lektionen igenom den metod som används, för i princip all moden AI-utveckling, för att göra detta: **Gradient Descent**.\n",
    "\n",
    "Syftet med följande uppgifter är att vi ska få en lite bättre känsla för hur Gradient Descent fungerar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vad är Gradient Descent?**\n",
    "\n",
    "Gradient Descent är en optimeringsmetod som används för att minimera en funktion genom att iterativt justera parametrarna. \n",
    "\n",
    "Den grundläggande idén är att vi vid varje iteration beräknar gradienten (derivatan) av funktionen, som visar oss vilken riktning vi ska röra vår oberoende variabel i, för att minska funktionsvärdet. \n",
    "\n",
    "I funktionen ovan är x vår oberoende variablen, och Vi uppdaterar vårt x-värde enligt följande formel:\n",
    "\n",
    "$$\n",
    "x_{\\text{ny}} = x_{\\text{gammal}} - \\text{learning rate} \\times \\text{gradient}\n",
    "$$\n",
    "Genom att justera learning rate kan vi styra hur stora steg vi tar mot minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi definierar först här en funktion som vi vill hitta minimum till. Specifikt vill vi hitta det x-värde som ger funktionens minsta värde. Vi definierar även funktionens derivata - den behövs för Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 - 4*x + 4\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 2*x - 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisering av Funktionen\n",
    "\n",
    "Innan vi kör Gradient Descent, låt oss visualisera funktionen *f* för att se var minimum ligger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function\n",
    "x_values = np.linspace(-6, 10, 100)\n",
    "y_values = f(x_values)\n",
    "\n",
    "plt.plot(x_values, y_values, label='f(x) = x^2 - 4x + 4')\n",
    "plt.axhline(0, color='black', lw=1)\n",
    "plt.axvline(0, color='black', lw=1)\n",
    "plt.axvline(2, color='red', lw=0.5, ls='--', label='Minimum at x=2')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser här direkt via plotten att funktionen en har ett minimum vid x=2. Nu ska vi försöka hitta detta minimum automatiskt med hjälp av Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Låt oss nu definiera funktionen som utför Gradient Descent, vilket interativt hittar oss fram till det x-värde som ger oss minsta funktionsvärdet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_derivative, initial_x, learning_rate=0.1, iterations=1000):\n",
    "\n",
    "    x = initial_x   # startvärdet av x\n",
    "    x_values = []   # lista att samla alla värden av x\n",
    "\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        x_values.append(x)\n",
    "        \n",
    "        gradient = f_derivative(x)\n",
    "        \n",
    "        x = x - learning_rate * gradient\n",
    "\n",
    "    # ------------ notera att följande endast är kod för som hanterar plotten, ej nödvändig för själva gradient descent ------------\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        x_plot = np.linspace(np.abs(initial_x)+1, -np.abs(initial_x)-1, 100)\n",
    "        plt.plot(x_plot, f(x_plot), label='f(x) = x^2 - 4x + 4', color='blue')\n",
    "        plt.axhline(0, color='black', lw=1)\n",
    "        plt.axvline(0, color='black', lw=1)\n",
    "        plt.axvline(2, color='red', lw=0.5, ls='--', label='Minimum at x=2')\n",
    "\n",
    "        # plot all previous x values\n",
    "        plt.scatter(x_values, f(np.array(x_values)), color='orange', label='Previous x values')\n",
    "        # Plot the current x value\n",
    "        plt.scatter(x, f(x), color='green', label='Current x value', s=100)\n",
    "        \n",
    "        plt.title(f'Gradient Descent Iteration {i + 1}')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('f(x)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    # ------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        if abs(gradient) < 0.005:   # bryt om ändringen i gradienten är mindre än ett visst tröskelvärde\n",
    "            break\n",
    "        \n",
    "        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}, gradient = {gradient}\")\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa att köra funktionen *gradient_descent()* med olika värden på på *initial_x*. Börja med x=6 och testa sedan fler värden, både positiva och negativa.\n",
    "\n",
    "Vad händer? Varför?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(f, f_derivative, initial_x=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**\n",
    "\n",
    "Vi kan ge funktionen ytterligare en input, learning rate. I klassen gick vi igenom att learning rate är \"hur mycket\" vi backar i gradientens riktning (dvs, hur snabbt vi närmare oss det optimala värdet av x) vid varje iteration. \n",
    "\n",
    "Välj återigen x=6 som startgissning, och testa därefter att köra igenom funktionen för följande värden på learning rate: \n",
    "\n",
    "[0.1, 0.01, 0,001]\n",
    "\n",
    "Vad händer? Hur många iterationen krävs i varje fall för att slutföra funktionen?\n",
    "\n",
    "Tips: du kan manuellt avbryta funktionen om det tar för lång tid...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(f, f_derivative, learning_rate=0.001, initial_x=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3**\n",
    "\n",
    "\n",
    "Vi har i uppgiften ovan testat vad som händer om man har relativt små värden på learning rate. Nu ska vi testa motsatsen, dvs lite större värden på learning rate.\n",
    "\n",
    "Kör funktionen igen, men x=6 som startgissning, men denna gång testa följande learning rates:\n",
    "\n",
    "[0.8, 1, 1.2]\n",
    "\n",
    "Vad händer? \n",
    "\n",
    "Återigen, du kan manuellt abryta funktionen om det tar för lång tid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent(f, f_derivative, learning_rate=0.8, initial_x=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**\n",
    "\n",
    "Testa runt för fler värden på learning rate. \n",
    "\n",
    "Blir det som du förväntar dig?\n",
    "\n",
    "Vad kan du dra för slutsatser av problem 1-3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**\n",
    "\n",
    "Plotta nu funktionen $f(x) = x^4 - 6x^2 + 4x + 12$. \n",
    "\n",
    "Hur ser den ut, generellt?\n",
    "\n",
    "Vad tror du händer om vi hade försökt köra Gradient Descent på funktionen du plottade ovan? Vad förutser du?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**\n",
    "\n",
    "Anta att vi nu har\n",
    "\n",
    "$f(x) = sin(2x) + x$ \n",
    "\n",
    " - Kolla upp derivatan $f'(x)$ av denna funktion online (ex via wolframalpha.com) och skriv ner den. \n",
    " - Skriv därefter ned iterationsformeln för x i gradient descent, med hjälp av derivatan.\n",
    "\n",
    "Vad händer om vi skulle kört gradient descent på denna funktion? Förklara."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**\n",
    "\n",
    "Ett problem för gradient descent ärdet s.k. problemet med globala- och lokala minimum. \n",
    "\n",
    "Hur stort problem är detta? Sök efter referenser på nätet (ej ChatGPT) och förklara med egna ord vad detta innebär, hur det påverkar gradient gescent och hur man kan försöka kringgå det."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
