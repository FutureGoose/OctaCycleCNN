{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in PyTorch\n",
    "\n",
    "This lecture will walk through the steps for fine-tuning a pre-trained CNN (e.g., AlexNet) in PyTorch. We'll explore:\n",
    "\n",
    "1. **Loading a pre-trained model**  \n",
    "2. **Inspecting trainable parameters**  \n",
    "3. **Adjusting layers: freezing and unfreezing**  \n",
    "4. **Replacing the output (classification) layer**  \n",
    "5. **Replacing the entire (classification) head**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading a pre-trained model**\n",
    "\n",
    "PyTorch makes it easy to load pre-trained models from `torchvision.models`. \n",
    "\n",
    "Let's load AlexNet, a CNN that was one of the early deep learning models to perform well on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustaf/projects/deeplearning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gustaf/projects/deeplearning/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /home/gustaf/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Load AlexNet with pre-trained weights\n",
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AlexNet architecture consists of convolutional layers (for feature extraction) followed by a fully connected (FC) classification head. \n",
    "\n",
    "The FC layer outputs predictions for 1,000 ImageNet classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# note that all layers are shown in order below. The first layers are printed first, and the last ones last.\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Inspecting trainable parameters**\n",
    "\n",
    "In PyTorch, each layer has an attribute `.requires_grad` that determines whether the parameters of that layer are updated during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: True | Parameter name: features.0.weight\n",
      "Requires grad: True | Parameter name: features.0.bias\n",
      "Requires grad: True | Parameter name: features.3.weight\n",
      "Requires grad: True | Parameter name: features.3.bias\n",
      "Requires grad: True | Parameter name: features.6.weight\n",
      "Requires grad: True | Parameter name: features.6.bias\n",
      "Requires grad: True | Parameter name: features.8.weight\n",
      "Requires grad: True | Parameter name: features.8.bias\n",
      "Requires grad: True | Parameter name: features.10.weight\n",
      "Requires grad: True | Parameter name: features.10.bias\n",
      "Requires grad: True | Parameter name: classifier.1.weight\n",
      "Requires grad: True | Parameter name: classifier.1.bias\n",
      "Requires grad: True | Parameter name: classifier.4.weight\n",
      "Requires grad: True | Parameter name: classifier.4.bias\n",
      "Requires grad: True | Parameter name: classifier.6.weight\n",
      "Requires grad: True | Parameter name: classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Requires grad: {param.requires_grad} | Parameter name: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to **inspect the trainable parameters** and calculate:\n",
    "- The total number of parameters.\n",
    "- The number and percentage of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 61,100,840\n",
      "Trainable Parameters: 61,100,840\n",
      "Percentage Trainable: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Percentage Trainable: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Trainable\n",
      "==============================================================================================================================================================================================\n",
      "AlexNet                                  [1, 3, 224, 224]          [1, 1000]                 --                             --                   --                        True\n",
      "├─Sequential: 1-1                        [1, 3, 224, 224]          [1, 256, 6, 6]            --                             --                   --                        True\n",
      "│    └─Conv2d: 2-1                       [1, 3, 224, 224]          [1, 64, 55, 55]           23,296                      0.04%                   [11, 11]                  True\n",
      "│    └─ReLU: 2-2                         [1, 64, 55, 55]           [1, 64, 55, 55]           --                             --                   --                        --\n",
      "│    └─MaxPool2d: 2-3                    [1, 64, 55, 55]           [1, 64, 27, 27]           --                             --                   3                         --\n",
      "│    └─Conv2d: 2-4                       [1, 64, 27, 27]           [1, 192, 27, 27]          307,392                     0.50%                   [5, 5]                    True\n",
      "│    └─ReLU: 2-5                         [1, 192, 27, 27]          [1, 192, 27, 27]          --                             --                   --                        --\n",
      "│    └─MaxPool2d: 2-6                    [1, 192, 27, 27]          [1, 192, 13, 13]          --                             --                   3                         --\n",
      "│    └─Conv2d: 2-7                       [1, 192, 13, 13]          [1, 384, 13, 13]          663,936                     1.09%                   [3, 3]                    True\n",
      "│    └─ReLU: 2-8                         [1, 384, 13, 13]          [1, 384, 13, 13]          --                             --                   --                        --\n",
      "│    └─Conv2d: 2-9                       [1, 384, 13, 13]          [1, 256, 13, 13]          884,992                     1.45%                   [3, 3]                    True\n",
      "│    └─ReLU: 2-10                        [1, 256, 13, 13]          [1, 256, 13, 13]          --                             --                   --                        --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 13, 13]          [1, 256, 13, 13]          590,080                     0.97%                   [3, 3]                    True\n",
      "│    └─ReLU: 2-12                        [1, 256, 13, 13]          [1, 256, 13, 13]          --                             --                   --                        --\n",
      "│    └─MaxPool2d: 2-13                   [1, 256, 13, 13]          [1, 256, 6, 6]            --                             --                   3                         --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            [1, 256, 6, 6]            --                             --                   --                        --\n",
      "├─Sequential: 1-3                        [1, 9216]                 [1, 1000]                 --                             --                   --                        True\n",
      "│    └─Dropout: 2-14                     [1, 9216]                 [1, 9216]                 --                             --                   --                        --\n",
      "│    └─Linear: 2-15                      [1, 9216]                 [1, 4096]                 37,752,832                 61.79%                   --                        True\n",
      "│    └─ReLU: 2-16                        [1, 4096]                 [1, 4096]                 --                             --                   --                        --\n",
      "│    └─Dropout: 2-17                     [1, 4096]                 [1, 4096]                 --                             --                   --                        --\n",
      "│    └─Linear: 2-18                      [1, 4096]                 [1, 4096]                 16,781,312                 27.46%                   --                        True\n",
      "│    └─ReLU: 2-19                        [1, 4096]                 [1, 4096]                 --                             --                   --                        --\n",
      "│    └─Linear: 2-20                      [1, 4096]                 [1, 1000]                 4,097,000                   6.71%                   --                        True\n",
      "==============================================================================================================================================================================================\n",
      "Total params: 61,100,840\n",
      "Trainable params: 61,100,840\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 714.68\n",
      "==============================================================================================================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 3.95\n",
      "Params size (MB): 244.40\n",
      "Estimated Total Size (MB): 248.96\n",
      "==============================================================================================================================================================================================\n",
      "Percentage Trainable: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# own version\n",
    "import torchinfo\n",
    "print(str(torchinfo.summary(model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"trainable\",])))\n",
    "print(f\"Percentage Trainable: {100 * sum(p.numel() for p in model.parameters() if p.requires_grad) / sum(p.numel() for p in model.parameters()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, all parameters are initially trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Adjusting layers, freezing and unfreezing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **freeze a layer**, set its `.requires_grad` attribute to `False`. This prevents the optimizer from updating the layer's weights during training. Freezing is typically applied to lower layers (convolutional layers), which extract generic features, while leaving the classification head trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's freeze all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 61,100,840\n",
      "Trainable Parameters: 0\n",
      "Percentage Trainable: 0.00%\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():      # loops over all layers\n",
    "    param.requires_grad = False                   # freezes the actual layer\n",
    "\n",
    "# Check trainable parameters again\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False | Parameter name: features.0.weight\n",
      "Requires grad: False | Parameter name: features.0.bias\n",
      "Requires grad: False | Parameter name: features.3.weight\n",
      "Requires grad: False | Parameter name: features.3.bias\n",
      "Requires grad: False | Parameter name: features.6.weight\n",
      "Requires grad: False | Parameter name: features.6.bias\n",
      "Requires grad: False | Parameter name: features.8.weight\n",
      "Requires grad: False | Parameter name: features.8.bias\n",
      "Requires grad: False | Parameter name: features.10.weight\n",
      "Requires grad: False | Parameter name: features.10.bias\n",
      "Requires grad: False | Parameter name: classifier.1.weight\n",
      "Requires grad: False | Parameter name: classifier.1.bias\n",
      "Requires grad: False | Parameter name: classifier.4.weight\n",
      "Requires grad: False | Parameter name: classifier.4.bias\n",
      "Requires grad: False | Parameter name: classifier.6.weight\n",
      "Requires grad: False | Parameter name: classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# double check requires grad status of layers\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'Requires grad: {param.requires_grad} | Parameter name: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unfreezing Specific Layers**\n",
    "\n",
    "If your task requires fine-tuning certain layers, you can selectively unfreeze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 61,100,840\n",
      "Trainable Parameters: 4,097,000\n",
      "Percentage Trainable: 6.71%\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze only the last classifier layer (the output layer) \n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier.6\" in name:                \n",
    "        param.requires_grad = True\n",
    "\n",
    "# Check trainable parameters again\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False | Parameter name: features.0.weight\n",
      "Requires grad: False | Parameter name: features.0.bias\n",
      "Requires grad: False | Parameter name: features.3.weight\n",
      "Requires grad: False | Parameter name: features.3.bias\n",
      "Requires grad: False | Parameter name: features.6.weight\n",
      "Requires grad: False | Parameter name: features.6.bias\n",
      "Requires grad: False | Parameter name: features.8.weight\n",
      "Requires grad: False | Parameter name: features.8.bias\n",
      "Requires grad: False | Parameter name: features.10.weight\n",
      "Requires grad: False | Parameter name: features.10.bias\n",
      "Requires grad: False | Parameter name: classifier.1.weight\n",
      "Requires grad: False | Parameter name: classifier.1.bias\n",
      "Requires grad: False | Parameter name: classifier.4.weight\n",
      "Requires grad: False | Parameter name: classifier.4.bias\n",
      "Requires grad: True | Parameter name: classifier.6.weight\n",
      "Requires grad: True | Parameter name: classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Requires grad: {param.requires_grad} | Parameter name: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Replacing the Classification Layer**\n",
    "\n",
    "The classification layer of AlexNet is named `model.classifier[6]`, which is a fully connected (FC) layer with 1,000 outputs suitable for ImageNet classes. \n",
    "\n",
    "For a new task (e.g., 10-class classification), replace this with a new FC layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_features = model.classifier[6].in_features     # get the number of input features the current classification layer\n",
    "model.classifier[6] = nn.Linear(num_features, 10)  # replace it with a new classification layer with 10 outputs\n",
    "\n",
    "# print updated model, check the the last layer output size is now 10 (previous was 1,000)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 57,044,810\n",
      "Trainable Parameters: 40,970\n",
      "Percentage Trainable: 0.07%\n"
     ]
    }
   ],
   "source": [
    "# count current trainable parameters\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False | Parameter name: features.0.weight\n",
      "Requires grad: False | Parameter name: features.0.bias\n",
      "Requires grad: False | Parameter name: features.3.weight\n",
      "Requires grad: False | Parameter name: features.3.bias\n",
      "Requires grad: False | Parameter name: features.6.weight\n",
      "Requires grad: False | Parameter name: features.6.bias\n",
      "Requires grad: False | Parameter name: features.8.weight\n",
      "Requires grad: False | Parameter name: features.8.bias\n",
      "Requires grad: False | Parameter name: features.10.weight\n",
      "Requires grad: False | Parameter name: features.10.bias\n",
      "Requires grad: False | Parameter name: classifier.1.weight\n",
      "Requires grad: False | Parameter name: classifier.1.bias\n",
      "Requires grad: False | Parameter name: classifier.4.weight\n",
      "Requires grad: False | Parameter name: classifier.4.bias\n",
      "Requires grad: True | Parameter name: classifier.6.weight\n",
      "Requires grad: True | Parameter name: classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Requires grad: {param.requires_grad} | Parameter name: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now replaced the original classification (ouput) layer with a customer layer. This layer is now both unfrozeen and initialized with random parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing the Entire Classification Head**\n",
    "\n",
    "For more flexibility, you can replace the **entire classification head**, not just the last layer. In AlexNet, the head typically starts after the feature extractor (convolutional layers).\n",
    "\n",
    "Note also that the the whole classification head is encapsulated in 'model.classifier', using the so-called nn.Sequential syntax. \n",
    "\n",
    "We can define our own nn.Sequential head and replace the built-in one with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of nn.Sequential syntax\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "                                    # Layer 1: Conv -> ReLU -> Pool\n",
    "                                    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1), \n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "                                    # Layer 2: Conv -> ReLU -> Pool\n",
    "                                    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "                                    # Layer 3: Conv -> ReLU -> Pool\n",
    "                                    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "                                    # Flatten\n",
    "                                    nn.Flatten(),\n",
    "\n",
    "                                    # One-layer classification head\n",
    "                                    nn.Linear(64 * 4 * 4, num_classes)  # Assuming input size 32x32\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5, inplace=False)\n",
       "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the whole classifier encapulated in model.classifier\n",
    "\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the first layer of neurons in the classifier head has 9216 in_features, we need to have the same in our new head.\n",
    "\n",
    "head_in_features = 9216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define a custom sequence and replace the entire classification head with it\n",
    "# note that our custom sequence architecture is completely arbitrary and up to us to decide\n",
    "# only constrains are the 9216 in_features and 10 out_features of the last layer (which corresponds to the problem we want to solve)\n",
    "\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "                                 nn.Linear(head_in_features, 512),  # First FC layer\n",
    "                                 nn.ReLU(),                         \n",
    "                                 nn.Dropout(0.5),                   \n",
    "                                 nn.Linear(512, 10)                 # Output layer\n",
    "                                )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 7,193,930\n",
      "Trainable Parameters: 4,724,234\n",
      "Percentage Trainable: 65.67%\n"
     ]
    }
   ],
   "source": [
    "# Check parameters again after adding a new head\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False | Parameter name: features.0.weight\n",
      "Requires grad: False | Parameter name: features.0.bias\n",
      "Requires grad: False | Parameter name: features.3.weight\n",
      "Requires grad: False | Parameter name: features.3.bias\n",
      "Requires grad: False | Parameter name: features.6.weight\n",
      "Requires grad: False | Parameter name: features.6.bias\n",
      "Requires grad: False | Parameter name: features.8.weight\n",
      "Requires grad: False | Parameter name: features.8.bias\n",
      "Requires grad: False | Parameter name: features.10.weight\n",
      "Requires grad: False | Parameter name: features.10.bias\n",
      "Requires grad: True | Parameter name: classifier.0.weight\n",
      "Requires grad: True | Parameter name: classifier.0.bias\n",
      "Requires grad: True | Parameter name: classifier.3.weight\n",
      "Requires grad: True | Parameter name: classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Requires grad: {param.requires_grad} | Parameter name: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we'd like to unfreeze the last conv layer aswell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 7,193,930\n",
      "Trainable Parameters: 5,314,314\n",
      "Percentage Trainable: 73.87%\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze only the last classifier layer (the output layer) \n",
    "for name, param in model.named_parameters():\n",
    "    if \"features.10\" in name:                \n",
    "        param.requires_grad = True\n",
    "\n",
    "# Check trainable parameters again\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires grad: False | Parameter name: features.0.weight\n",
      "Requires grad: False | Parameter name: features.0.bias\n",
      "Requires grad: False | Parameter name: features.3.weight\n",
      "Requires grad: False | Parameter name: features.3.bias\n",
      "Requires grad: False | Parameter name: features.6.weight\n",
      "Requires grad: False | Parameter name: features.6.bias\n",
      "Requires grad: False | Parameter name: features.8.weight\n",
      "Requires grad: False | Parameter name: features.8.bias\n",
      "Requires grad: True | Parameter name: features.10.weight\n",
      "Requires grad: True | Parameter name: features.10.bias\n",
      "Requires grad: True | Parameter name: classifier.0.weight\n",
      "Requires grad: True | Parameter name: classifier.0.bias\n",
      "Requires grad: True | Parameter name: classifier.3.weight\n",
      "Requires grad: True | Parameter name: classifier.3.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f'Requires grad: {param.requires_grad} | Parameter name: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our model for training is super easy. We have it saved in the variable `model` and can, at any point,\n",
    "\n",
    "throw it into our training loop and train it - usually once we're satisfied with freezing/unfreezing of layers \n",
    "\n",
    "and potential replacement of the classification head/layers. \n",
    "\n",
    "**Note**: \n",
    "\n",
    "For optimal performance, you might need to resize your images to the input size (often 224x224) of the pre-trained model. \n",
    "\n",
    "This can easily be done using transformations though. Look it up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
