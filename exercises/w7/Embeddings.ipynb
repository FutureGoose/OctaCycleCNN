{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings in Deep Learning\n",
    "---\n",
    "\n",
    "*What are Embeddings?*\n",
    "\n",
    "In deep learning, **embeddings** are dense vector representations of data. They are essentially learned numerical representations of input data (e.g., images, words, sentences) in a continuous vector space. Embeddings are particularly powerful because they encode **meaningful features** that capture the intrinsic relationships within the data.\n",
    "\n",
    "For Convolutional Neural Networks (CNNs), embeddings are typically found in the final layer (or penultimate layer, the last layer *before* the output layer) of the network before classification. \n",
    "\n",
    "At this stage, the CNN has distilled the input image into a highly compressed form—a vector containing high-level abstract features learned during training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why are Embeddings important?*\n",
    "\n",
    "Embeddings are essential in deep learning because they:\n",
    "\n",
    "1. **Capture High-Level Features:** The embeddings represent a distilled version of the most relevant information in the input (e.g., visual features in images).\n",
    "2. **Enable Comparisons:** The embeddings of similar inputs are close in vector space, enabling us to compute similarity or clustering.\n",
    "3. **Transfer Knowledge:** Pre-trained CNN embeddings can generalize well to new tasks without requiring re-training, saving computational resources.\n",
    "4. **Reduce Dimensionality:** Raw input data (e.g., pixels of an image) is typically very high-dimensional. Embeddings reduce this to a compact vector representation while retaining meaningful features.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How CNNs work (a brief recap)*\n",
    "\n",
    "CNNs process images through a series of **convolutions, pooling, and nonlinear activations** to extract increasingly abstract features:\n",
    "\n",
    "- **Early layers** focus on low-level features like edges, textures, or colors.\n",
    "- **Deeper layers** learn higher-level features, such as shapes, objects, or patterns.\n",
    "- **Output layer** delivers a task tailerd output, e.g., class scores (for classification) or a continous number (for regression).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Where Embeddings fit in CNNs*\n",
    "\n",
    "Usually, the **last hidden layer** (the output layer) is where embeddings are most commonly extracted from. This layer represents the high-level features learned by the CNN in a compact vector. For example:\n",
    "\n",
    "- In a CNN trained on ImageNet, the last layer might output a **1000-dimensional vector** for classification across 1000 categories.\n",
    "- By removing the classification layer, the model's last hidden layer instead outputs a **feature vector (embedding)** for any input image, which can be used for tasks like similarity search, clustering, and transfer learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Applications of CNN Embeddings\n",
    "\n",
    "**1. Image Similarity Search**\n",
    "\n",
    "CNN embeddings allow us to compare images by computing the **distance** (e.g., cosine similarity or Euclidean distance) between their feature vectors. For instance:\n",
    "\n",
    "- Images of dogs will have embeddings that are closer together in vector space than embeddings of dogs and cats.\n",
    "- This makes embeddings ideal for **content-based image retrieval (CBIR)** systems.\n",
    "\n",
    "**2. Transfer Learning**\n",
    "\n",
    "CNN embeddings allow us to transfer knowledge from a pre-trained model to a new task. Instead of training a new model from scratch:\n",
    "\n",
    "- Use a pre-trained CNN to process all the images in your dataset in order to generate rich embeddings for each one of them. \n",
    "- Use these embeddings as input features to our new model.\n",
    "- Train a lightweight classifier or regressor on these features for your task.\n",
    "\n",
    "\n",
    "**3. Clustering and Dimensionality Reduction**\n",
    "\n",
    "Embedding spaces make it easier to cluster images into groups or visualize them using techniques like **t-SNE** or **UMAP**.\n",
    "\n",
    "\n",
    "**4. Anomaly Detection**\n",
    "\n",
    "If the embeddings of a CNN are clustered around typical examples in your dataset, any input image that generates an **outlier embedding** (far from the cluster) can be flagged as anomalous.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  How do Embeddings work?\n",
    "\n",
    "Some key properties of Embeddings are as follows\n",
    "\n",
    "1. **Proximity Represents Similarity:** Similar inputs produce embeddings that are close in vector space.\n",
    "2. **Disentangled Features:** Embeddings separate distinct features (e.g., color, texture, shape) into independent dimensions.\n",
    "3. **Generalization:** Pre-trained embeddings generalize well across domains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example - Extracting and using Image Embeddings\n",
    "\n",
    "We’ll use PyTorch to load a pre-trained ResNet-50 model and remove its final classification layer. We'll then use the model to generate embeddings for the images we feed into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Load Pre-trained ResNet-50\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Inspect the model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the classification head\n",
    "\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model\n",
    "# Note that this will now output a 2048-dimensional vector, for each input image\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tranformation to preprocess the input image, to the expected 224x224 input size of the ResNet-50 model\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224, 224)),  # Resize to 224x224 (ResNet input size)\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as ResNet expects\n",
    "                               ])\n",
    "\n",
    "\n",
    "# Define a function to get the embedding for an image\n",
    "\n",
    "def get_embedding(image_path, model):\n",
    "    \n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Generate embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image).squeeze()  # Remove batch and spatial dimensions\n",
    "\n",
    "    result = embedding.numpy().reshape(1, -1) # Convert to numpy array and reshape to (1, output_dim_size)\n",
    " \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Excercises\n",
    "\n",
    "### Problem 1.\n",
    "\n",
    "**a)**\n",
    "\n",
    "Begin by downloading (from arbitrary source) images of cats, dogs and birds. Make sure to have 3 different images of each animal.\n",
    "\n",
    "Then, generate and extract embeddings for all 9 images. \n",
    "\n",
    "Once that's done, start calculating the cosine similarity (imported from sklearn, above) between each pair of embeddings for your animal pictures. Does the results make sense?\n",
    "\n",
    "\n",
    "*Also, do you recall cosine similarity from Linear Algebra course? :)*\n",
    "\n",
    "If not, here's the official documentation https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
    "\n",
    "What does the cosine_similarity measure, do you think?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "Repeat the above excercise but now instead of cosine similarity, calculate the *euclidean distance* between the all the image embeddings pairs. Does the result make sense?\n",
    "\n",
    "*Note: what does the euclidean distance (pythagoran distance) between two vectors mean?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)**\n",
    "\n",
    "Can we draw any conclusions from the above results? What does it mean for the embeddings of two images to be close or far apart? Are the results \"perfect\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)**\n",
    "\n",
    "Now try another pre-trained model and repeat the above. Do you get similar, better or worse results? What conclusions can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Use-cases everywhere!\n",
    "\n",
    "Look at the list of example use-cases above. Can you think of concrete examples of each? Write down specific applications for each use-case, and discuss with your classmates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Tiiituuut - Vem där?! :)\n",
    "\n",
    "Let's apply what we've learned so far to try building a *face recognition* app. \n",
    "\n",
    "**a)**\n",
    "\n",
    "Start by collecting a bunch of pictures of yourself, and then some of your friends and/or random people you find on the internet. \n",
    "\n",
    "Then, repeat the excercise from Problem 1, but now with these images instead. Can you accurately pinpoint yourself in images?\n",
    "\n",
    "\n",
    ".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**\n",
    "\n",
    "Ok, let's get serious with a challange here. Assume now that we want to create an app that connect to your webcam (if you have one) and analyze pictures taken from it. If you don't have a webcam, team up with a friend that does.\n",
    "\n",
    "This problem can be split into a few steps:\n",
    "\n",
    "1. Connect to your webcam via Python. Search online on how to do it.\n",
    "2. Take pictures in regular intervals (e.g. every 5 seconds, or perhaps less).\n",
    "3. Generate embeddings of these images the moment they are taken, and compare them with the embeddings of the images you've collected in **a)**. If the embeddings are close, you've hopefully found a match!\n",
    "4. Display the result on the screen somehow. \n",
    "\n",
    "If done correctly, this will work as \"real-time\" face recognition app!\n",
    "\n",
    "*Note: How will you compare the embeddings of the images taken from the camera, with the existing images of yourself? What criteria do you set for a match? Discuss with your classmates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
