{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding LLMs and Transformers: Tokenization and Embeddings\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large Language Models (LLMs), like GPT or BERT, are advanced AI models designed to understand and generate natural language. \n",
    "\n",
    "They rely on the **Transformer architecture**, a revolutionary approach that has reshaped Natural Language Processing (NLP).\n",
    "\n",
    "Before diving into how transformers work, it is essential to understand the foundational concepts:\n",
    "1. **Tokenization**: How text is broken into smaller units for the model to process.\n",
    "2. **Embeddings**: How tokens are represented as numerical vectors.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units called **tokens**. Tokens could be:\n",
    "- **Words**: `\"The quick brown fox\"` ‚Üí `[\"The\", \"quick\", \"brown\", \"fox\"]`\n",
    "- **Subwords**: `\"unbelievable\"` ‚Üí `[\"un\", \"believable\"]`\n",
    "- **Characters**: `\"hello\"` ‚Üí `[\"h\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "\n",
    "### Why Tokenize?\n",
    "\n",
    "Models work with numbers, not raw text. Tokenization converts raw text into numerical data that a model can process.\n",
    "\n",
    "### Types of Tokenizers\n",
    "1. **Word-based Tokenizer**: Splits at spaces and punctuation (naive but fast).\n",
    "2. **Character-based Tokenizer**: Breaks text into individual characters (useful for non-Latin scripts).\n",
    "3. **Subword-based Tokenizer**: Used in modern LLMs (e.g., Byte Pair Encoding, WordPiece). \n",
    "   - Balances vocabulary size and token granularity by splitting into common subwords.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Code Example: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-based Tokens: ['Transformers', 'are', 'amazing!', 'ü§Ø', 'AIJId878vs¬§13']\n",
      "Character-based Tokens: ['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', 's', ' ', 'a', 'r', 'e', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', '!', ' ', 'ü§Ø', ' ', 'A', 'I', 'J', 'I', 'd', '8', '7', '8', 'v', 's', '¬§', '1', '3']\n"
     ]
    }
   ],
   "source": [
    "# Simple Tokenization Example\n",
    "text = \"Transformers are amazing! ü§Ø AIJId878vs¬§13\"\n",
    "\n",
    "# Naive word-based tokenizer\n",
    "tokens = text.split()\n",
    "print(\"Word-based Tokens:\", tokens)\n",
    "\n",
    "# Character-based tokenizer\n",
    "char_tokens = list(text)\n",
    "print(\"Character-based Tokens:\", char_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustaf/projects/deeplearning/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword Tokens: ['transformers', 'are', 'amazing', '!', '[UNK]', 'ai', '##ji', '##d', '##8', '##7', '##8', '##vs', '##¬§', '##13']\n"
     ]
    }
   ],
   "source": [
    "# Using Hugging Face Tokenizer (Subword-based)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text\n",
    "subword_tokens = tokenizer.tokenize(text)\n",
    "print(\"Subword Tokens:\", subword_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "After running the code above, you will see:\n",
    "- **Word-based Tokens**: `['Transformers', 'are', 'amazing!']`\n",
    "- **Character-based Tokens**: `['T', 'r', 'a', 'n', ...]`\n",
    "- **Subword-based Tokens**: `['transformers', 'are', 'amazing', '!']`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Tokens to Embeddings\n",
    "\n",
    "**What Does \"Mapping Tokens to Embeddings\" Mean?**\n",
    "\n",
    "Once the text is tokenized, the tokens are still symbolic representations (e.g., `['transformers', 'are']`). \n",
    "\n",
    "The model cannot process these directly because neural networks operate on numerical data. To enable computation:\n",
    "\n",
    "1. Tokens are first **mapped to unique IDs** using a **vocabulary**.\n",
    "2. These IDs are then mapped to their corresponding embeddings using an **embedding matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Step 1: Token-to-ID Mapping**\n",
    "\n",
    "- Each token has a unique integer ID, assigned by the tokenizer.\n",
    "- The tokenizer's vocabulary is a lookup table mapping tokens to IDs.\n",
    "\n",
    "**Code Example: Token-to-ID Mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['transformers', 'are', 'amazing', '!']\n",
      "Token IDs: [19081, 2024, 6429, 999]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the text\n",
    "#text = \"ÿ≥ÿßÿ±ÿ© ŸáŸä ŸÅŸä ŸÉŸÖÿ®ŸàÿØŸäÿß ÿ£Ÿàÿ™ÿ¥ ÿ¥ŸäŸÑÿßÿ±\"\n",
    "text = \"Transformers are amazing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Mapping IDs to Embeddings**\n",
    "\n",
    "The model uses an embedding matrix (a large table) where:\n",
    "- Rows represent tokens in the vocabulary.\n",
    "- Columns represent dimensions of the embedding (e.g., 768 in BERT).\n",
    "\n",
    "A token ID is used as an index to retrieve its corresponding row in the embedding matrix.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "- Assume the vocabulary size is $V$ (e.g., 30,000) and the embedding size is $D$ (e.g., 768).\n",
    "- The embedding matrix is then a $V \\times D$ matrix.\n",
    "- For each token ID, the model retrieves the corresponding row as the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix:\n",
      " tensor([[ 0.5127, -0.1147,  0.8532],\n",
      "        [ 0.4294, -1.5529,  1.0748],\n",
      "        [ 0.6209,  1.3002,  0.1265],\n",
      "        [-0.2731, -0.3250,  0.1171],\n",
      "        [ 1.4866,  0.8414,  0.6397]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example embedding matrix (vocab_size=5, embedding_dim=3 for simplicity)\n",
    "vocab_size = 5\n",
    "embedding_dim = 3\n",
    "embedding_matrix = torch.randn(vocab_size, embedding_dim)\n",
    "\n",
    "print(\"Embedding Matrix:\\n\", embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids: tensor([0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example token IDs\n",
    "token_ids = torch.tensor([0, 3, 4])  # Simulated token IDs\n",
    "print(f'token_ids: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Embeddings:\n",
      " tensor([[ 0.5127, -0.1147,  0.8532],\n",
      "        [-0.2731, -0.3250,  0.1171],\n",
      "        [ 1.4866,  0.8414,  0.6397]])\n"
     ]
    }
   ],
   "source": [
    "# Retrieve embeddings\n",
    "embeddings = embedding_matrix[token_ids]\n",
    "print(\"Retrieved Embeddings:\\n\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Models: Token Mapping with Embeddings\n",
    "\n",
    "In real-world models like BERT, the embedding matrix is trained to represent meaningful relationships between tokens.\n",
    "\n",
    "**Code Example: Mapping with a Pretrained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[  101, 19081,  2024,  6429,   999,   102]])\n",
      "Embeddings Shape: torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input text\n",
    "text = \"Transformers are amazing!\"\n",
    "\n",
    "# Tokenize and get token IDs\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # Token IDs are in inputs['input_ids']\n",
    "token_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Retrieve the embedding matrix\n",
    "embedding_layer = model.embeddings.word_embeddings\n",
    "\n",
    "# Get embeddings for input tokens\n",
    "embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Embeddings Shape:\", embeddings.shape)  # (batch_size, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "         [ 0.0189, -0.0289, -0.0768,  ...,  0.0116, -0.0212,  0.0171],\n",
       "         [-0.0134, -0.0135,  0.0250,  ...,  0.0013, -0.0183,  0.0227],\n",
       "         [ 0.0168, -0.0245, -0.0513,  ..., -0.0209, -0.0529, -0.0645],\n",
       "         [ 0.0298, -0.0373, -0.0356,  ...,  0.0161,  0.0192,  0.0173],\n",
       "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3416e-02, -1.3482e-02,  2.5002e-02, -5.1527e-02, -5.1794e-02,\n",
       "         9.6558e-03,  6.7361e-03, -5.4607e-02,  2.5705e-02,  7.2570e-03,\n",
       "         4.4699e-03, -1.5962e-02, -6.9308e-02, -1.6379e-02, -1.2439e-02,\n",
       "         8.5646e-04,  1.3288e-02, -3.7826e-02,  8.5045e-03, -2.1850e-02,\n",
       "         3.6346e-02, -4.0202e-02, -1.4137e-02, -5.1339e-02, -1.6540e-02,\n",
       "        -3.9799e-02,  1.6954e-02,  1.0004e-02, -2.2243e-02,  4.5188e-02,\n",
       "         3.2355e-02, -8.1226e-05,  3.9237e-02, -2.4096e-02,  8.4741e-03,\n",
       "         7.3312e-03,  6.7126e-03, -1.9774e-03, -8.1773e-02,  9.8314e-03,\n",
       "         1.2894e-02,  3.2907e-02,  3.0951e-02,  3.2138e-02,  1.4520e-02,\n",
       "        -2.9762e-02, -5.1314e-02,  1.1868e-02, -3.0954e-02,  1.3684e-02,\n",
       "        -4.3889e-02, -1.1218e-02,  1.1214e-02, -4.5011e-02,  2.4642e-02,\n",
       "        -1.6498e-03,  2.1277e-02, -1.3931e-02, -1.3391e-02,  3.5602e-03,\n",
       "         3.8181e-02, -4.9932e-02, -2.6025e-02,  5.4804e-03,  3.5982e-03,\n",
       "        -2.2312e-02, -5.7361e-03, -4.6598e-02,  9.9819e-03,  3.7946e-02,\n",
       "        -1.6794e-02, -3.1839e-02, -3.7090e-03, -8.1078e-02, -6.3573e-02,\n",
       "         1.8759e-02,  2.8869e-02, -2.4654e-02,  2.5345e-02,  2.6138e-03,\n",
       "        -2.2385e-02,  1.0016e-02, -4.5895e-02, -1.9754e-02, -4.0777e-02,\n",
       "         4.3386e-02, -3.5680e-02, -5.7786e-02,  3.4418e-02,  4.3676e-02,\n",
       "        -6.0147e-03,  4.6158e-02,  1.5136e-02, -5.5040e-02,  7.2238e-03,\n",
       "        -5.8384e-02, -5.6430e-02,  1.1827e-02,  2.9456e-02,  2.6321e-02,\n",
       "        -2.2398e-02, -8.6444e-03,  4.0199e-02,  2.3808e-02, -4.5819e-02,\n",
       "        -6.8353e-02, -2.1579e-02, -1.6438e-02, -1.3285e-02,  1.4338e-03,\n",
       "        -3.8313e-02, -4.2315e-02, -1.9704e-02, -1.5014e-02,  1.5876e-02,\n",
       "         1.6107e-02, -8.3360e-03,  3.1758e-02, -3.2211e-02, -4.4467e-02,\n",
       "        -2.6626e-02, -8.0435e-02, -6.1151e-02,  1.2445e-02, -1.1818e-03,\n",
       "        -4.6182e-02, -4.3277e-02, -5.1449e-02, -1.3498e-02,  2.4500e-03,\n",
       "         2.8939e-02, -6.5151e-02, -2.7538e-02, -5.2979e-02, -7.1972e-03,\n",
       "        -1.8231e-02, -1.5318e-02, -1.7029e-02,  1.8930e-02,  6.3299e-03,\n",
       "        -7.2061e-03, -2.3169e-03,  2.0286e-02, -6.2768e-02, -5.6003e-02,\n",
       "        -8.5838e-02, -1.4409e-03, -3.0378e-02, -3.9430e-02,  7.7516e-03,\n",
       "         2.8290e-02, -3.9174e-03, -7.4839e-02, -1.6286e-02,  1.6632e-02,\n",
       "        -4.6163e-02,  2.8590e-02, -6.5148e-03, -7.0198e-02, -4.1037e-02,\n",
       "        -3.5507e-02, -4.1685e-02, -4.6906e-02, -1.4162e-02,  1.2057e-02,\n",
       "        -2.4942e-03,  5.7166e-02,  3.0679e-02, -2.7590e-02, -1.6325e-02,\n",
       "         1.7813e-02, -1.2713e-02, -1.3442e-02, -7.5056e-03, -1.7433e-02,\n",
       "        -2.8402e-02, -4.0933e-02,  4.4005e-02,  3.3679e-03, -3.2694e-02,\n",
       "         4.9119e-02, -2.4152e-02,  1.3229e-02, -1.0262e-02, -2.9566e-02,\n",
       "         5.0616e-03, -8.6684e-03, -3.9263e-02, -3.6976e-02, -2.6288e-02,\n",
       "        -1.0991e-02, -4.0893e-02, -1.1049e-02,  4.9182e-03,  1.0509e-02,\n",
       "         7.3631e-03,  1.0427e-02, -3.2447e-02,  1.9438e-02, -8.5361e-03,\n",
       "         5.5391e-03,  4.5541e-03, -3.2260e-02, -6.8500e-03,  6.3933e-03,\n",
       "         1.4712e-03, -4.0494e-02, -4.5613e-03,  2.1326e-02,  2.1196e-02,\n",
       "         2.1657e-02,  9.1655e-03, -2.6218e-02, -4.4359e-02, -3.4391e-02,\n",
       "        -5.1088e-02, -4.6619e-02, -7.1842e-02, -3.1078e-04, -7.6728e-03,\n",
       "         6.2725e-03, -3.6905e-02, -3.3447e-02,  4.6055e-02, -5.4201e-03,\n",
       "        -2.0807e-02,  1.2754e-02,  1.1834e-02, -4.1648e-02,  4.6626e-03,\n",
       "         1.4180e-02, -1.0659e-02, -1.1456e-02, -3.9669e-02, -4.5416e-02,\n",
       "         2.4918e-02,  2.3373e-02, -1.0119e-02, -4.0458e-02, -3.5299e-02,\n",
       "         4.8104e-03, -1.7226e-02, -1.3711e-02,  1.0148e-02, -5.5793e-02,\n",
       "         2.9190e-02, -2.9284e-02, -3.4313e-02, -1.0433e-02, -3.7420e-02,\n",
       "        -2.2952e-03,  2.2815e-02, -3.9058e-02,  4.6287e-02,  3.8906e-03,\n",
       "         6.4483e-02, -2.8638e-02,  6.1775e-03,  1.6355e-02, -7.0636e-02,\n",
       "         2.3707e-03,  2.2994e-02,  2.8215e-02,  1.4204e-02,  1.3049e-02,\n",
       "         6.5604e-03, -2.4113e-02,  1.3734e-02,  2.7755e-02, -5.1954e-02,\n",
       "        -2.5840e-02,  2.0045e-03,  5.6520e-03, -6.7909e-02,  1.6322e-02,\n",
       "        -5.9330e-02, -3.3836e-02,  1.7976e-02, -4.0300e-04,  2.6047e-02,\n",
       "        -4.4740e-02,  3.7239e-02, -1.3640e-02, -2.4097e-02, -4.6153e-02,\n",
       "        -5.1094e-02, -4.0420e-02,  1.2464e-02,  1.6524e-03,  1.5382e-02,\n",
       "        -5.1163e-02, -2.0834e-02,  1.5841e-02, -3.6289e-02,  1.0781e-03,\n",
       "        -3.0570e-02, -4.3453e-02,  7.3338e-03,  1.2754e-03, -8.3906e-03,\n",
       "         1.0718e-02, -1.0137e-02,  5.5154e-03, -1.3150e-02, -2.6515e-02,\n",
       "        -4.6529e-02, -1.1266e-02, -3.6700e-02, -1.8910e-01, -6.1311e-02,\n",
       "        -2.5093e-02, -1.5464e-02, -4.2526e-02,  4.4626e-03, -5.0455e-02,\n",
       "        -1.5735e-02, -3.7920e-02,  1.1734e-02, -8.4953e-03, -2.9054e-02,\n",
       "        -4.5284e-02, -1.1079e-02,  1.7145e-02,  3.0711e-02,  1.0643e-02,\n",
       "        -1.3768e-02, -3.6054e-02,  5.9606e-04, -5.0664e-02, -5.6973e-02,\n",
       "        -1.7092e-02, -3.7763e-02,  3.3490e-03,  1.9429e-02,  4.2300e-02,\n",
       "        -4.2713e-02, -1.7356e-02, -3.0200e-02,  1.4990e-02, -1.3802e-02,\n",
       "         1.2738e-02, -9.8730e-03,  3.0995e-03,  8.4024e-03, -1.7834e-02,\n",
       "        -4.8597e-02, -4.1284e-02, -4.4243e-02,  2.6081e-02,  1.9196e-02,\n",
       "        -6.7617e-03, -3.6019e-02,  1.9794e-02, -3.7101e-02, -1.9617e-02,\n",
       "        -7.7958e-03,  2.0106e-02, -3.3023e-02, -5.6577e-02, -2.0688e-03,\n",
       "        -4.3952e-02,  3.3839e-03, -3.9306e-02,  1.2438e-02, -3.1736e-02,\n",
       "        -4.3576e-02, -6.6551e-02, -1.4734e-02,  2.0043e-02, -4.4955e-02,\n",
       "        -2.6548e-02,  8.3874e-03,  3.0139e-02,  4.5645e-03,  1.5674e-02,\n",
       "         2.8479e-02,  3.1153e-02, -1.0109e-02, -1.3460e-02,  4.7165e-02,\n",
       "         2.8934e-04,  3.1107e-03, -3.4219e-04, -7.6794e-02, -3.6207e-02,\n",
       "         3.2310e-02, -4.0081e-02, -4.0278e-02, -2.2199e-02, -2.9117e-02,\n",
       "        -1.1597e-02, -1.3400e-02,  7.4639e-04, -3.3626e-04,  1.5549e-02,\n",
       "         1.6962e-02, -1.0699e-02,  2.7157e-02,  1.2452e-02, -3.4510e-04,\n",
       "        -4.6550e-02, -3.4925e-02, -3.7990e-02,  1.4249e-02, -6.8462e-02,\n",
       "         4.3585e-03,  1.6871e-02,  1.7991e-02,  3.6876e-02, -5.1317e-03,\n",
       "        -1.3653e-02,  1.9712e-02,  1.5843e-02,  1.7354e-02, -5.8786e-03,\n",
       "        -5.7748e-03, -2.7907e-02,  3.6668e-02, -2.6611e-02,  3.2195e-02,\n",
       "         5.9867e-02, -2.7929e-02, -1.8245e-02, -3.0877e-02,  3.6787e-03,\n",
       "        -4.7625e-02,  2.6744e-02,  7.3799e-03, -8.3183e-03, -7.0325e-02,\n",
       "        -1.9415e-02,  6.1916e-03, -2.1078e-02,  1.8745e-02, -1.4730e-02,\n",
       "        -5.6382e-02, -5.2923e-02,  8.7823e-03,  8.1615e-03, -4.6936e-03,\n",
       "         2.5126e-02, -2.6073e-02, -6.8062e-02, -2.8900e-02,  7.5205e-03,\n",
       "         1.0542e-02,  8.1530e-03,  1.0316e-02, -5.1839e-03, -1.3846e-02,\n",
       "        -4.6317e-02, -8.2716e-03, -3.1014e-02, -4.2175e-02,  1.1886e-02,\n",
       "        -3.9957e-02,  1.4252e-02, -5.0324e-02, -5.5565e-02, -4.5391e-03,\n",
       "        -4.3180e-02, -2.5319e-02,  1.0982e-02,  1.6036e-02,  1.0194e-02,\n",
       "        -2.2568e-02, -2.9964e-02, -1.2809e-02,  2.3767e-02, -1.9401e-02,\n",
       "         2.3244e-02, -3.4570e-02, -1.0431e-02,  3.5209e-02,  1.5189e-02,\n",
       "        -4.9824e-03, -1.2949e-02,  9.5615e-03, -3.6498e-02,  2.9829e-02,\n",
       "        -3.1012e-02, -4.0486e-02, -3.1296e-02, -9.9845e-03, -5.5263e-02,\n",
       "        -2.2631e-02, -1.3672e-02,  2.2956e-02, -6.4841e-02, -3.5742e-02,\n",
       "        -3.3314e-02,  9.8433e-04, -4.9488e-02,  1.1510e-02, -4.6530e-02,\n",
       "         2.5190e-03,  1.5078e-03, -4.2401e-02, -1.9225e-02, -2.1557e-02,\n",
       "        -5.7676e-02,  1.4247e-02, -5.0511e-03,  1.1951e-02,  7.1571e-03,\n",
       "        -9.6917e-03, -3.7985e-02,  1.0996e-02, -4.6459e-02, -5.1737e-02,\n",
       "        -7.6179e-03, -2.2953e-02,  1.7940e-03, -1.6093e-02, -4.7090e-02,\n",
       "        -1.8720e-02, -3.6088e-02, -4.1416e-02,  8.3508e-03, -1.8404e-02,\n",
       "         3.3876e-02,  3.0412e-02,  2.8414e-02, -3.1198e-03, -1.8742e-02,\n",
       "         2.9048e-02, -6.2716e-02, -5.4464e-02,  1.6659e-02, -8.4780e-02,\n",
       "         1.3159e-02, -2.3386e-02, -5.5656e-02, -6.1884e-02,  1.7754e-02,\n",
       "        -4.2724e-02, -3.0989e-02,  2.7591e-02, -4.9407e-02, -2.9873e-02,\n",
       "         2.4476e-02, -6.5920e-02,  3.2327e-02, -7.2544e-03,  3.4776e-02,\n",
       "        -4.2750e-02, -2.1168e-02,  2.3588e-02, -7.2862e-02,  1.8900e-02,\n",
       "        -2.2893e-02, -4.3493e-02,  2.9531e-02, -2.6275e-03, -4.7503e-02,\n",
       "         7.1311e-03,  3.8472e-02, -3.7647e-02, -9.8610e-03,  2.7268e-02,\n",
       "         2.8358e-02, -3.4689e-02, -2.7728e-02, -5.8774e-02, -5.0847e-02,\n",
       "        -2.3381e-02, -6.6383e-02, -1.4728e-02, -1.6726e-02,  3.4026e-02,\n",
       "         2.5088e-02, -4.3638e-02,  1.5283e-02, -1.7251e-03,  3.6871e-02,\n",
       "        -1.3993e-02, -2.1976e-02, -3.2279e-02, -3.2898e-02, -9.5876e-03,\n",
       "         2.9660e-02, -3.8168e-03, -2.3541e-02, -2.2549e-02,  1.2584e-02,\n",
       "         4.7286e-03,  4.1851e-02, -4.9565e-02,  1.4165e-02, -5.8084e-03,\n",
       "        -8.0346e-02,  3.4833e-03, -9.7177e-03, -3.2658e-02, -1.8841e-02,\n",
       "         3.5258e-02, -7.0653e-02,  3.2791e-02, -1.3113e-02, -2.9500e-02,\n",
       "        -4.6177e-02, -2.0319e-02,  3.5732e-02,  1.0491e-02, -5.0885e-02,\n",
       "        -4.9052e-02,  1.2942e-02, -8.4418e-03, -3.4722e-02,  1.0949e-02,\n",
       "         7.3655e-03,  3.0341e-03, -5.9534e-02, -3.7469e-02, -8.2318e-03,\n",
       "        -5.4583e-02, -1.5238e-02,  1.1628e-02,  1.9336e-02, -6.1753e-02,\n",
       "        -6.8730e-02,  4.5812e-02,  7.5088e-05, -6.1677e-02,  6.2522e-04,\n",
       "         1.4800e-02, -9.9586e-03,  2.3769e-02, -4.6727e-02, -2.0083e-02,\n",
       "        -2.5099e-02, -1.6615e-02, -3.0881e-02, -5.4889e-02,  1.3875e-02,\n",
       "         3.9111e-03,  2.7407e-02, -2.1915e-02, -6.0756e-03, -3.3692e-02,\n",
       "        -1.3508e-02, -6.8114e-02, -5.2222e-02,  3.2124e-02,  3.1999e-02,\n",
       "         3.9976e-02, -5.4993e-02, -6.8447e-03, -4.3021e-02, -4.1945e-02,\n",
       "         4.9897e-03, -2.8957e-02,  1.0163e-02,  3.0123e-03, -5.4356e-02,\n",
       "         1.1511e-02, -5.1681e-02,  9.1705e-03,  3.0153e-02,  8.5653e-03,\n",
       "         1.8641e-03,  1.2968e-03,  2.2158e-02, -2.2812e-02, -1.9084e-02,\n",
       "        -5.1237e-03, -6.0095e-02, -5.8474e-02, -5.4202e-02,  2.5057e-02,\n",
       "        -3.0260e-02,  7.3986e-03,  1.6803e-03, -1.6800e-02,  3.3573e-02,\n",
       "        -5.8320e-02, -3.8298e-02, -4.4421e-02, -7.2980e-02, -3.0720e-02,\n",
       "        -1.5112e-02, -1.9936e-02, -2.9129e-02,  3.7876e-03,  1.7234e-02,\n",
       "         1.6788e-02, -4.5273e-02, -1.3937e-02, -5.9354e-02, -4.0015e-02,\n",
       "        -1.6052e-02,  9.4995e-03, -2.6376e-02, -3.5771e-02, -1.2397e-02,\n",
       "        -4.8187e-02,  2.3886e-02,  1.3792e-02, -8.3646e-03, -5.4652e-02,\n",
       "         2.0616e-02,  9.0516e-03, -3.5506e-02,  5.6907e-03,  7.6951e-03,\n",
       "        -2.1933e-02, -6.4086e-02, -1.5313e-02, -4.6814e-03, -4.7312e-03,\n",
       "        -5.2693e-02,  7.6947e-03, -1.3025e-02, -4.3640e-03, -6.6272e-03,\n",
       "         2.0507e-02, -5.0983e-02,  2.5577e-02, -2.1654e-02, -2.3506e-02,\n",
       "         1.0896e-02, -3.9943e-02,  1.7539e-04, -2.8245e-03,  3.2148e-02,\n",
       "        -1.1382e-02,  2.0755e-02, -4.5362e-02, -2.9683e-02,  2.2916e-04,\n",
       "        -6.6497e-03,  2.9812e-03,  8.3247e-03,  2.4288e-02, -8.0031e-03,\n",
       "         3.9281e-02,  9.7768e-03, -4.4819e-02,  2.6022e-02,  3.0449e-02,\n",
       "        -5.4448e-02,  5.0985e-03,  2.4355e-02, -4.8735e-02, -5.4140e-02,\n",
       "         2.6519e-02, -4.7153e-02,  2.0925e-02,  2.9909e-02,  4.3028e-02,\n",
       "        -1.7326e-02,  1.7904e-02,  3.8604e-02,  8.7910e-03, -5.7731e-03,\n",
       "        -4.4559e-02,  2.2014e-02, -2.0505e-02,  5.5945e-03, -2.8768e-02,\n",
       "         1.3384e-02, -6.6515e-02,  8.6701e-03, -5.1404e-02, -3.0021e-02,\n",
       "         1.2720e-03, -1.8292e-02,  2.2709e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.reshape(6,-1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.reshape(6,-1)[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
