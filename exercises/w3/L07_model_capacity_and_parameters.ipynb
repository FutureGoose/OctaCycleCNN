{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Curves for Neural Networks\n",
    "**Objectives**\n",
    "- Analyze and interpret training and testing loss curves for tabular data.\n",
    "- Identify common training issues such as overfitting, underfitting, and instability.\n",
    "- Experiment with model hyperparameters and training parameters to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **UCI Adult Income dataset** is a tabular dataset used for binary classification tasks. The goal is to predict whether an individual's income exceeds $50,000 per year based on census data.\n",
    "\n",
    "**Details**:\n",
    "- **Features:** 14 attributes (e.g., age, education, occupation).\n",
    "- **Target:** Binary class (income >50K or <=50K).\n",
    "- **Size:** 32,561\n",
    "\n",
    "**Main Tasks**:\n",
    "1. Load and preprocess the dataset.\n",
    "2. Split it into a training and test set.\n",
    "3. Train a neural network and analyze the training/test loss curves.\n",
    "4. Perform experiments to understand how model capacity and learning rate impact performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "\n",
    "Objectives:\n",
    "1. Load the dataset using `pandas`.\n",
    "2. Split the data into a training and test set (say, 90%/10%)\n",
    "3. Prepare data for training and inference by adequately handling missing values, categorical features, and normalizing the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "           \"hours-per-week\", \"native-country\", \"income\"]\n",
    "\n",
    "df = pd.read_csv(url, names=columns, sep=',\\s*', engine='python', na_values=\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (32561, 15)\n",
      "\n",
      "df.info():\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n",
      "None\n",
      "\n",
      "df.head():\n",
      "\n",
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n",
      "\n",
      "df.describe():\n",
      "\n",
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
      "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
      "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    32561.000000  \n",
      "mean        40.437456  \n",
      "std         12.347429  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n",
      "\n",
      "df.describe(include=['category', 'object']):\n",
      "\n",
      "       workclass education      marital-status      occupation relationship  \\\n",
      "count      32561     32561               32561           32561        32561   \n",
      "unique         9        16                   7              15            6   \n",
      "top      Private   HS-grad  Married-civ-spouse  Prof-specialty      Husband   \n",
      "freq       22696     10501               14976            4140        13193   \n",
      "\n",
      "         race    sex native-country income  \n",
      "count   32561  32561          32561  32561  \n",
      "unique      5      2             42      2  \n",
      "top     White   Male  United-States  <=50K  \n",
      "freq    27816  21790          29170  24720  \n",
      "\n",
      "Correlation matrix:\n",
      "\n",
      "                     age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "age             1.000000 -0.076646       0.036527      0.077674      0.057775   \n",
      "fnlwgt         -0.076646  1.000000      -0.043195      0.000432     -0.010252   \n",
      "education-num   0.036527 -0.043195       1.000000      0.122630      0.079923   \n",
      "capital-gain    0.077674  0.000432       0.122630      1.000000     -0.031615   \n",
      "capital-loss    0.057775 -0.010252       0.079923     -0.031615      1.000000   \n",
      "hours-per-week  0.068756 -0.018768       0.148123      0.078409      0.054256   \n",
      "\n",
      "                hours-per-week  \n",
      "age                   0.068756  \n",
      "fnlwgt               -0.018768  \n",
      "education-num         0.148123  \n",
      "capital-gain          0.078409  \n",
      "capital-loss          0.054256  \n",
      "hours-per-week        1.000000  \n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values per column:\n",
      "\n",
      "age               0.0\n",
      "workclass         0.0\n",
      "fnlwgt            0.0\n",
      "education         0.0\n",
      "education-num     0.0\n",
      "marital-status    0.0\n",
      "occupation        0.0\n",
      "relationship      0.0\n",
      "race              0.0\n",
      "sex               0.0\n",
      "capital-gain      0.0\n",
      "capital-loss      0.0\n",
      "hours-per-week    0.0\n",
      "native-country    0.0\n",
      "income            0.0\n",
      "dtype: float64\n",
      "\n",
      "Unique values per column:\n",
      "\n",
      "age                  73\n",
      "workclass             9\n",
      "fnlwgt            21648\n",
      "education            16\n",
      "education-num        16\n",
      "marital-status        7\n",
      "occupation           15\n",
      "relationship          6\n",
      "race                  5\n",
      "sex                   2\n",
      "capital-gain        119\n",
      "capital-loss         92\n",
      "hours-per-week       94\n",
      "native-country       42\n",
      "income                2\n",
      "dtype: int64\n",
      "\n",
      "Value counts for categorical columns:\n",
      "\n",
      "workclass:\n",
      "workclass\n",
      "Private             22696\n",
      "Self-emp-not-inc     2541\n",
      "Local-gov            2093\n",
      "?                    1836\n",
      "State-gov            1298\n",
      "Self-emp-inc         1116\n",
      "Federal-gov           960\n",
      "Without-pay            14\n",
      "Never-worked            7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "education:\n",
      "education\n",
      "HS-grad         10501\n",
      "Some-college     7291\n",
      "Bachelors        5355\n",
      "Masters          1723\n",
      "Assoc-voc        1382\n",
      "11th             1175\n",
      "Assoc-acdm       1067\n",
      "10th              933\n",
      "7th-8th           646\n",
      "Prof-school       576\n",
      "9th               514\n",
      "12th              433\n",
      "Doctorate         413\n",
      "5th-6th           333\n",
      "1st-4th           168\n",
      "Preschool          51\n",
      "Name: count, dtype: int64\n",
      "\n",
      "marital-status:\n",
      "marital-status\n",
      "Married-civ-spouse       14976\n",
      "Never-married            10683\n",
      "Divorced                  4443\n",
      "Separated                 1025\n",
      "Widowed                    993\n",
      "Married-spouse-absent      418\n",
      "Married-AF-spouse           23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "occupation:\n",
      "occupation\n",
      "Prof-specialty       4140\n",
      "Craft-repair         4099\n",
      "Exec-managerial      4066\n",
      "Adm-clerical         3770\n",
      "Sales                3650\n",
      "Other-service        3295\n",
      "Machine-op-inspct    2002\n",
      "?                    1843\n",
      "Transport-moving     1597\n",
      "Handlers-cleaners    1370\n",
      "Farming-fishing       994\n",
      "Tech-support          928\n",
      "Protective-serv       649\n",
      "Priv-house-serv       149\n",
      "Armed-Forces            9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "relationship:\n",
      "relationship\n",
      "Husband           13193\n",
      "Not-in-family      8305\n",
      "Own-child          5068\n",
      "Unmarried          3446\n",
      "Wife               1568\n",
      "Other-relative      981\n",
      "Name: count, dtype: int64\n",
      "\n",
      "race:\n",
      "race\n",
      "White                 27816\n",
      "Black                  3124\n",
      "Asian-Pac-Islander     1039\n",
      "Amer-Indian-Eskimo      311\n",
      "Other                   271\n",
      "Name: count, dtype: int64\n",
      "\n",
      "sex:\n",
      "sex\n",
      "Male      21790\n",
      "Female    10771\n",
      "Name: count, dtype: int64\n",
      "\n",
      "native-country:\n",
      "native-country\n",
      "United-States                 29170\n",
      "Mexico                          643\n",
      "?                               583\n",
      "Philippines                     198\n",
      "Germany                         137\n",
      "Canada                          121\n",
      "Puerto-Rico                     114\n",
      "El-Salvador                     106\n",
      "India                           100\n",
      "Cuba                             95\n",
      "England                          90\n",
      "Jamaica                          81\n",
      "South                            80\n",
      "China                            75\n",
      "Italy                            73\n",
      "Dominican-Republic               70\n",
      "Vietnam                          67\n",
      "Guatemala                        64\n",
      "Japan                            62\n",
      "Poland                           60\n",
      "Columbia                         59\n",
      "Taiwan                           51\n",
      "Haiti                            44\n",
      "Iran                             43\n",
      "Portugal                         37\n",
      "Nicaragua                        34\n",
      "Peru                             31\n",
      "France                           29\n",
      "Greece                           29\n",
      "Ecuador                          28\n",
      "Ireland                          24\n",
      "Hong                             20\n",
      "Trinadad&Tobago                  19\n",
      "Cambodia                         19\n",
      "Thailand                         18\n",
      "Laos                             18\n",
      "Yugoslavia                       16\n",
      "Outlying-US(Guam-USVI-etc)       14\n",
      "Honduras                         13\n",
      "Hungary                          13\n",
      "Scotland                         12\n",
      "Holand-Netherlands                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "income:\n",
      "income\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mode of categorical columns:\n",
      "\n",
      "  workclass education      marital-status      occupation relationship   race  \\\n",
      "0   Private   HS-grad  Married-civ-spouse  Prof-specialty      Husband  White   \n",
      "\n",
      "    sex native-country income  \n",
      "0  Male  United-States  <=50K  \n",
      "\n",
      "Most frequent category in categorical columns:\n",
      "\n",
      "workclass: Private, count: 22696\n",
      "education: HS-grad, count: 10501\n",
      "marital-status: Married-civ-spouse, count: 14976\n",
      "occupation: Prof-specialty, count: 4140\n",
      "relationship: Husband, count: 13193\n",
      "race: White, count: 27816\n",
      "sex: Male, count: 21790\n",
      "native-country: United-States, count: 29170\n",
      "income: <=50K, count: 24720\n",
      "\n",
      "Least frequent category in categorical columns:\n",
      "\n",
      "workclass: Never-worked, count: 7\n",
      "education: Preschool, count: 51\n",
      "marital-status: Married-AF-spouse, count: 23\n",
      "occupation: Armed-Forces, count: 9\n",
      "relationship: Other-relative, count: 981\n",
      "race: Other, count: 271\n",
      "sex: Female, count: 10771\n",
      "native-country: Holand-Netherlands, count: 1\n",
      "income: >50K, count: 7841\n",
      "\n",
      "Range of numerical columns:\n",
      "\n",
      "age: 17 - 90\n",
      "fnlwgt: 12285 - 1484705\n",
      "education-num: 1 - 16\n",
      "capital-gain: 0 - 99999\n",
      "capital-loss: 0 - 4356\n",
      "hours-per-week: 1 - 99\n",
      "\n",
      "Skewness of numerical columns:\n",
      "\n",
      "age                0.558743\n",
      "fnlwgt             1.446980\n",
      "education-num     -0.311676\n",
      "capital-gain      11.953848\n",
      "capital-loss       4.594629\n",
      "hours-per-week     0.227643\n",
      "dtype: float64\n",
      "\n",
      "Kurtosis of numerical columns:\n",
      "\n",
      "age                -0.166127\n",
      "fnlwgt              6.218811\n",
      "education-num       0.623444\n",
      "capital-gain      154.799438\n",
      "capital-loss       20.376802\n",
      "hours-per-week      2.916687\n",
      "dtype: float64\n",
      "\n",
      "Outliers in numerical columns:\n",
      "\n",
      "age: 143 outliers\n",
      "fnlwgt: 992 outliers\n",
      "education-num: 1198 outliers\n",
      "capital-gain: 2712 outliers\n",
      "capital-loss: 1519 outliers\n",
      "hours-per-week: 9008 outliers\n",
      "\n",
      "Duplicated rows:\n",
      "\n",
      "24\n",
      "\n",
      "Distribution of target variable (income):\n",
      "\n",
      "income\n",
      "<=50K    24720\n",
      ">50K      7841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def analyze_df(df, target_col=None):\n",
    "    print(f\"Data shape: {df.shape}\\n\")\n",
    "    print(\"df.info():\\n\")\n",
    "    print(df.info())\n",
    "    print(\"\\ndf.head():\\n\")\n",
    "    print(df.head())\n",
    "    print(\"\\ndf.describe():\\n\")\n",
    "    print(df.describe())\n",
    "    print(\"\\ndf.describe(include=['category', 'object']):\\n\")\n",
    "    print(df.describe(include=['category', 'object']))\n",
    "    print(\"\\nCorrelation matrix:\\n\")\n",
    "    print(df.corr(numeric_only=True))\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nMissing values per column:\\n\")\n",
    "    print(missing_values)\n",
    "    print(\"\\nPercentage of missing values per column:\\n\")\n",
    "    print(missing_values / len(df) * 100)\n",
    "    print(\"\\nUnique values per column:\\n\")\n",
    "    print(df.nunique())\n",
    "    print(\"\\nValue counts for categorical columns:\\n\")\n",
    "    for col in df.select_dtypes(include=['category', 'object']).columns:\n",
    "        print(f\"{col}:\\n{df[col].value_counts()}\\n\")\n",
    "    print(\"Mode of categorical columns:\\n\")\n",
    "    print(df.select_dtypes(include=['category', 'object']).mode())\n",
    "    print(\"\\nMost frequent category in categorical columns:\\n\")\n",
    "    for col in df.select_dtypes(include=['category', 'object']).columns:\n",
    "        print(f\"{col}: {df[col].mode().values[0]}, count: {df[col].value_counts().values[0]}\")\n",
    "    print(\"\\nLeast frequent category in categorical columns:\\n\")\n",
    "    for col in df.select_dtypes(include=['category', 'object']).columns:\n",
    "        print(f\"{col}: {df[col].value_counts().idxmin()}, count: {df[col].value_counts().values[-1]}\")\n",
    "    print(\"\\nRange of numerical columns:\\n\")\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        print(f\"{col}: {df[col].min()} - {df[col].max()}\")\n",
    "    print(\"\\nSkewness of numerical columns:\\n\")\n",
    "    print(df.select_dtypes(include=np.number).skew())\n",
    "    print(\"\\nKurtosis of numerical columns:\\n\")\n",
    "    print(df.select_dtypes(include=np.number).kurt())\n",
    "    print(\"\\nOutliers in numerical columns:\\n\")\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        print(f\"{col}: {((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()} outliers\")\n",
    "    print(\"\\nDuplicated rows:\\n\")\n",
    "    print(df.duplicated().sum())\n",
    "    if target_col:\n",
    "        print(f\"\\nDistribution of target variable ({target_col}):\\n\")\n",
    "        print(df[target_col].value_counts())\n",
    "\n",
    "analyze_df(df, target_col=\"income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 120)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n",
      "CUDA device name: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "######## Data Preparation ########\n",
    "# rename ? labels to Unknown in columns: workclass, occupation, native-country\n",
    "df.replace('?', 'Unknown', inplace=True)\n",
    "\n",
    "# drop fnlwgt\n",
    "df.drop(columns=['fnlwgt'], inplace=True)\n",
    "\n",
    "# onehot encode categorical features\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "df = pd.get_dummies(df, columns=categorical_features, dtype=int, drop_first=True)\n",
    "\n",
    "######## Data Split ########\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['income_>50K']), \n",
    "                                                    df['income_>50K'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "######## Data Normalization ########\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "######## Deeplearning Preparations ########\n",
    "# Convert to Tensor\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Available device: {device}\")\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "X_train = torch.from_numpy(X_train.values).type(torch.FloatTensor).to(device)\n",
    "X_test = torch.from_numpy(X_test.values).type(torch.FloatTensor).to(device)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.FloatTensor).to(device).reshape([-1, 1])\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.FloatTensor).to(device).reshape([-1, 1])\n",
    "\n",
    "# Create training and test sets\n",
    "training_set = list(zip(X_train, y_train))\n",
    "test_set = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build and Train the Model\n",
    "\n",
    "**Initial Model Architecture**:\n",
    "\n",
    "\n",
    "- *Input layer:* Size matches the number of features after encoding.\n",
    "- *Hidden layers:*\n",
    "  - Hidden Layer 1: 8 units, ReLU activation.\n",
    "  - Hidden Layer 2: 8 units, ReLU activation.\n",
    "- *Output layer:* 1 unit (binary classification), Sigmoid activation.\n",
    "\n",
    "\n",
    "**Loss Function and Optimizer**:\n",
    "- Loss: Binary Cross-Entropy Loss (`BCELoss`).\n",
    "- Optimizer: SGD with a learning rate of 0.001.\n",
    "\n",
    "**Training**:\n",
    "- Train the model for 20 epochs.\n",
    "- Record (store) the training and test losses at each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# hint: recall how we did this previously\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        self.fc2 = nn.Linear(8, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "input_size = len(training_set[0][0])  # 99\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 20                     # default = 20\n",
    "learning_rate = 0.001           # default = 0.001\n",
    " \n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#    initera modell, loss_function, optimizer & dataloader\n",
    "\n",
    "\n",
    "model = Net(input_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr = learning_rate)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "train_dataloader = DataLoader(training_set,                 \n",
    "                              batch_size = batch_size,       \n",
    "                              shuffle=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#    trna\n",
    "\n",
    "\n",
    "\n",
    "batch_train_losses = []\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_evaluation_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        y_true = batch[1]\n",
    "        input_features = batch[0]\n",
    "\n",
    "        y_pred=model(input_features)\n",
    "        loss=loss_function(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        batch_train_losses.append(batch_loss)\n",
    "    \n",
    "    epoch_average_loss = np.average(batch_train_losses[-len(train_dataloader):])\n",
    "\n",
    "    epoch_train_losses.append(epoch_average_loss)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#   evalueringssektion \n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = y_test\n",
    "    input_features = X_test\n",
    "    \n",
    "    y_pred = model(input_features)\n",
    "    loss = loss_function(y_pred, y_true)\n",
    "    \n",
    "    evaluation_loss = loss.item()\n",
    "    epoch_evaluation_losses.append(evaluation_loss)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#   plotta resultat \n",
    "\n",
    "plt.plot(epoch_train_losses, label = 'train loss')\n",
    "plt.plot(epoch_evaluation_losses, label = 'test loss')\n",
    "plt.legend()\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Losses')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "input_size = len(training_set[0][0])  # 99\n",
    "batch_size = 16\n",
    "\n",
    "epochs = 20                     # default = 20\n",
    "learning_rate = 0.001           # default = 0.001\n",
    " \n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#    initera modell, loss_function, optimizer & dataloader\n",
    "\n",
    "\n",
    "model = Net(input_size)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr = learning_rate)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "train_dataloader = DataLoader(training_set,                 \n",
    "                              batch_size = batch_size,       \n",
    "                              shuffle=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#    trna\n",
    "\n",
    "\n",
    "\n",
    "batch_train_losses = []\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_evaluation_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        y_true = batch[1]\n",
    "        input_features = batch[0]\n",
    "\n",
    "        y_pred=model(input_features)\n",
    "        loss=loss_function(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        batch_train_losses.append(batch_loss)\n",
    "    \n",
    "    epoch_average_loss = np.average(batch_train_losses[-len(train_dataloader):])\n",
    "\n",
    "    epoch_train_losses.append(epoch_average_loss)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#   evalueringssektion \n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_true = y_test\n",
    "    input_features = X_test\n",
    "    \n",
    "    y_pred = model(input_features)\n",
    "    loss = loss_function(y_pred, y_true)\n",
    "    \n",
    "    evaluation_loss = loss.item()\n",
    "    epoch_evaluation_losses.append(evaluation_loss)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "#   plotta resultat \n",
    "\n",
    "plt.plot(epoch_train_losses, label = 'train loss')\n",
    "plt.plot(epoch_evaluation_losses, label = 'test loss')\n",
    "plt.legend()\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Losses')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/29304 (0%)]\tLoss: 11.437766\n",
      "Train Epoch: 1 [160/29304 (1%)]\tLoss: 11.229589\n",
      "Train Epoch: 1 [320/29304 (1%)]\tLoss: 11.209473\n",
      "Train Epoch: 1 [480/29304 (2%)]\tLoss: 11.138556\n",
      "Train Epoch: 1 [640/29304 (2%)]\tLoss: 11.093992\n",
      "Train Epoch: 1 [800/29304 (3%)]\tLoss: 11.238601\n",
      "Train Epoch: 1 [960/29304 (3%)]\tLoss: 11.029280\n",
      "Train Epoch: 1 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [1280/29304 (4%)]\tLoss: 11.074173\n",
      "Train Epoch: 1 [1440/29304 (5%)]\tLoss: 11.087687\n",
      "Train Epoch: 1 [1600/29304 (5%)]\tLoss: 11.097668\n",
      "Train Epoch: 1 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [1920/29304 (7%)]\tLoss: 11.098116\n",
      "Train Epoch: 1 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [2240/29304 (8%)]\tLoss: 11.038948\n",
      "Train Epoch: 1 [2400/29304 (8%)]\tLoss: 11.169795\n",
      "Train Epoch: 1 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [2880/29304 (10%)]\tLoss: 11.115763\n",
      "Train Epoch: 1 [3040/29304 (10%)]\tLoss: 11.060958\n",
      "Train Epoch: 1 [3200/29304 (11%)]\tLoss: 11.135307\n",
      "Train Epoch: 1 [3360/29304 (11%)]\tLoss: 11.085802\n",
      "Train Epoch: 1 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [4000/29304 (14%)]\tLoss: 11.119320\n",
      "Train Epoch: 1 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [4480/29304 (15%)]\tLoss: 11.170693\n",
      "Train Epoch: 1 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [4800/29304 (16%)]\tLoss: 11.091755\n",
      "Train Epoch: 1 [4960/29304 (17%)]\tLoss: 11.088419\n",
      "Train Epoch: 1 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [6400/29304 (22%)]\tLoss: 11.095403\n",
      "Train Epoch: 1 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [6880/29304 (23%)]\tLoss: 11.092028\n",
      "Train Epoch: 1 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [7200/29304 (25%)]\tLoss: 11.108541\n",
      "Train Epoch: 1 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [7840/29304 (27%)]\tLoss: 11.094596\n",
      "Train Epoch: 1 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [10720/29304 (37%)]\tLoss: 11.089518\n",
      "Train Epoch: 1 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [11040/29304 (38%)]\tLoss: 11.088537\n",
      "Train Epoch: 1 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [16320/29304 (56%)]\tLoss: 11.052926\n",
      "Train Epoch: 1 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [18720/29304 (64%)]\tLoss: 11.079001\n",
      "Train Epoch: 1 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [24000/29304 (82%)]\tLoss: 11.090837\n",
      "Train Epoch: 1 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [26880/29304 (92%)]\tLoss: 11.144467\n",
      "Train Epoch: 1 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [27680/29304 (94%)]\tLoss: 11.062157\n",
      "Train Epoch: 1 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 1 [29120/29304 (99%)]\tLoss: 11.016953\n",
      "Train Epoch: 1 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6932, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [1920/29304 (7%)]\tLoss: 11.096218\n",
      "Train Epoch: 2 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [14880/29304 (51%)]\tLoss: 11.118672\n",
      "Train Epoch: 2 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 2 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6932, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [12000/29304 (41%)]\tLoss: 11.056710\n",
      "Train Epoch: 3 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [23520/29304 (80%)]\tLoss: 11.086505\n",
      "Train Epoch: 3 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 3 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 4 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [11040/29304 (38%)]\tLoss: 11.117405\n",
      "Train Epoch: 4 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 4 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 5 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [16480/29304 (56%)]\tLoss: 11.086886\n",
      "Train Epoch: 5 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 5 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 6 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 6 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 7 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 7 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 8 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 8 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 9 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [19200/29304 (66%)]\tLoss: 11.096537\n",
      "Train Epoch: 9 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 9 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 10 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 10 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 11 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 11 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 12 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 12 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 13 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 13 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 14 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 14 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 15 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 15 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 16 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 16 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 17 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 17 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 18 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 18 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 19 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [7840/29304 (27%)]\tLoss: 11.063959\n",
      "Train Epoch: 19 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 19 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n",
      "Train Epoch: 20 [0/29304 (0%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [160/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [320/29304 (1%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [480/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [640/29304 (2%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [800/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [960/29304 (3%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [1120/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [1280/29304 (4%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [1440/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [1600/29304 (5%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [1760/29304 (6%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [1920/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [2080/29304 (7%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [2240/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [2400/29304 (8%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [2560/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [2720/29304 (9%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [2880/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [3040/29304 (10%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [3200/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [3360/29304 (11%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [3520/29304 (12%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [3680/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [3840/29304 (13%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4000/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4160/29304 (14%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4320/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4480/29304 (15%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4640/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4800/29304 (16%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [4960/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [5120/29304 (17%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [5280/29304 (18%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [5440/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [5600/29304 (19%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [5760/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [5920/29304 (20%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [6080/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [6240/29304 (21%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [6400/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [6560/29304 (22%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [6720/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [6880/29304 (23%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [7040/29304 (24%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [7200/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [7360/29304 (25%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [7520/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [7680/29304 (26%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [7840/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8000/29304 (27%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8160/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8320/29304 (28%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8480/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8640/29304 (29%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8800/29304 (30%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [8960/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [9120/29304 (31%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [9280/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [9440/29304 (32%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [9600/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [9760/29304 (33%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [9920/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [10080/29304 (34%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [10240/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [10400/29304 (35%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [10560/29304 (36%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [10720/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [10880/29304 (37%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [11040/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [11200/29304 (38%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [11360/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [11520/29304 (39%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [11680/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [11840/29304 (40%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12000/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12160/29304 (41%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12320/29304 (42%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12480/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12640/29304 (43%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12800/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [12960/29304 (44%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [13120/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [13280/29304 (45%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [13440/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [13600/29304 (46%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [13760/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [13920/29304 (47%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [14080/29304 (48%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [14240/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [14400/29304 (49%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [14560/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [14720/29304 (50%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [14880/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [15040/29304 (51%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [15200/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [15360/29304 (52%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [15520/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [15680/29304 (53%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [15840/29304 (54%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16000/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16160/29304 (55%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16320/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16480/29304 (56%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16640/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16800/29304 (57%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [16960/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [17120/29304 (58%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [17280/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [17440/29304 (59%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [17600/29304 (60%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [17760/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [17920/29304 (61%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [18080/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [18240/29304 (62%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [18400/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [18560/29304 (63%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [18720/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [18880/29304 (64%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [19040/29304 (65%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [19200/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [19360/29304 (66%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [19520/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [19680/29304 (67%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [19840/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20000/29304 (68%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20160/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20320/29304 (69%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20480/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20640/29304 (70%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20800/29304 (71%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [20960/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [21120/29304 (72%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [21280/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [21440/29304 (73%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [21600/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [21760/29304 (74%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [21920/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [22080/29304 (75%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [22240/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [22400/29304 (76%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [22560/29304 (77%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [22720/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [22880/29304 (78%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [23040/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [23200/29304 (79%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [23360/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [23520/29304 (80%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [23680/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [23840/29304 (81%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24000/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24160/29304 (82%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24320/29304 (83%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24480/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24640/29304 (84%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24800/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [24960/29304 (85%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [25120/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [25280/29304 (86%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [25440/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [25600/29304 (87%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [25760/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [25920/29304 (88%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [26080/29304 (89%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [26240/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [26400/29304 (90%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [26560/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [26720/29304 (91%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [26880/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [27040/29304 (92%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [27200/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [27360/29304 (93%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [27520/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [27680/29304 (94%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [27840/29304 (95%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28000/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28160/29304 (96%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28320/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28480/29304 (97%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28640/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28800/29304 (98%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [28960/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [29120/29304 (99%)]\tLoss: 11.090355\n",
      "Train Epoch: 20 [29280/29304 (100%)]\tLoss: 11.090355\n",
      "\n",
      "Test set: Average loss: 0.6931, Accuracy: 2456/3257 (75%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Define Training and Testing Functions\n",
    "def train(model, device, train_loader, optimizer, epoch, train_losses):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()                  # zero the gradients\n",
    "        output = model(data)                   # forward pass\n",
    "        loss = loss_function(output, target)   # calculate loss\n",
    "        loss.backward()                        # backpropagation\n",
    "        optimizer.step()                       # update weights\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:                # log every 10 batches\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    # calculate average loss for epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, test_losses):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)                                             # forward pass\n",
    "            test_loss += loss_function(output, target).item()                # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)                        # get index of max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()            # count correct predictions\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)                                    # average loss\n",
    "    test_losses.append(test_loss)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "    return test_loss\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "input_size = len(training_set[0][0]) # 99\n",
    "batch_size = 16                      # default = 16\n",
    "num_epochs = 20                      # default = 20\n",
    "learning_rate = 0.001                # default = 0.001\n",
    "\n",
    "# Initialize the Model, Optimizer, and DataLoader\n",
    "model = Net(input_size).to(device)\n",
    "optimizer = SGD(model.parameters(), lr = learning_rate)\n",
    "loss_function = torch.nn.BCELoss(reduction='sum')\n",
    "train_dataloader = DataLoader(training_set,                 \n",
    "                              batch_size = batch_size,       \n",
    "                              shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Run Training and Testing Loops\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, device, train_dataloader, optimizer, epoch, train_losses)\n",
    "    test_loss = test(model, device, test_dataloader, test_losses)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Losses')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualize and Interpret Loss Curves\n",
    "\n",
    "**Tasks**:\n",
    "1. Plot the training and test loss curves.\n",
    "2. Answer the following questions:\n",
    "   - Is the model underfitting, overfitting, or neither? Provide evidence.\n",
    "   - If overfitting occurs, at what epoch does it begin?\n",
    "   - What can you infer about the model's performance from the loss curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Experimentation with Hyperparameters\n",
    "\n",
    "**Experiment**: Varying model capacity\n",
    "- Now train models with different hidden layer sizes (width & depth):\n",
    "  - **Smaller model:** \n",
    "    - Try architectures will LESS representation capacity than we had initially\n",
    "  - **Larger model:** \n",
    "    - Try architectures will MORE representation capacity than we had initially\n",
    "\n",
    "- Train each of those architectures, with the same initial learning rate, 20 epochs each and plot their loss curves.\n",
    "\n",
    "**Questions:**\n",
    "1. How does changing the model size affect training and test loss?\n",
    "2. What interesting findings have you found? Prepare to present and share with your classmates (just show the loss curves and explain your findings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Experimentation with learning rate\n",
    "\n",
    "\n",
    "**Experiment**: Periodic reduction of learning rate\n",
    "- During class we learned that dropping the learning rate, after a period of training, could lead to better results. Can we try to replicate that?\n",
    "\n",
    "**Question:**\n",
    "1. Did you succeed? If so, why did it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
