{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Matrix operations for a full Neural Network**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will highlight and make explicit the math for a Neural Network with three Layers.\n",
    "An input layer, a hidden layer and an output layer.\n",
    "\n",
    "Architecture:\n",
    "1. **Layer 0** : Contains **$k_0$ neurons**.\n",
    "2. **Layer 1** : Contains **$k_1$ neurons**.\n",
    "3. **Layer 2** : Contains **1 neuron**.\n",
    "\n",
    "#### General Setup\n",
    "\n",
    "Let the number of training samples be **$m$**, and the input feature size be **$n$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Layer 0: Input layer**\n",
    "\n",
    "1. **Inputs**: The input matrix $ \\mathbf{X} \\in \\mathbb{R}^{m \\times n} $, where each row corresponds to a training sample:\n",
    "   $$\n",
    "   \\mathbf{X} = \\begin{bmatrix}\n",
    "   x_{1,1} & x_{1,2} & \\dots & x_{1,n} \\\\\n",
    "   x_{2,1} & x_{2,2} & \\dots & x_{2,n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x_{m,1} & x_{m,2} & \\dots & x_{m,n}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Weights**: A weight matrix $ \\mathbf{W}^{[0]} \\in \\mathbb{R}^{k_0 \\times n} $:\n",
    "   $$\n",
    "   \\mathbf{W}^{[0]} = \\begin{bmatrix}\n",
    "   w^{[0]}_{1,1} & w^{[0]}_{1,2} & \\dots & w^{[0]}_{1,n} \\\\\n",
    "   w^{[0]}_{2,1} & w^{[0]}_{2,2} & \\dots & w^{[0]}_{2,n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   w^{[0]}_{k_0,1} & w^{[0]}_{k_0,2} & \\dots & w^{[0]}_{k_0,n}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Biases**: A bias matrix $ \\mathbf{B}^{[0]} \\in \\mathbb{R}^{m \\times k_0} $, where each row is the same:\n",
    "   $$\n",
    "   \\mathbf{B}^{[0]} = \\begin{bmatrix}\n",
    "   b^{[0]}_1 & b^{[0]}_2 & \\dots & b^{[0]}_{k_0} \\\\\n",
    "   b^{[0]}_1 & b^{[0]}_2 & \\dots & b^{[0]}_{k_0} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   b^{[0]}_1 & b^{[0]}_2 & \\dots & b^{[0]}_{k_0}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **Linear Combination**:\n",
    "   $$\n",
    "   \\mathbf{Z}^{[0]} = \\mathbf{X} \\mathbf{W}^{[0]\\top} + \\mathbf{B}^{[0]}\n",
    "   $$\n",
    "\n",
    "5. **Activation**: Apply activation function $ \\sigma $ element-wise:\n",
    "   $$\n",
    "   \\mathbf{A}^{[0]} = \\sigma(\\mathbf{Z}^{[0]})\n",
    "   $$\n",
    "   Here, $ \\mathbf{A}^{[0]} \\in \\mathbb{R}^{m \\times k_1} $ represents activations for the input layer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer 1: Hidden Layer**\n",
    "\n",
    "1. **Inputs**: The activations from Layer 0:\n",
    "   $$\n",
    "   \\mathbf{A}^{[0]} \\in \\mathbb{R}^{m \\times k_0}\n",
    "   $$\n",
    "\n",
    "2. **Weights**: A weight matrix $ \\mathbf{W}^{[1]} \\in \\mathbb{R}^{k_1 \\times k_0} $:\n",
    "   $$\n",
    "   \\mathbf{W}^{[1]} = \\begin{bmatrix}\n",
    "   w^{[1]}_{1,1} & w^{[1]}_{1,2} & \\dots & w^{[1]}_{1,k_0} \\\\\n",
    "   w^{[1]}_{2,1} & w^{[1]}_{2,2} & \\dots & w^{[1]}_{2,k_0} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   w^{[1]}_{k_1,1} & w^{[1]}_{k_1,2} & \\dots & w^{[1]}_{k_1,k_0}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Biases**: A bias matrix $ \\mathbf{B}^{[1]} \\in \\mathbb{R}^{m \\times k_1} $, where each row is the same:\n",
    "   $$\n",
    "   \\mathbf{B}^{[1]} = \\begin{bmatrix}\n",
    "   b^{[1]}_1 & b^{[1]}_2 & \\dots & b^{[1]}_{k_1} \\\\\n",
    "   b^{[1]}_1 & b^{[1]}_2 & \\dots & b^{[1]}_{k_1} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   b^{[!]}_1 & b^{[1]}_2 & \\dots & b^{[1]}_{k_1}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **Linear Combination**:\n",
    "   $$\n",
    "   \\mathbf{Z}^{[1]} = \\mathbf{A}^{[0]} \\mathbf{W}^{[1]\\top} + \\mathbf{B}^{[1]}\n",
    "   $$\n",
    "\n",
    "5. **Activation**: Apply activation function $ \\sigma $ element-wise:\n",
    "   $$\n",
    "   \\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]})\n",
    "   $$\n",
    "   Here, $ \\mathbf{A}^{[1]} \\in \\mathbb{R}^{m \\times k_1} $ represents activations for the first hidden layer.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Layer 2: Output Layer**\n",
    "\n",
    "1. **Inputs**: The activations from Layer 2:\n",
    "   $$\n",
    "   \\mathbf{A}^{[1]} \\in \\mathbb{R}^{m \\times k_1}\n",
    "   $$\n",
    "\n",
    "2. **Weights**: A weight vector $ \\mathbf{W}^{[3]} \\in \\mathbb{R}^{1 \\times k_0} $:\n",
    "   $$\n",
    "   \\mathbf{W}^{[2]} = \\begin{bmatrix}\n",
    "   w^{[2]}_{1} & w^{[2]}_{2} & \\dots & w^{[2]}_{k_0}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Bias**: A bias vector $ \\mathbf{B}^{[2]} \\in \\mathbb{R}^{m \\times 1} $, where each row is the same:\n",
    "   $$\n",
    "   \\mathbf{B}^{[2]} = \\begin{bmatrix}\n",
    "   b^{[3]} \\\\\n",
    "   b^{[3]} \\\\\n",
    "   \\vdots \\\\\n",
    "   b^{[3]}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "4. **Linear Combination**:\n",
    "   $$\n",
    "   \\mathbf{Z}^{[2]} = \\mathbf{A}^{[1]} \\mathbf{W}^{[2]\\top} + \\mathbf{B}^{[2]}\n",
    "   $$\n",
    "\n",
    "5. **Activation (Optional)**: If the output activation $ \\sigma_{\\text{out}} $ is applied (e.g., sigmoid for binary classification):\n",
    "   $$\n",
    "   \\mathbf{A}^{[2]} = \\sigma_{\\text{out}}(\\mathbf{Z}^{[2]})\n",
    "   $$\n",
    "   Here, $ \\mathbf{A}^{[2]} \\in \\mathbb{R}^{m \\times 1} $ contains the final predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "The outputs for a three-layer neural network are computed as:\n",
    "1. Layer 1:\n",
    "   $$\n",
    "   \\mathbf{Z}^{[0]} = \\mathbf{X} \\mathbf{W}^{[0]\\top} + \\mathbf{B}^{[0]}, \\quad \\mathbf{A}^{[0]} = \\sigma(\\mathbf{Z}^{[0]})\n",
    "   $$\n",
    "2. Layer 2:\n",
    "   $$\n",
    "   \\mathbf{Z}^{[1]} = \\mathbf{A}^{[0]} \\mathbf{W}^{[1]\\top} + \\mathbf{B}^{[1]}, \\quad \\mathbf{A}^{[1]} = \\sigma(\\mathbf{Z}^{[1]})\n",
    "   $$\n",
    "3. Layer 3:\n",
    "   $$\n",
    "   \\mathbf{Z}^{[2]} = \\mathbf{A}^{[1]} \\mathbf{W}^{[2]\\top} + \\mathbf{B}^{[2]}, \\quad \\mathbf{A}^{[2]} = \\sigma_{\\text{out}}(\\mathbf{Z}^{[2]})\n",
    "   $$\n",
    "\n",
    "This process demonstrates the forward propagation for a three-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
