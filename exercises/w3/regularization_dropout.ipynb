{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization & Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout_layer = nn.Dropout(p=0.2)   \n",
    "input = torch.randn(1,10)   \n",
    "output = dropout_layer(input)   \n",
    " \n",
    " \n",
    "1) Vad tror du koden gör? \n",
    "    - Instantierar en dropout-layer med en dropout-sannolikhet på 0.2.\n",
    "    - Skapar en input tensor med slumpmässiga värden.\n",
    "    - Använder dropout-layern för att slumpmässigt sätta vissa värden till 0.\n",
    " \n",
    "2) Vad får du för output om du kör koden ett par gånger? Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4683,  0.0000, -0.0930, -0.3898,  0.0000,  1.4398,  0.2452, -0.1876,\n",
      "          1.3543,  0.0000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def dropout_layer(p):   \n",
    "    dropout_layer = nn.Dropout(p=p)   \n",
    "    input = torch.randn(1,10)   \n",
    "    output = dropout_layer(input)\n",
    "    print(output)\n",
    "    print()\n",
    "\n",
    "dropout_layer(0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0982, -1.1914,  0.1600, -1.4243, -0.0000, -1.0856,  0.4940, -1.3026,\n",
      "         -0.2855, -1.4615]])\n",
      "\n",
      "tensor([[-0.5208,  1.5293,  0.0000,  0.0000, -0.0744,  0.0000, -0.6586,  0.0000,\n",
      "         -0.1313, -0.9790]])\n",
      "\n",
      "tensor([[ 1.3673,  3.0584, -0.1385, -0.0000,  0.4944,  2.2025,  0.0121,  0.5890,\n",
      "         -0.8027, -0.3626]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    dropout_layer(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Anta att vi ändrar batch_size till 3. Hur ändrar vi i koden ovan? \n",
    " \n",
    "4) Ändra batch_size till 3, och kör koden ett par gånger. Vad får du för output? Does it \n",
    "make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -8.3208e-02, -2.4334e-01, -3.1955e-01, -1.0970e+00,\n",
      "          2.5186e+00, -7.0810e-01,  1.2329e-01,  1.6666e+00, -1.3047e+00],\n",
      "        [-1.7077e-01,  1.0022e+00,  1.1543e+00,  8.8854e-04, -1.6718e+00,\n",
      "          2.0395e+00, -3.8314e-01, -0.0000e+00, -0.0000e+00, -1.3076e+00],\n",
      "        [ 1.7936e+00, -5.2125e-01, -1.8403e+00, -0.0000e+00, -0.0000e+00,\n",
      "         -4.4352e-01, -0.0000e+00, -1.8251e+00,  3.1619e-01, -0.0000e+00]])\n",
      "\n",
      "tensor([[-1.0987, -0.0000,  0.5303, -0.0000, -0.3355, -1.2541,  2.6457, -1.4698,\n",
      "         -0.0000,  0.3010],\n",
      "        [ 0.2814, -1.0495, -0.2086,  0.0000,  0.0000,  0.9369, -0.0000,  0.5031,\n",
      "         -0.1209, -1.2793],\n",
      "        [ 2.3910, -0.6372, -0.7304,  1.0292,  0.0000,  0.0000, -0.0000,  0.6349,\n",
      "          0.8173, -0.0000]])\n",
      "\n",
      "tensor([[-0.9152,  1.3163, -0.4748, -0.9594,  0.0000, -0.5086,  0.0000,  0.0000,\n",
      "          2.2922,  0.0000],\n",
      "        [-1.2689, -1.7809,  0.2919, -0.8319,  0.4700,  0.0626, -0.0000,  2.0722,\n",
      "          0.3753,  0.9772],\n",
      "        [ 0.0000, -0.4435,  0.9667, -1.9867,  0.0777, -0.0609, -1.5911,  1.9083,\n",
      "          0.1148, -1.5007]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anta att vi ändrar batch_size till 3. Hur ändrar vi i koden ovan? \n",
    "\n",
    "def dropout_layer(p):   \n",
    "    dropout_layer = nn.Dropout(p=p)   \n",
    "    input = torch.randn(3,10)    \n",
    "    output = dropout_layer(input)\n",
    "    print(output)\n",
    "    print()\n",
    "\n",
    "for i in range(3):\n",
    "    dropout_layer(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Välj nu ett enkelt träningsdataset som du jobbat med tidigare, och testa nu att träna ett Neuralt Nätverk med regularisering och/eller dropout layers.\n",
    "Försök att träna större och större nätverk och (förhoppningsvis) overfit på din data. Lägg därtil på\n",
    "1.  Regularisering\n",
    "2.  Dropout (olika lager, olika rates, testa!)\n",
    "3.  Både Regularisering och Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n",
      "CUDA device name: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "           \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "           \"hours-per-week\", \"native-country\", \"income\"]\n",
    "\n",
    "df = pd.read_csv(url, names=columns, sep=',\\s*', engine='python', na_values=\"NA\")\n",
    "\n",
    "######## Data Preparation ########\n",
    "# rename ? labels to Unknown in columns: workclass, occupation, native-country\n",
    "df.replace('?', 'Unknown', inplace=True)\n",
    "\n",
    "# drop fnlwgt\n",
    "df.drop(columns=['fnlwgt'], inplace=True)\n",
    "\n",
    "# onehot encode categorical features\n",
    "categorical_features = df.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "df = pd.get_dummies(df, columns=categorical_features, dtype=int, drop_first=True)\n",
    "\n",
    "######## Data Split ########\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['income_>50K']), \n",
    "                                                    df['income_>50K'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "######## Data Normalization ########\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "######## Deeplearning Preparations ########\n",
    "# Convert to Tensor\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Available device: {device}\")\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "X_train = torch.from_numpy(X_train.values).type(torch.FloatTensor).to(device)\n",
    "X_test = torch.from_numpy(X_test.values).type(torch.FloatTensor).to(device)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.FloatTensor).to(device).reshape([-1, 1])\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.FloatTensor).to(device).reshape([-1, 1])\n",
    "\n",
    "# Create training and test sets\n",
    "training_set = list(zip(X_train, y_train))\n",
    "test_set = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import warnings\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, input_size, device, batch_size=16, learning_rate=0.001, verbose=True):\n",
    "        self.input_size = input_size\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose  # control print statements\n",
    "        \n",
    "        # initialize model, loss function\n",
    "        self.model = Net(input_size).to(device)\n",
    "        self.loss_function = torch.nn.BCELoss()\n",
    "        self.optimizer = SGD(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # initialize storage for epoch averages of losses\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        # initialize scheduler as None\n",
    "        self.scheduler = None\n",
    "\n",
    "    def set_learning_rate(self, new_lr):\n",
    "        \"\"\"Update learning rate of the optimizer\"\"\"\n",
    "        self.learning_rate = new_lr\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "        \n",
    "    def setup_data_loaders(self, training_set, test_set):\n",
    "        self.train_dataloader = DataLoader(training_set,\n",
    "                                         batch_size=self.batch_size,\n",
    "                                         shuffle=True)\n",
    "        self.test_dataloader = DataLoader(test_set,\n",
    "                                        batch_size=len(test_set),\n",
    "                                        shuffle=False)\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        batch_losses = []\n",
    "        \n",
    "        # verbose print formatting: calculate field widths based on total values\n",
    "        total_samples = len(self.train_dataloader.dataset)\n",
    "        epoch_width = len(str(self.current_epoch + 1))\n",
    "        sample_width = len(str(total_samples))\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_dataloader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.loss_function(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            batch_losses.append(loss.item())\n",
    "            if self.verbose and batch_idx % 10 == 0:\n",
    "                samples_processed = batch_idx * len(data)\n",
    "                percentage = 100. * batch_idx / len(self.train_dataloader)\n",
    "                print(f'train epoch: {epoch:{epoch_width}} '\n",
    "                    f'[{samples_processed:>{sample_width}}/{total_samples} ({percentage:.0f}%)]\\t'\n",
    "                    f'loss: {loss.item():.6f}')\n",
    "        \n",
    "        epoch_average_loss = np.average(batch_losses)\n",
    "        self.train_losses.append(epoch_average_loss)\n",
    "        return epoch_average_loss\n",
    "    \n",
    "    def test(self, epoch):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            data, target = next(iter(self.test_dataloader))\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            output = self.model(data)\n",
    "            loss = self.loss_function(output, target)\n",
    "            test_loss = loss.item()\n",
    "\n",
    "            # calculate accuracy\n",
    "            predicted = (output > 0.5).float()  # threshold at 0.5 for binary classification\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total = target.size(0)\n",
    "            \n",
    "            self.test_losses.append(test_loss)  # save loss for this epoch\n",
    "\n",
    "            train_loss = self.train_losses[-1]  # get latest epoch average training loss\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}:')\n",
    "                print(f'Training loss: {train_loss:.4f}')\n",
    "                print(f'Test loss: {test_loss:.4f}')\n",
    "                print(f'Accuracy: {correct}/{total} ({100. * correct / total:.0f}%)\\n')\n",
    "            else:\n",
    "                print(f'[epoch {epoch}] train loss: {train_loss:.4f}, test loss: {test_loss:.4f}, '\n",
    "                    f'accuracy: {correct}/{total} ({100. * correct / total:.0f}%)')\n",
    "            return test_loss\n",
    "    \n",
    "    def train(self, training_set, test_set, num_epochs=5, gamma=None):\n",
    "        # setup data loaders\n",
    "        self.setup_data_loaders(training_set, test_set)\n",
    "\n",
    "        # initialize scheduler if gamma is provided\n",
    "        if gamma is not None:\n",
    "            self.scheduler = StepLR(self.optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "        start_epoch = self.current_epoch + 1\n",
    "        end_epoch = start_epoch + num_epochs\n",
    "        \n",
    "        # training loop\n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            test_loss = self.test(epoch)\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "            # step the scheduler if it's initialized\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "        \n",
    "        # plot results\n",
    "        self.plot_losses()\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def plot_losses(self, start_epoch=None, end_epoch=None):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # handle negative start_epoch (counting from end)\n",
    "        if start_epoch and start_epoch < 0:\n",
    "            start_epoch = self.current_epoch + start_epoch + 1\n",
    "        \n",
    "        # default to all epochs if no range specified\n",
    "        if start_epoch is None:\n",
    "            start_epoch = 1\n",
    "        if end_epoch is None:\n",
    "            end_epoch = self.current_epoch\n",
    "\n",
    "        # adjust for zero-based index\n",
    "        start_index = start_epoch - 1\n",
    "        end_index = end_epoch\n",
    "\n",
    "        epochs = range(start_epoch, end_epoch + 1)\n",
    "        train_losses = self.train_losses[start_index:end_index]\n",
    "        test_losses = self.test_losses[start_index:end_index]\n",
    "\n",
    "        plt.plot(epochs, train_losses, label='train loss')\n",
    "        plt.plot(epochs, test_losses, label='test loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title('training and test losses')\n",
    "        plt.legend()\n",
    "\n",
    "        # set y-axis to standard decimal notation\n",
    "        plt.gca().yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:.4f}'))\n",
    "        \n",
    "        # show at most 20 ticks on x-axis\n",
    "        max_ticks = 20\n",
    "        step = max(len(epochs) // max_ticks, 1)\n",
    "        plt.xticks(list(epochs)[::step])\n",
    "        \n",
    "        plt.grid(False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [    0/29304 (0%)]\tloss: 0.624635\n",
      "train epoch: 1 [  160/29304 (1%)]\tloss: 0.629120\n",
      "train epoch: 1 [  320/29304 (1%)]\tloss: 0.641982\n",
      "train epoch: 1 [  480/29304 (2%)]\tloss: 0.634948\n",
      "train epoch: 1 [  640/29304 (2%)]\tloss: 0.631564\n",
      "train epoch: 1 [  800/29304 (3%)]\tloss: 0.604651\n",
      "train epoch: 1 [  960/29304 (3%)]\tloss: 0.618536\n",
      "train epoch: 1 [ 1120/29304 (4%)]\tloss: 0.640784\n",
      "train epoch: 1 [ 1280/29304 (4%)]\tloss: 0.656767\n",
      "train epoch: 1 [ 1440/29304 (5%)]\tloss: 0.626223\n",
      "train epoch: 1 [ 1600/29304 (5%)]\tloss: 0.690504\n",
      "train epoch: 1 [ 1760/29304 (6%)]\tloss: 0.658748\n",
      "train epoch: 1 [ 1920/29304 (7%)]\tloss: 0.618562\n",
      "train epoch: 1 [ 2080/29304 (7%)]\tloss: 0.617201\n",
      "train epoch: 1 [ 2240/29304 (8%)]\tloss: 0.634695\n",
      "train epoch: 1 [ 2400/29304 (8%)]\tloss: 0.628040\n",
      "train epoch: 1 [ 2560/29304 (9%)]\tloss: 0.643579\n",
      "train epoch: 1 [ 2720/29304 (9%)]\tloss: 0.639648\n",
      "train epoch: 1 [ 2880/29304 (10%)]\tloss: 0.614537\n",
      "train epoch: 1 [ 3040/29304 (10%)]\tloss: 0.637625\n",
      "train epoch: 1 [ 3200/29304 (11%)]\tloss: 0.712701\n",
      "train epoch: 1 [ 3360/29304 (11%)]\tloss: 0.672889\n",
      "train epoch: 1 [ 3520/29304 (12%)]\tloss: 0.600319\n",
      "train epoch: 1 [ 3680/29304 (13%)]\tloss: 0.654455\n",
      "train epoch: 1 [ 3840/29304 (13%)]\tloss: 0.589142\n",
      "train epoch: 1 [ 4000/29304 (14%)]\tloss: 0.683913\n",
      "train epoch: 1 [ 4160/29304 (14%)]\tloss: 0.608639\n",
      "train epoch: 1 [ 4320/29304 (15%)]\tloss: 0.610356\n",
      "train epoch: 1 [ 4480/29304 (15%)]\tloss: 0.560045\n",
      "train epoch: 1 [ 4640/29304 (16%)]\tloss: 0.615098\n",
      "train epoch: 1 [ 4800/29304 (16%)]\tloss: 0.594815\n",
      "train epoch: 1 [ 4960/29304 (17%)]\tloss: 0.651796\n",
      "train epoch: 1 [ 5120/29304 (17%)]\tloss: 0.677837\n",
      "train epoch: 1 [ 5280/29304 (18%)]\tloss: 0.563206\n",
      "train epoch: 1 [ 5440/29304 (19%)]\tloss: 0.579714\n",
      "train epoch: 1 [ 5600/29304 (19%)]\tloss: 0.650632\n",
      "train epoch: 1 [ 5760/29304 (20%)]\tloss: 0.615052\n",
      "train epoch: 1 [ 5920/29304 (20%)]\tloss: 0.617356\n",
      "train epoch: 1 [ 6080/29304 (21%)]\tloss: 0.639682\n",
      "train epoch: 1 [ 6240/29304 (21%)]\tloss: 0.589778\n",
      "train epoch: 1 [ 6400/29304 (22%)]\tloss: 0.589119\n",
      "train epoch: 1 [ 6560/29304 (22%)]\tloss: 0.643886\n",
      "train epoch: 1 [ 6720/29304 (23%)]\tloss: 0.651140\n",
      "train epoch: 1 [ 6880/29304 (23%)]\tloss: 0.603129\n",
      "train epoch: 1 [ 7040/29304 (24%)]\tloss: 0.605664\n",
      "train epoch: 1 [ 7200/29304 (25%)]\tloss: 0.625723\n",
      "train epoch: 1 [ 7360/29304 (25%)]\tloss: 0.621177\n",
      "train epoch: 1 [ 7520/29304 (26%)]\tloss: 0.725872\n",
      "train epoch: 1 [ 7680/29304 (26%)]\tloss: 0.615355\n",
      "train epoch: 1 [ 7840/29304 (27%)]\tloss: 0.632575\n",
      "train epoch: 1 [ 8000/29304 (27%)]\tloss: 0.596229\n",
      "train epoch: 1 [ 8160/29304 (28%)]\tloss: 0.687196\n",
      "train epoch: 1 [ 8320/29304 (28%)]\tloss: 0.599595\n",
      "train epoch: 1 [ 8480/29304 (29%)]\tloss: 0.648349\n",
      "train epoch: 1 [ 8640/29304 (29%)]\tloss: 0.562269\n",
      "train epoch: 1 [ 8800/29304 (30%)]\tloss: 0.643477\n",
      "train epoch: 1 [ 8960/29304 (31%)]\tloss: 0.580599\n",
      "train epoch: 1 [ 9120/29304 (31%)]\tloss: 0.650241\n",
      "train epoch: 1 [ 9280/29304 (32%)]\tloss: 0.668075\n",
      "train epoch: 1 [ 9440/29304 (32%)]\tloss: 0.615974\n",
      "train epoch: 1 [ 9600/29304 (33%)]\tloss: 0.628075\n",
      "train epoch: 1 [ 9760/29304 (33%)]\tloss: 0.668557\n",
      "train epoch: 1 [ 9920/29304 (34%)]\tloss: 0.548491\n",
      "train epoch: 1 [10080/29304 (34%)]\tloss: 0.613755\n",
      "train epoch: 1 [10240/29304 (35%)]\tloss: 0.568398\n",
      "train epoch: 1 [10400/29304 (35%)]\tloss: 0.593172\n",
      "train epoch: 1 [10560/29304 (36%)]\tloss: 0.658710\n",
      "train epoch: 1 [10720/29304 (37%)]\tloss: 0.591656\n",
      "train epoch: 1 [10880/29304 (37%)]\tloss: 0.522090\n",
      "train epoch: 1 [11040/29304 (38%)]\tloss: 0.567614\n",
      "train epoch: 1 [11200/29304 (38%)]\tloss: 0.595661\n",
      "train epoch: 1 [11360/29304 (39%)]\tloss: 0.619233\n",
      "train epoch: 1 [11520/29304 (39%)]\tloss: 0.663789\n",
      "train epoch: 1 [11680/29304 (40%)]\tloss: 0.617350\n",
      "train epoch: 1 [11840/29304 (40%)]\tloss: 0.650887\n",
      "train epoch: 1 [12000/29304 (41%)]\tloss: 0.695426\n",
      "train epoch: 1 [12160/29304 (41%)]\tloss: 0.635260\n",
      "train epoch: 1 [12320/29304 (42%)]\tloss: 0.568043\n",
      "train epoch: 1 [12480/29304 (43%)]\tloss: 0.588857\n",
      "train epoch: 1 [12640/29304 (43%)]\tloss: 0.612821\n",
      "train epoch: 1 [12800/29304 (44%)]\tloss: 0.636563\n",
      "train epoch: 1 [12960/29304 (44%)]\tloss: 0.718150\n",
      "train epoch: 1 [13120/29304 (45%)]\tloss: 0.679112\n",
      "train epoch: 1 [13280/29304 (45%)]\tloss: 0.595231\n",
      "train epoch: 1 [13440/29304 (46%)]\tloss: 0.587875\n",
      "train epoch: 1 [13600/29304 (46%)]\tloss: 0.691876\n",
      "train epoch: 1 [13760/29304 (47%)]\tloss: 0.644340\n",
      "train epoch: 1 [13920/29304 (47%)]\tloss: 0.576258\n",
      "train epoch: 1 [14080/29304 (48%)]\tloss: 0.515412\n",
      "train epoch: 1 [14240/29304 (49%)]\tloss: 0.569522\n",
      "train epoch: 1 [14400/29304 (49%)]\tloss: 0.626526\n",
      "train epoch: 1 [14560/29304 (50%)]\tloss: 0.590427\n",
      "train epoch: 1 [14720/29304 (50%)]\tloss: 0.547269\n",
      "train epoch: 1 [14880/29304 (51%)]\tloss: 0.527387\n",
      "train epoch: 1 [15040/29304 (51%)]\tloss: 0.552620\n",
      "train epoch: 1 [15200/29304 (52%)]\tloss: 0.673138\n",
      "train epoch: 1 [15360/29304 (52%)]\tloss: 0.581874\n",
      "train epoch: 1 [15520/29304 (53%)]\tloss: 0.540561\n",
      "train epoch: 1 [15680/29304 (53%)]\tloss: 0.615667\n",
      "train epoch: 1 [15840/29304 (54%)]\tloss: 0.675501\n",
      "train epoch: 1 [16000/29304 (55%)]\tloss: 0.621722\n",
      "train epoch: 1 [16160/29304 (55%)]\tloss: 0.597161\n",
      "train epoch: 1 [16320/29304 (56%)]\tloss: 0.577235\n",
      "train epoch: 1 [16480/29304 (56%)]\tloss: 0.605614\n",
      "train epoch: 1 [16640/29304 (57%)]\tloss: 0.543349\n",
      "train epoch: 1 [16800/29304 (57%)]\tloss: 0.601848\n",
      "train epoch: 1 [16960/29304 (58%)]\tloss: 0.558814\n",
      "train epoch: 1 [17120/29304 (58%)]\tloss: 0.658467\n",
      "train epoch: 1 [17280/29304 (59%)]\tloss: 0.570438\n",
      "train epoch: 1 [17440/29304 (59%)]\tloss: 0.620704\n",
      "train epoch: 1 [17600/29304 (60%)]\tloss: 0.520758\n",
      "train epoch: 1 [17760/29304 (61%)]\tloss: 0.579517\n",
      "train epoch: 1 [17920/29304 (61%)]\tloss: 0.676768\n",
      "train epoch: 1 [18080/29304 (62%)]\tloss: 0.636268\n",
      "train epoch: 1 [18240/29304 (62%)]\tloss: 0.599920\n",
      "train epoch: 1 [18400/29304 (63%)]\tloss: 0.601633\n",
      "train epoch: 1 [18560/29304 (63%)]\tloss: 0.580285\n",
      "train epoch: 1 [18720/29304 (64%)]\tloss: 0.592568\n",
      "train epoch: 1 [18880/29304 (64%)]\tloss: 0.585460\n",
      "train epoch: 1 [19040/29304 (65%)]\tloss: 0.525884\n",
      "train epoch: 1 [19200/29304 (66%)]\tloss: 0.595934\n",
      "train epoch: 1 [19360/29304 (66%)]\tloss: 0.505251\n",
      "train epoch: 1 [19520/29304 (67%)]\tloss: 0.524059\n",
      "train epoch: 1 [19680/29304 (67%)]\tloss: 0.686028\n",
      "train epoch: 1 [19840/29304 (68%)]\tloss: 0.496385\n",
      "train epoch: 1 [20000/29304 (68%)]\tloss: 0.592239\n",
      "train epoch: 1 [20160/29304 (69%)]\tloss: 0.527600\n",
      "train epoch: 1 [20320/29304 (69%)]\tloss: 0.627213\n",
      "train epoch: 1 [20480/29304 (70%)]\tloss: 0.587286\n",
      "train epoch: 1 [20640/29304 (70%)]\tloss: 0.596391\n",
      "train epoch: 1 [20800/29304 (71%)]\tloss: 0.567107\n",
      "train epoch: 1 [20960/29304 (72%)]\tloss: 0.561227\n",
      "train epoch: 1 [21120/29304 (72%)]\tloss: 0.533780\n",
      "train epoch: 1 [21280/29304 (73%)]\tloss: 0.539124\n",
      "train epoch: 1 [21440/29304 (73%)]\tloss: 0.557418\n",
      "train epoch: 1 [21600/29304 (74%)]\tloss: 0.522767\n",
      "train epoch: 1 [21760/29304 (74%)]\tloss: 0.533237\n",
      "train epoch: 1 [21920/29304 (75%)]\tloss: 0.478941\n",
      "train epoch: 1 [22080/29304 (75%)]\tloss: 0.522214\n",
      "train epoch: 1 [22240/29304 (76%)]\tloss: 0.564563\n",
      "train epoch: 1 [22400/29304 (76%)]\tloss: 0.559588\n",
      "train epoch: 1 [22560/29304 (77%)]\tloss: 0.660203\n",
      "train epoch: 1 [22720/29304 (78%)]\tloss: 0.696386\n",
      "train epoch: 1 [22880/29304 (78%)]\tloss: 0.598231\n",
      "train epoch: 1 [23040/29304 (79%)]\tloss: 0.526859\n",
      "train epoch: 1 [23200/29304 (79%)]\tloss: 0.560296\n",
      "train epoch: 1 [23360/29304 (80%)]\tloss: 0.562248\n",
      "train epoch: 1 [23520/29304 (80%)]\tloss: 0.543688\n",
      "train epoch: 1 [23680/29304 (81%)]\tloss: 0.499694\n",
      "train epoch: 1 [23840/29304 (81%)]\tloss: 0.564772\n",
      "train epoch: 1 [24000/29304 (82%)]\tloss: 0.524703\n",
      "train epoch: 1 [24160/29304 (82%)]\tloss: 0.556428\n",
      "train epoch: 1 [24320/29304 (83%)]\tloss: 0.571381\n",
      "train epoch: 1 [24480/29304 (84%)]\tloss: 0.604093\n",
      "train epoch: 1 [24640/29304 (84%)]\tloss: 0.643811\n",
      "train epoch: 1 [24800/29304 (85%)]\tloss: 0.620869\n",
      "train epoch: 1 [24960/29304 (85%)]\tloss: 0.564342\n",
      "train epoch: 1 [25120/29304 (86%)]\tloss: 0.589145\n",
      "train epoch: 1 [25280/29304 (86%)]\tloss: 0.586761\n",
      "train epoch: 1 [25440/29304 (87%)]\tloss: 0.620342\n",
      "train epoch: 1 [25600/29304 (87%)]\tloss: 0.552569\n",
      "train epoch: 1 [25760/29304 (88%)]\tloss: 0.585521\n",
      "train epoch: 1 [25920/29304 (88%)]\tloss: 0.539981\n",
      "train epoch: 1 [26080/29304 (89%)]\tloss: 0.548440\n",
      "train epoch: 1 [26240/29304 (90%)]\tloss: 0.549014\n",
      "train epoch: 1 [26400/29304 (90%)]\tloss: 0.601839\n",
      "train epoch: 1 [26560/29304 (91%)]\tloss: 0.529893\n",
      "train epoch: 1 [26720/29304 (91%)]\tloss: 0.582141\n",
      "train epoch: 1 [26880/29304 (92%)]\tloss: 0.728460\n",
      "train epoch: 1 [27040/29304 (92%)]\tloss: 0.547781\n",
      "train epoch: 1 [27200/29304 (93%)]\tloss: 0.712724\n",
      "train epoch: 1 [27360/29304 (93%)]\tloss: 0.582198\n",
      "train epoch: 1 [27520/29304 (94%)]\tloss: 0.527589\n",
      "train epoch: 1 [27680/29304 (94%)]\tloss: 0.666365\n",
      "train epoch: 1 [27840/29304 (95%)]\tloss: 0.625580\n",
      "train epoch: 1 [28000/29304 (96%)]\tloss: 0.553755\n",
      "train epoch: 1 [28160/29304 (96%)]\tloss: 0.622792\n",
      "train epoch: 1 [28320/29304 (97%)]\tloss: 0.587920\n",
      "train epoch: 1 [28480/29304 (97%)]\tloss: 0.583731\n",
      "train epoch: 1 [28640/29304 (98%)]\tloss: 0.545316\n",
      "train epoch: 1 [28800/29304 (98%)]\tloss: 0.531128\n",
      "train epoch: 1 [28960/29304 (99%)]\tloss: 0.810923\n",
      "train epoch: 1 [29120/29304 (99%)]\tloss: 0.620862\n",
      "train epoch: 1 [29280/29304 (100%)]\tloss: 0.574428\n",
      "\n",
      "Epoch 1:\n",
      "Training loss: 0.6043\n",
      "Test loss: 0.5792\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 2 [    0/29304 (0%)]\tloss: 0.637458\n",
      "train epoch: 2 [  160/29304 (1%)]\tloss: 0.494121\n",
      "train epoch: 2 [  320/29304 (1%)]\tloss: 0.567554\n",
      "train epoch: 2 [  480/29304 (2%)]\tloss: 0.536717\n",
      "train epoch: 2 [  640/29304 (2%)]\tloss: 0.624120\n",
      "train epoch: 2 [  800/29304 (3%)]\tloss: 0.500703\n",
      "train epoch: 2 [  960/29304 (3%)]\tloss: 0.646483\n",
      "train epoch: 2 [ 1120/29304 (4%)]\tloss: 0.735330\n",
      "train epoch: 2 [ 1280/29304 (4%)]\tloss: 0.550214\n",
      "train epoch: 2 [ 1440/29304 (5%)]\tloss: 0.440287\n",
      "train epoch: 2 [ 1600/29304 (5%)]\tloss: 0.464188\n",
      "train epoch: 2 [ 1760/29304 (6%)]\tloss: 0.454622\n",
      "train epoch: 2 [ 1920/29304 (7%)]\tloss: 0.779166\n",
      "train epoch: 2 [ 2080/29304 (7%)]\tloss: 0.522192\n",
      "train epoch: 2 [ 2240/29304 (8%)]\tloss: 0.589100\n",
      "train epoch: 2 [ 2400/29304 (8%)]\tloss: 0.626990\n",
      "train epoch: 2 [ 2560/29304 (9%)]\tloss: 0.500831\n",
      "train epoch: 2 [ 2720/29304 (9%)]\tloss: 0.697759\n",
      "train epoch: 2 [ 2880/29304 (10%)]\tloss: 0.630575\n",
      "train epoch: 2 [ 3040/29304 (10%)]\tloss: 0.549725\n",
      "train epoch: 2 [ 3200/29304 (11%)]\tloss: 0.571558\n",
      "train epoch: 2 [ 3360/29304 (11%)]\tloss: 0.609175\n",
      "train epoch: 2 [ 3520/29304 (12%)]\tloss: 0.655109\n",
      "train epoch: 2 [ 3680/29304 (13%)]\tloss: 0.631638\n",
      "train epoch: 2 [ 3840/29304 (13%)]\tloss: 0.610768\n",
      "train epoch: 2 [ 4000/29304 (14%)]\tloss: 0.588981\n",
      "train epoch: 2 [ 4160/29304 (14%)]\tloss: 0.582297\n",
      "train epoch: 2 [ 4320/29304 (15%)]\tloss: 0.521122\n",
      "train epoch: 2 [ 4480/29304 (15%)]\tloss: 0.623745\n",
      "train epoch: 2 [ 4640/29304 (16%)]\tloss: 0.561545\n",
      "train epoch: 2 [ 4800/29304 (16%)]\tloss: 0.581118\n",
      "train epoch: 2 [ 4960/29304 (17%)]\tloss: 0.619095\n",
      "train epoch: 2 [ 5120/29304 (17%)]\tloss: 0.532336\n",
      "train epoch: 2 [ 5280/29304 (18%)]\tloss: 0.473541\n",
      "train epoch: 2 [ 5440/29304 (19%)]\tloss: 0.639475\n",
      "train epoch: 2 [ 5600/29304 (19%)]\tloss: 0.648466\n",
      "train epoch: 2 [ 5760/29304 (20%)]\tloss: 0.632965\n",
      "train epoch: 2 [ 5920/29304 (20%)]\tloss: 0.654716\n",
      "train epoch: 2 [ 6080/29304 (21%)]\tloss: 0.606624\n",
      "train epoch: 2 [ 6240/29304 (21%)]\tloss: 0.532649\n",
      "train epoch: 2 [ 6400/29304 (22%)]\tloss: 0.623310\n",
      "train epoch: 2 [ 6560/29304 (22%)]\tloss: 0.456030\n",
      "train epoch: 2 [ 6720/29304 (23%)]\tloss: 0.598266\n",
      "train epoch: 2 [ 6880/29304 (23%)]\tloss: 0.539031\n",
      "train epoch: 2 [ 7040/29304 (24%)]\tloss: 0.488018\n",
      "train epoch: 2 [ 7200/29304 (25%)]\tloss: 0.457340\n",
      "train epoch: 2 [ 7360/29304 (25%)]\tloss: 0.580797\n",
      "train epoch: 2 [ 7520/29304 (26%)]\tloss: 0.568741\n",
      "train epoch: 2 [ 7680/29304 (26%)]\tloss: 0.605246\n",
      "train epoch: 2 [ 7840/29304 (27%)]\tloss: 0.606311\n",
      "train epoch: 2 [ 8000/29304 (27%)]\tloss: 0.541125\n",
      "train epoch: 2 [ 8160/29304 (28%)]\tloss: 0.434563\n",
      "train epoch: 2 [ 8320/29304 (28%)]\tloss: 0.484281\n",
      "train epoch: 2 [ 8480/29304 (29%)]\tloss: 0.565189\n",
      "train epoch: 2 [ 8640/29304 (29%)]\tloss: 0.652341\n",
      "train epoch: 2 [ 8800/29304 (30%)]\tloss: 0.648346\n",
      "train epoch: 2 [ 8960/29304 (31%)]\tloss: 0.536314\n",
      "train epoch: 2 [ 9120/29304 (31%)]\tloss: 0.521194\n",
      "train epoch: 2 [ 9280/29304 (32%)]\tloss: 0.585025\n",
      "train epoch: 2 [ 9440/29304 (32%)]\tloss: 0.565288\n",
      "train epoch: 2 [ 9600/29304 (33%)]\tloss: 0.466848\n",
      "train epoch: 2 [ 9760/29304 (33%)]\tloss: 0.629982\n",
      "train epoch: 2 [ 9920/29304 (34%)]\tloss: 0.722224\n",
      "train epoch: 2 [10080/29304 (34%)]\tloss: 0.533989\n",
      "train epoch: 2 [10240/29304 (35%)]\tloss: 0.436299\n",
      "train epoch: 2 [10400/29304 (35%)]\tloss: 0.695141\n",
      "train epoch: 2 [10560/29304 (36%)]\tloss: 0.563565\n",
      "train epoch: 2 [10720/29304 (37%)]\tloss: 0.566840\n",
      "train epoch: 2 [10880/29304 (37%)]\tloss: 0.397210\n",
      "train epoch: 2 [11040/29304 (38%)]\tloss: 0.570807\n",
      "train epoch: 2 [11200/29304 (38%)]\tloss: 0.682432\n",
      "train epoch: 2 [11360/29304 (39%)]\tloss: 0.616721\n",
      "train epoch: 2 [11520/29304 (39%)]\tloss: 0.426704\n",
      "train epoch: 2 [11680/29304 (40%)]\tloss: 0.646403\n",
      "train epoch: 2 [11840/29304 (40%)]\tloss: 0.623303\n",
      "train epoch: 2 [12000/29304 (41%)]\tloss: 0.603190\n",
      "train epoch: 2 [12160/29304 (41%)]\tloss: 0.480888\n",
      "train epoch: 2 [12320/29304 (42%)]\tloss: 0.476447\n",
      "train epoch: 2 [12480/29304 (43%)]\tloss: 0.528239\n",
      "train epoch: 2 [12640/29304 (43%)]\tloss: 0.521306\n",
      "train epoch: 2 [12800/29304 (44%)]\tloss: 0.535211\n",
      "train epoch: 2 [12960/29304 (44%)]\tloss: 0.408320\n",
      "train epoch: 2 [13120/29304 (45%)]\tloss: 0.559982\n",
      "train epoch: 2 [13280/29304 (45%)]\tloss: 0.516920\n",
      "train epoch: 2 [13440/29304 (46%)]\tloss: 0.653386\n",
      "train epoch: 2 [13600/29304 (46%)]\tloss: 0.565861\n",
      "train epoch: 2 [13760/29304 (47%)]\tloss: 0.617497\n",
      "train epoch: 2 [13920/29304 (47%)]\tloss: 0.565814\n",
      "train epoch: 2 [14080/29304 (48%)]\tloss: 0.560049\n",
      "train epoch: 2 [14240/29304 (49%)]\tloss: 0.528022\n",
      "train epoch: 2 [14400/29304 (49%)]\tloss: 0.416020\n",
      "train epoch: 2 [14560/29304 (50%)]\tloss: 0.507580\n",
      "train epoch: 2 [14720/29304 (50%)]\tloss: 0.502538\n",
      "train epoch: 2 [14880/29304 (51%)]\tloss: 0.618458\n",
      "train epoch: 2 [15040/29304 (51%)]\tloss: 0.563978\n",
      "train epoch: 2 [15200/29304 (52%)]\tloss: 0.574297\n",
      "train epoch: 2 [15360/29304 (52%)]\tloss: 0.529530\n",
      "train epoch: 2 [15520/29304 (53%)]\tloss: 0.562322\n",
      "train epoch: 2 [15680/29304 (53%)]\tloss: 0.418844\n",
      "train epoch: 2 [15840/29304 (54%)]\tloss: 0.518101\n",
      "train epoch: 2 [16000/29304 (55%)]\tloss: 0.470123\n",
      "train epoch: 2 [16160/29304 (55%)]\tloss: 0.596931\n",
      "train epoch: 2 [16320/29304 (56%)]\tloss: 0.512284\n",
      "train epoch: 2 [16480/29304 (56%)]\tloss: 0.517853\n",
      "train epoch: 2 [16640/29304 (57%)]\tloss: 0.408617\n",
      "train epoch: 2 [16800/29304 (57%)]\tloss: 0.620815\n",
      "train epoch: 2 [16960/29304 (58%)]\tloss: 0.599651\n",
      "train epoch: 2 [17120/29304 (58%)]\tloss: 0.514744\n",
      "train epoch: 2 [17280/29304 (59%)]\tloss: 0.597169\n",
      "train epoch: 2 [17440/29304 (59%)]\tloss: 0.512449\n",
      "train epoch: 2 [17600/29304 (60%)]\tloss: 0.507259\n",
      "train epoch: 2 [17760/29304 (61%)]\tloss: 0.645146\n",
      "train epoch: 2 [17920/29304 (61%)]\tloss: 0.603964\n",
      "train epoch: 2 [18080/29304 (62%)]\tloss: 0.525981\n",
      "train epoch: 2 [18240/29304 (62%)]\tloss: 0.599228\n",
      "train epoch: 2 [18400/29304 (63%)]\tloss: 0.468572\n",
      "train epoch: 2 [18560/29304 (63%)]\tloss: 0.507055\n",
      "train epoch: 2 [18720/29304 (64%)]\tloss: 0.567535\n",
      "train epoch: 2 [18880/29304 (64%)]\tloss: 0.561814\n",
      "train epoch: 2 [19040/29304 (65%)]\tloss: 0.680501\n",
      "train epoch: 2 [19200/29304 (66%)]\tloss: 0.784556\n",
      "train epoch: 2 [19360/29304 (66%)]\tloss: 0.415649\n",
      "train epoch: 2 [19520/29304 (67%)]\tloss: 0.567272\n",
      "train epoch: 2 [19680/29304 (67%)]\tloss: 0.509903\n",
      "train epoch: 2 [19840/29304 (68%)]\tloss: 0.648475\n",
      "train epoch: 2 [20000/29304 (68%)]\tloss: 0.590483\n",
      "train epoch: 2 [20160/29304 (69%)]\tloss: 0.510453\n",
      "train epoch: 2 [20320/29304 (69%)]\tloss: 0.413309\n",
      "train epoch: 2 [20480/29304 (70%)]\tloss: 0.685185\n",
      "train epoch: 2 [20640/29304 (70%)]\tloss: 0.572672\n",
      "train epoch: 2 [20800/29304 (71%)]\tloss: 0.511361\n",
      "train epoch: 2 [20960/29304 (72%)]\tloss: 0.600715\n",
      "train epoch: 2 [21120/29304 (72%)]\tloss: 0.474931\n",
      "train epoch: 2 [21280/29304 (73%)]\tloss: 0.567660\n",
      "train epoch: 2 [21440/29304 (73%)]\tloss: 0.592512\n",
      "train epoch: 2 [21600/29304 (74%)]\tloss: 0.510984\n",
      "train epoch: 2 [21760/29304 (74%)]\tloss: 0.463103\n",
      "train epoch: 2 [21920/29304 (75%)]\tloss: 0.460924\n",
      "train epoch: 2 [22080/29304 (75%)]\tloss: 0.416358\n",
      "train epoch: 2 [22240/29304 (76%)]\tloss: 0.443411\n",
      "train epoch: 2 [22400/29304 (76%)]\tloss: 0.581942\n",
      "train epoch: 2 [22560/29304 (77%)]\tloss: 0.503083\n",
      "train epoch: 2 [22720/29304 (78%)]\tloss: 0.614004\n",
      "train epoch: 2 [22880/29304 (78%)]\tloss: 0.547987\n",
      "train epoch: 2 [23040/29304 (79%)]\tloss: 0.417744\n",
      "train epoch: 2 [23200/29304 (79%)]\tloss: 0.620770\n",
      "train epoch: 2 [23360/29304 (80%)]\tloss: 0.519411\n",
      "train epoch: 2 [23520/29304 (80%)]\tloss: 0.556956\n",
      "train epoch: 2 [23680/29304 (81%)]\tloss: 0.559640\n",
      "train epoch: 2 [23840/29304 (81%)]\tloss: 0.531490\n",
      "train epoch: 2 [24000/29304 (82%)]\tloss: 0.495625\n",
      "train epoch: 2 [24160/29304 (82%)]\tloss: 0.753873\n",
      "train epoch: 2 [24320/29304 (83%)]\tloss: 0.551618\n",
      "train epoch: 2 [24480/29304 (84%)]\tloss: 0.408547\n",
      "train epoch: 2 [24640/29304 (84%)]\tloss: 0.603231\n",
      "train epoch: 2 [24800/29304 (85%)]\tloss: 0.509734\n",
      "train epoch: 2 [24960/29304 (85%)]\tloss: 0.591003\n",
      "train epoch: 2 [25120/29304 (86%)]\tloss: 0.528881\n",
      "train epoch: 2 [25280/29304 (86%)]\tloss: 0.616965\n",
      "train epoch: 2 [25440/29304 (87%)]\tloss: 0.656669\n",
      "train epoch: 2 [25600/29304 (87%)]\tloss: 0.546511\n",
      "train epoch: 2 [25760/29304 (88%)]\tloss: 0.557887\n",
      "train epoch: 2 [25920/29304 (88%)]\tloss: 0.463764\n",
      "train epoch: 2 [26080/29304 (89%)]\tloss: 0.391448\n",
      "train epoch: 2 [26240/29304 (90%)]\tloss: 0.547964\n",
      "train epoch: 2 [26400/29304 (90%)]\tloss: 0.401767\n",
      "train epoch: 2 [26560/29304 (91%)]\tloss: 0.425850\n",
      "train epoch: 2 [26720/29304 (91%)]\tloss: 0.658851\n",
      "train epoch: 2 [26880/29304 (92%)]\tloss: 0.510918\n",
      "train epoch: 2 [27040/29304 (92%)]\tloss: 0.397271\n",
      "train epoch: 2 [27200/29304 (93%)]\tloss: 0.502826\n",
      "train epoch: 2 [27360/29304 (93%)]\tloss: 0.577479\n",
      "train epoch: 2 [27520/29304 (94%)]\tloss: 0.675855\n",
      "train epoch: 2 [27680/29304 (94%)]\tloss: 0.634215\n",
      "train epoch: 2 [27840/29304 (95%)]\tloss: 0.757142\n",
      "train epoch: 2 [28000/29304 (96%)]\tloss: 0.466410\n",
      "train epoch: 2 [28160/29304 (96%)]\tloss: 0.597060\n",
      "train epoch: 2 [28320/29304 (97%)]\tloss: 0.634858\n",
      "train epoch: 2 [28480/29304 (97%)]\tloss: 0.513469\n",
      "train epoch: 2 [28640/29304 (98%)]\tloss: 0.394441\n",
      "train epoch: 2 [28800/29304 (98%)]\tloss: 0.704026\n",
      "train epoch: 2 [28960/29304 (99%)]\tloss: 0.341384\n",
      "train epoch: 2 [29120/29304 (99%)]\tloss: 0.546040\n",
      "train epoch: 2 [29280/29304 (100%)]\tloss: 0.451201\n",
      "\n",
      "Epoch 2:\n",
      "Training loss: 0.5579\n",
      "Test loss: 0.5471\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 3 [    0/29304 (0%)]\tloss: 0.488656\n",
      "train epoch: 3 [  160/29304 (1%)]\tloss: 0.666312\n",
      "train epoch: 3 [  320/29304 (1%)]\tloss: 0.486749\n",
      "train epoch: 3 [  480/29304 (2%)]\tloss: 0.564657\n",
      "train epoch: 3 [  640/29304 (2%)]\tloss: 0.638588\n",
      "train epoch: 3 [  800/29304 (3%)]\tloss: 0.550994\n",
      "train epoch: 3 [  960/29304 (3%)]\tloss: 0.508440\n",
      "train epoch: 3 [ 1120/29304 (4%)]\tloss: 0.528882\n",
      "train epoch: 3 [ 1280/29304 (4%)]\tloss: 0.598811\n",
      "train epoch: 3 [ 1440/29304 (5%)]\tloss: 0.498195\n",
      "train epoch: 3 [ 1600/29304 (5%)]\tloss: 0.532307\n",
      "train epoch: 3 [ 1760/29304 (6%)]\tloss: 0.453600\n",
      "train epoch: 3 [ 1920/29304 (7%)]\tloss: 0.647280\n",
      "train epoch: 3 [ 2080/29304 (7%)]\tloss: 0.594798\n",
      "train epoch: 3 [ 2240/29304 (8%)]\tloss: 0.535612\n",
      "train epoch: 3 [ 2400/29304 (8%)]\tloss: 0.471019\n",
      "train epoch: 3 [ 2560/29304 (9%)]\tloss: 0.448448\n",
      "train epoch: 3 [ 2720/29304 (9%)]\tloss: 0.627266\n",
      "train epoch: 3 [ 2880/29304 (10%)]\tloss: 0.336558\n",
      "train epoch: 3 [ 3040/29304 (10%)]\tloss: 0.475432\n",
      "train epoch: 3 [ 3200/29304 (11%)]\tloss: 0.363741\n",
      "train epoch: 3 [ 3360/29304 (11%)]\tloss: 0.600119\n",
      "train epoch: 3 [ 3520/29304 (12%)]\tloss: 0.538290\n",
      "train epoch: 3 [ 3680/29304 (13%)]\tloss: 0.583954\n",
      "train epoch: 3 [ 3840/29304 (13%)]\tloss: 0.619774\n",
      "train epoch: 3 [ 4000/29304 (14%)]\tloss: 0.588688\n",
      "train epoch: 3 [ 4160/29304 (14%)]\tloss: 0.511570\n",
      "train epoch: 3 [ 4320/29304 (15%)]\tloss: 0.590306\n",
      "train epoch: 3 [ 4480/29304 (15%)]\tloss: 0.645953\n",
      "train epoch: 3 [ 4640/29304 (16%)]\tloss: 0.453734\n",
      "train epoch: 3 [ 4800/29304 (16%)]\tloss: 0.524142\n",
      "train epoch: 3 [ 4960/29304 (17%)]\tloss: 0.419810\n",
      "train epoch: 3 [ 5120/29304 (17%)]\tloss: 0.529809\n",
      "train epoch: 3 [ 5280/29304 (18%)]\tloss: 0.588777\n",
      "train epoch: 3 [ 5440/29304 (19%)]\tloss: 0.488164\n",
      "train epoch: 3 [ 5600/29304 (19%)]\tloss: 0.605230\n",
      "train epoch: 3 [ 5760/29304 (20%)]\tloss: 0.597769\n",
      "train epoch: 3 [ 5920/29304 (20%)]\tloss: 0.643594\n",
      "train epoch: 3 [ 6080/29304 (21%)]\tloss: 0.409854\n",
      "train epoch: 3 [ 6240/29304 (21%)]\tloss: 0.644428\n",
      "train epoch: 3 [ 6400/29304 (22%)]\tloss: 0.473535\n",
      "train epoch: 3 [ 6560/29304 (22%)]\tloss: 0.565027\n",
      "train epoch: 3 [ 6720/29304 (23%)]\tloss: 0.612659\n",
      "train epoch: 3 [ 6880/29304 (23%)]\tloss: 0.444415\n",
      "train epoch: 3 [ 7040/29304 (24%)]\tloss: 0.568881\n",
      "train epoch: 3 [ 7200/29304 (25%)]\tloss: 0.564682\n",
      "train epoch: 3 [ 7360/29304 (25%)]\tloss: 0.531368\n",
      "train epoch: 3 [ 7520/29304 (26%)]\tloss: 0.465276\n",
      "train epoch: 3 [ 7680/29304 (26%)]\tloss: 0.518566\n",
      "train epoch: 3 [ 7840/29304 (27%)]\tloss: 0.674701\n",
      "train epoch: 3 [ 8000/29304 (27%)]\tloss: 0.498997\n",
      "train epoch: 3 [ 8160/29304 (28%)]\tloss: 0.561485\n",
      "train epoch: 3 [ 8320/29304 (28%)]\tloss: 0.611682\n",
      "train epoch: 3 [ 8480/29304 (29%)]\tloss: 0.429388\n",
      "train epoch: 3 [ 8640/29304 (29%)]\tloss: 0.408206\n",
      "train epoch: 3 [ 8800/29304 (30%)]\tloss: 0.503411\n",
      "train epoch: 3 [ 8960/29304 (31%)]\tloss: 0.383915\n",
      "train epoch: 3 [ 9120/29304 (31%)]\tloss: 0.548348\n",
      "train epoch: 3 [ 9280/29304 (32%)]\tloss: 0.520151\n",
      "train epoch: 3 [ 9440/29304 (32%)]\tloss: 0.901373\n",
      "train epoch: 3 [ 9600/29304 (33%)]\tloss: 0.434695\n",
      "train epoch: 3 [ 9760/29304 (33%)]\tloss: 0.306074\n",
      "train epoch: 3 [ 9920/29304 (34%)]\tloss: 0.468159\n",
      "train epoch: 3 [10080/29304 (34%)]\tloss: 0.588685\n",
      "train epoch: 3 [10240/29304 (35%)]\tloss: 0.522714\n",
      "train epoch: 3 [10400/29304 (35%)]\tloss: 0.448203\n",
      "train epoch: 3 [10560/29304 (36%)]\tloss: 0.580738\n",
      "train epoch: 3 [10720/29304 (37%)]\tloss: 0.627810\n",
      "train epoch: 3 [10880/29304 (37%)]\tloss: 0.632211\n",
      "train epoch: 3 [11040/29304 (38%)]\tloss: 0.430022\n",
      "train epoch: 3 [11200/29304 (38%)]\tloss: 0.427710\n",
      "train epoch: 3 [11360/29304 (39%)]\tloss: 0.445796\n",
      "train epoch: 3 [11520/29304 (39%)]\tloss: 0.478317\n",
      "train epoch: 3 [11680/29304 (40%)]\tloss: 0.485727\n",
      "train epoch: 3 [11840/29304 (40%)]\tloss: 0.575971\n",
      "train epoch: 3 [12000/29304 (41%)]\tloss: 0.742276\n",
      "train epoch: 3 [12160/29304 (41%)]\tloss: 0.414882\n",
      "train epoch: 3 [12320/29304 (42%)]\tloss: 0.552125\n",
      "train epoch: 3 [12480/29304 (43%)]\tloss: 0.339837\n",
      "train epoch: 3 [12640/29304 (43%)]\tloss: 0.368368\n",
      "train epoch: 3 [12800/29304 (44%)]\tloss: 0.508996\n",
      "train epoch: 3 [12960/29304 (44%)]\tloss: 0.532542\n",
      "train epoch: 3 [13120/29304 (45%)]\tloss: 0.544118\n",
      "train epoch: 3 [13280/29304 (45%)]\tloss: 0.553821\n",
      "train epoch: 3 [13440/29304 (46%)]\tloss: 0.541335\n",
      "train epoch: 3 [13600/29304 (46%)]\tloss: 0.642672\n",
      "train epoch: 3 [13760/29304 (47%)]\tloss: 0.598980\n",
      "train epoch: 3 [13920/29304 (47%)]\tloss: 0.662037\n",
      "train epoch: 3 [14080/29304 (48%)]\tloss: 0.429739\n",
      "train epoch: 3 [14240/29304 (49%)]\tloss: 0.421611\n",
      "train epoch: 3 [14400/29304 (49%)]\tloss: 0.546582\n",
      "train epoch: 3 [14560/29304 (50%)]\tloss: 0.375694\n",
      "train epoch: 3 [14720/29304 (50%)]\tloss: 0.525340\n",
      "train epoch: 3 [14880/29304 (51%)]\tloss: 0.533243\n",
      "train epoch: 3 [15040/29304 (51%)]\tloss: 0.569924\n",
      "train epoch: 3 [15200/29304 (52%)]\tloss: 0.529072\n",
      "train epoch: 3 [15360/29304 (52%)]\tloss: 0.583843\n",
      "train epoch: 3 [15520/29304 (53%)]\tloss: 0.538893\n",
      "train epoch: 3 [15680/29304 (53%)]\tloss: 0.546729\n",
      "train epoch: 3 [15840/29304 (54%)]\tloss: 0.471030\n",
      "train epoch: 3 [16000/29304 (55%)]\tloss: 0.644895\n",
      "train epoch: 3 [16160/29304 (55%)]\tloss: 0.496739\n",
      "train epoch: 3 [16320/29304 (56%)]\tloss: 0.706316\n",
      "train epoch: 3 [16480/29304 (56%)]\tloss: 0.502488\n",
      "train epoch: 3 [16640/29304 (57%)]\tloss: 0.537732\n",
      "train epoch: 3 [16800/29304 (57%)]\tloss: 0.549816\n",
      "train epoch: 3 [16960/29304 (58%)]\tloss: 0.643025\n",
      "train epoch: 3 [17120/29304 (58%)]\tloss: 0.423857\n",
      "train epoch: 3 [17280/29304 (59%)]\tloss: 0.526283\n",
      "train epoch: 3 [17440/29304 (59%)]\tloss: 0.664396\n",
      "train epoch: 3 [17600/29304 (60%)]\tloss: 0.542680\n",
      "train epoch: 3 [17760/29304 (61%)]\tloss: 0.581970\n",
      "train epoch: 3 [17920/29304 (61%)]\tloss: 0.507064\n",
      "train epoch: 3 [18080/29304 (62%)]\tloss: 0.476313\n",
      "train epoch: 3 [18240/29304 (62%)]\tloss: 0.490315\n",
      "train epoch: 3 [18400/29304 (63%)]\tloss: 0.461739\n",
      "train epoch: 3 [18560/29304 (63%)]\tloss: 0.716079\n",
      "train epoch: 3 [18720/29304 (64%)]\tloss: 0.374485\n",
      "train epoch: 3 [18880/29304 (64%)]\tloss: 0.583510\n",
      "train epoch: 3 [19040/29304 (65%)]\tloss: 0.370449\n",
      "train epoch: 3 [19200/29304 (66%)]\tloss: 0.548403\n",
      "train epoch: 3 [19360/29304 (66%)]\tloss: 0.535341\n",
      "train epoch: 3 [19520/29304 (67%)]\tloss: 0.612976\n",
      "train epoch: 3 [19680/29304 (67%)]\tloss: 0.574197\n",
      "train epoch: 3 [19840/29304 (68%)]\tloss: 0.653122\n",
      "train epoch: 3 [20000/29304 (68%)]\tloss: 0.524538\n",
      "train epoch: 3 [20160/29304 (69%)]\tloss: 0.399159\n",
      "train epoch: 3 [20320/29304 (69%)]\tloss: 0.488759\n",
      "train epoch: 3 [20480/29304 (70%)]\tloss: 0.511292\n",
      "train epoch: 3 [20640/29304 (70%)]\tloss: 0.440125\n",
      "train epoch: 3 [20800/29304 (71%)]\tloss: 0.580752\n",
      "train epoch: 3 [20960/29304 (72%)]\tloss: 0.419048\n",
      "train epoch: 3 [21120/29304 (72%)]\tloss: 0.540069\n",
      "train epoch: 3 [21280/29304 (73%)]\tloss: 0.650768\n",
      "train epoch: 3 [21440/29304 (73%)]\tloss: 0.378148\n",
      "train epoch: 3 [21600/29304 (74%)]\tloss: 0.410638\n",
      "train epoch: 3 [21760/29304 (74%)]\tloss: 0.420389\n",
      "train epoch: 3 [21920/29304 (75%)]\tloss: 0.399566\n",
      "train epoch: 3 [22080/29304 (75%)]\tloss: 0.415380\n",
      "train epoch: 3 [22240/29304 (76%)]\tloss: 0.602886\n",
      "train epoch: 3 [22400/29304 (76%)]\tloss: 0.468832\n",
      "train epoch: 3 [22560/29304 (77%)]\tloss: 0.831075\n",
      "train epoch: 3 [22720/29304 (78%)]\tloss: 0.456908\n",
      "train epoch: 3 [22880/29304 (78%)]\tloss: 0.494396\n",
      "train epoch: 3 [23040/29304 (79%)]\tloss: 0.718317\n",
      "train epoch: 3 [23200/29304 (79%)]\tloss: 0.405686\n",
      "train epoch: 3 [23360/29304 (80%)]\tloss: 0.451499\n",
      "train epoch: 3 [23520/29304 (80%)]\tloss: 0.624244\n",
      "train epoch: 3 [23680/29304 (81%)]\tloss: 0.411513\n",
      "train epoch: 3 [23840/29304 (81%)]\tloss: 0.340280\n",
      "train epoch: 3 [24000/29304 (82%)]\tloss: 0.491042\n",
      "train epoch: 3 [24160/29304 (82%)]\tloss: 0.288847\n",
      "train epoch: 3 [24320/29304 (83%)]\tloss: 0.353310\n",
      "train epoch: 3 [24480/29304 (84%)]\tloss: 0.526448\n",
      "train epoch: 3 [24640/29304 (84%)]\tloss: 0.577780\n",
      "train epoch: 3 [24800/29304 (85%)]\tloss: 0.405326\n",
      "train epoch: 3 [24960/29304 (85%)]\tloss: 0.510571\n",
      "train epoch: 3 [25120/29304 (86%)]\tloss: 0.465882\n",
      "train epoch: 3 [25280/29304 (86%)]\tloss: 0.704600\n",
      "train epoch: 3 [25440/29304 (87%)]\tloss: 0.487835\n",
      "train epoch: 3 [25600/29304 (87%)]\tloss: 0.400069\n",
      "train epoch: 3 [25760/29304 (88%)]\tloss: 0.346241\n",
      "train epoch: 3 [25920/29304 (88%)]\tloss: 0.478484\n",
      "train epoch: 3 [26080/29304 (89%)]\tloss: 0.661797\n",
      "train epoch: 3 [26240/29304 (90%)]\tloss: 0.485841\n",
      "train epoch: 3 [26400/29304 (90%)]\tloss: 0.522248\n",
      "train epoch: 3 [26560/29304 (91%)]\tloss: 0.417750\n",
      "train epoch: 3 [26720/29304 (91%)]\tloss: 0.457105\n",
      "train epoch: 3 [26880/29304 (92%)]\tloss: 0.529576\n",
      "train epoch: 3 [27040/29304 (92%)]\tloss: 0.475662\n",
      "train epoch: 3 [27200/29304 (93%)]\tloss: 0.538570\n",
      "train epoch: 3 [27360/29304 (93%)]\tloss: 0.475950\n",
      "train epoch: 3 [27520/29304 (94%)]\tloss: 0.506582\n",
      "train epoch: 3 [27680/29304 (94%)]\tloss: 0.546328\n",
      "train epoch: 3 [27840/29304 (95%)]\tloss: 0.351422\n",
      "train epoch: 3 [28000/29304 (96%)]\tloss: 0.366405\n",
      "train epoch: 3 [28160/29304 (96%)]\tloss: 0.469402\n",
      "train epoch: 3 [28320/29304 (97%)]\tloss: 0.460567\n",
      "train epoch: 3 [28480/29304 (97%)]\tloss: 0.416979\n",
      "train epoch: 3 [28640/29304 (98%)]\tloss: 0.454962\n",
      "train epoch: 3 [28800/29304 (98%)]\tloss: 0.512326\n",
      "train epoch: 3 [28960/29304 (99%)]\tloss: 0.467015\n",
      "train epoch: 3 [29120/29304 (99%)]\tloss: 0.443542\n",
      "train epoch: 3 [29280/29304 (100%)]\tloss: 0.469607\n",
      "\n",
      "Epoch 3:\n",
      "Training loss: 0.5297\n",
      "Test loss: 0.5215\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 4 [    0/29304 (0%)]\tloss: 0.576429\n",
      "train epoch: 4 [  160/29304 (1%)]\tloss: 0.517128\n",
      "train epoch: 4 [  320/29304 (1%)]\tloss: 0.712096\n",
      "train epoch: 4 [  480/29304 (2%)]\tloss: 0.546138\n",
      "train epoch: 4 [  640/29304 (2%)]\tloss: 0.547066\n",
      "train epoch: 4 [  800/29304 (3%)]\tloss: 0.416506\n",
      "train epoch: 4 [  960/29304 (3%)]\tloss: 0.436924\n",
      "train epoch: 4 [ 1120/29304 (4%)]\tloss: 0.419242\n",
      "train epoch: 4 [ 1280/29304 (4%)]\tloss: 0.420204\n",
      "train epoch: 4 [ 1440/29304 (5%)]\tloss: 0.468047\n",
      "train epoch: 4 [ 1600/29304 (5%)]\tloss: 0.593209\n",
      "train epoch: 4 [ 1760/29304 (6%)]\tloss: 0.345050\n",
      "train epoch: 4 [ 1920/29304 (7%)]\tloss: 0.382579\n",
      "train epoch: 4 [ 2080/29304 (7%)]\tloss: 0.406581\n",
      "train epoch: 4 [ 2240/29304 (8%)]\tloss: 0.523669\n",
      "train epoch: 4 [ 2400/29304 (8%)]\tloss: 0.574348\n",
      "train epoch: 4 [ 2560/29304 (9%)]\tloss: 0.381115\n",
      "train epoch: 4 [ 2720/29304 (9%)]\tloss: 0.496888\n",
      "train epoch: 4 [ 2880/29304 (10%)]\tloss: 0.406140\n",
      "train epoch: 4 [ 3040/29304 (10%)]\tloss: 0.646816\n",
      "train epoch: 4 [ 3200/29304 (11%)]\tloss: 0.537552\n",
      "train epoch: 4 [ 3360/29304 (11%)]\tloss: 0.398131\n",
      "train epoch: 4 [ 3520/29304 (12%)]\tloss: 0.396732\n",
      "train epoch: 4 [ 3680/29304 (13%)]\tloss: 0.573361\n",
      "train epoch: 4 [ 3840/29304 (13%)]\tloss: 0.508132\n",
      "train epoch: 4 [ 4000/29304 (14%)]\tloss: 0.701087\n",
      "train epoch: 4 [ 4160/29304 (14%)]\tloss: 0.474014\n",
      "train epoch: 4 [ 4320/29304 (15%)]\tloss: 0.578752\n",
      "train epoch: 4 [ 4480/29304 (15%)]\tloss: 0.476504\n",
      "train epoch: 4 [ 4640/29304 (16%)]\tloss: 0.675017\n",
      "train epoch: 4 [ 4800/29304 (16%)]\tloss: 0.439785\n",
      "train epoch: 4 [ 4960/29304 (17%)]\tloss: 0.364926\n",
      "train epoch: 4 [ 5120/29304 (17%)]\tloss: 0.507069\n",
      "train epoch: 4 [ 5280/29304 (18%)]\tloss: 0.636026\n",
      "train epoch: 4 [ 5440/29304 (19%)]\tloss: 0.451506\n",
      "train epoch: 4 [ 5600/29304 (19%)]\tloss: 0.503481\n",
      "train epoch: 4 [ 5760/29304 (20%)]\tloss: 0.646527\n",
      "train epoch: 4 [ 5920/29304 (20%)]\tloss: 0.563503\n",
      "train epoch: 4 [ 6080/29304 (21%)]\tloss: 0.511959\n",
      "train epoch: 4 [ 6240/29304 (21%)]\tloss: 0.398270\n",
      "train epoch: 4 [ 6400/29304 (22%)]\tloss: 0.719978\n",
      "train epoch: 4 [ 6560/29304 (22%)]\tloss: 0.639908\n",
      "train epoch: 4 [ 6720/29304 (23%)]\tloss: 0.677478\n",
      "train epoch: 4 [ 6880/29304 (23%)]\tloss: 0.371790\n",
      "train epoch: 4 [ 7040/29304 (24%)]\tloss: 0.453883\n",
      "train epoch: 4 [ 7200/29304 (25%)]\tloss: 0.521927\n",
      "train epoch: 4 [ 7360/29304 (25%)]\tloss: 0.451328\n",
      "train epoch: 4 [ 7520/29304 (26%)]\tloss: 0.458135\n",
      "train epoch: 4 [ 7680/29304 (26%)]\tloss: 0.471593\n",
      "train epoch: 4 [ 7840/29304 (27%)]\tloss: 0.495898\n",
      "train epoch: 4 [ 8000/29304 (27%)]\tloss: 0.566998\n",
      "train epoch: 4 [ 8160/29304 (28%)]\tloss: 0.481991\n",
      "train epoch: 4 [ 8320/29304 (28%)]\tloss: 0.627680\n",
      "train epoch: 4 [ 8480/29304 (29%)]\tloss: 0.584272\n",
      "train epoch: 4 [ 8640/29304 (29%)]\tloss: 0.510920\n",
      "train epoch: 4 [ 8800/29304 (30%)]\tloss: 0.467287\n",
      "train epoch: 4 [ 8960/29304 (31%)]\tloss: 0.680291\n",
      "train epoch: 4 [ 9120/29304 (31%)]\tloss: 0.432700\n",
      "train epoch: 4 [ 9280/29304 (32%)]\tloss: 0.732379\n",
      "train epoch: 4 [ 9440/29304 (32%)]\tloss: 0.697429\n",
      "train epoch: 4 [ 9600/29304 (33%)]\tloss: 0.336211\n",
      "train epoch: 4 [ 9760/29304 (33%)]\tloss: 0.393764\n",
      "train epoch: 4 [ 9920/29304 (34%)]\tloss: 0.389385\n",
      "train epoch: 4 [10080/29304 (34%)]\tloss: 0.677356\n",
      "train epoch: 4 [10240/29304 (35%)]\tloss: 0.683360\n",
      "train epoch: 4 [10400/29304 (35%)]\tloss: 0.508956\n",
      "train epoch: 4 [10560/29304 (36%)]\tloss: 0.573563\n",
      "train epoch: 4 [10720/29304 (37%)]\tloss: 0.429280\n",
      "train epoch: 4 [10880/29304 (37%)]\tloss: 0.331980\n",
      "train epoch: 4 [11040/29304 (38%)]\tloss: 0.489370\n",
      "train epoch: 4 [11200/29304 (38%)]\tloss: 0.276908\n",
      "train epoch: 4 [11360/29304 (39%)]\tloss: 0.747960\n",
      "train epoch: 4 [11520/29304 (39%)]\tloss: 0.393757\n",
      "train epoch: 4 [11680/29304 (40%)]\tloss: 0.747176\n",
      "train epoch: 4 [11840/29304 (40%)]\tloss: 0.531881\n",
      "train epoch: 4 [12000/29304 (41%)]\tloss: 0.483882\n",
      "train epoch: 4 [12160/29304 (41%)]\tloss: 0.451721\n",
      "train epoch: 4 [12320/29304 (42%)]\tloss: 0.337843\n",
      "train epoch: 4 [12480/29304 (43%)]\tloss: 0.460545\n",
      "train epoch: 4 [12640/29304 (43%)]\tloss: 0.441485\n",
      "train epoch: 4 [12800/29304 (44%)]\tloss: 0.512860\n",
      "train epoch: 4 [12960/29304 (44%)]\tloss: 0.530733\n",
      "train epoch: 4 [13120/29304 (45%)]\tloss: 0.451006\n",
      "train epoch: 4 [13280/29304 (45%)]\tloss: 0.513933\n",
      "train epoch: 4 [13440/29304 (46%)]\tloss: 0.345584\n",
      "train epoch: 4 [13600/29304 (46%)]\tloss: 0.504259\n",
      "train epoch: 4 [13760/29304 (47%)]\tloss: 0.540369\n",
      "train epoch: 4 [13920/29304 (47%)]\tloss: 0.468783\n",
      "train epoch: 4 [14080/29304 (48%)]\tloss: 0.458635\n",
      "train epoch: 4 [14240/29304 (49%)]\tloss: 0.511776\n",
      "train epoch: 4 [14400/29304 (49%)]\tloss: 0.491929\n",
      "train epoch: 4 [14560/29304 (50%)]\tloss: 0.368513\n",
      "train epoch: 4 [14720/29304 (50%)]\tloss: 0.719374\n",
      "train epoch: 4 [14880/29304 (51%)]\tloss: 0.551459\n",
      "train epoch: 4 [15040/29304 (51%)]\tloss: 0.809775\n",
      "train epoch: 4 [15200/29304 (52%)]\tloss: 0.393366\n",
      "train epoch: 4 [15360/29304 (52%)]\tloss: 0.543780\n",
      "train epoch: 4 [15520/29304 (53%)]\tloss: 0.378121\n",
      "train epoch: 4 [15680/29304 (53%)]\tloss: 0.498758\n",
      "train epoch: 4 [15840/29304 (54%)]\tloss: 0.552648\n",
      "train epoch: 4 [16000/29304 (55%)]\tloss: 0.476051\n",
      "train epoch: 4 [16160/29304 (55%)]\tloss: 0.315758\n",
      "train epoch: 4 [16320/29304 (56%)]\tloss: 0.535261\n",
      "train epoch: 4 [16480/29304 (56%)]\tloss: 0.377671\n",
      "train epoch: 4 [16640/29304 (57%)]\tloss: 0.469423\n",
      "train epoch: 4 [16800/29304 (57%)]\tloss: 0.454998\n",
      "train epoch: 4 [16960/29304 (58%)]\tloss: 0.668693\n",
      "train epoch: 4 [17120/29304 (58%)]\tloss: 0.473160\n",
      "train epoch: 4 [17280/29304 (59%)]\tloss: 0.449591\n",
      "train epoch: 4 [17440/29304 (59%)]\tloss: 0.541926\n",
      "train epoch: 4 [17600/29304 (60%)]\tloss: 0.519443\n",
      "train epoch: 4 [17760/29304 (61%)]\tloss: 0.596025\n",
      "train epoch: 4 [17920/29304 (61%)]\tloss: 0.407085\n",
      "train epoch: 4 [18080/29304 (62%)]\tloss: 0.559910\n",
      "train epoch: 4 [18240/29304 (62%)]\tloss: 0.361663\n",
      "train epoch: 4 [18400/29304 (63%)]\tloss: 0.437524\n",
      "train epoch: 4 [18560/29304 (63%)]\tloss: 0.536225\n",
      "train epoch: 4 [18720/29304 (64%)]\tloss: 0.334890\n",
      "train epoch: 4 [18880/29304 (64%)]\tloss: 0.636333\n",
      "train epoch: 4 [19040/29304 (65%)]\tloss: 0.432370\n",
      "train epoch: 4 [19200/29304 (66%)]\tloss: 0.446810\n",
      "train epoch: 4 [19360/29304 (66%)]\tloss: 0.454433\n",
      "train epoch: 4 [19520/29304 (67%)]\tloss: 0.476509\n",
      "train epoch: 4 [19680/29304 (67%)]\tloss: 0.369400\n",
      "train epoch: 4 [19840/29304 (68%)]\tloss: 0.515691\n",
      "train epoch: 4 [20000/29304 (68%)]\tloss: 0.442178\n",
      "train epoch: 4 [20160/29304 (69%)]\tloss: 0.432266\n",
      "train epoch: 4 [20320/29304 (69%)]\tloss: 0.360355\n",
      "train epoch: 4 [20480/29304 (70%)]\tloss: 0.527081\n",
      "train epoch: 4 [20640/29304 (70%)]\tloss: 0.350383\n",
      "train epoch: 4 [20800/29304 (71%)]\tloss: 0.603894\n",
      "train epoch: 4 [20960/29304 (72%)]\tloss: 0.678088\n",
      "train epoch: 4 [21120/29304 (72%)]\tloss: 0.463999\n",
      "train epoch: 4 [21280/29304 (73%)]\tloss: 0.381582\n",
      "train epoch: 4 [21440/29304 (73%)]\tloss: 0.513880\n",
      "train epoch: 4 [21600/29304 (74%)]\tloss: 0.674892\n",
      "train epoch: 4 [21760/29304 (74%)]\tloss: 0.549009\n",
      "train epoch: 4 [21920/29304 (75%)]\tloss: 0.499057\n",
      "train epoch: 4 [22080/29304 (75%)]\tloss: 0.672048\n",
      "train epoch: 4 [22240/29304 (76%)]\tloss: 0.337832\n",
      "train epoch: 4 [22400/29304 (76%)]\tloss: 0.468032\n",
      "train epoch: 4 [22560/29304 (77%)]\tloss: 0.392828\n",
      "train epoch: 4 [22720/29304 (78%)]\tloss: 0.570901\n",
      "train epoch: 4 [22880/29304 (78%)]\tloss: 0.550114\n",
      "train epoch: 4 [23040/29304 (79%)]\tloss: 0.641843\n",
      "train epoch: 4 [23200/29304 (79%)]\tloss: 0.498309\n",
      "train epoch: 4 [23360/29304 (80%)]\tloss: 0.576011\n",
      "train epoch: 4 [23520/29304 (80%)]\tloss: 0.269209\n",
      "train epoch: 4 [23680/29304 (81%)]\tloss: 0.319575\n",
      "train epoch: 4 [23840/29304 (81%)]\tloss: 0.556576\n",
      "train epoch: 4 [24000/29304 (82%)]\tloss: 0.596123\n",
      "train epoch: 4 [24160/29304 (82%)]\tloss: 0.445663\n",
      "train epoch: 4 [24320/29304 (83%)]\tloss: 0.345796\n",
      "train epoch: 4 [24480/29304 (84%)]\tloss: 0.373069\n",
      "train epoch: 4 [24640/29304 (84%)]\tloss: 0.625321\n",
      "train epoch: 4 [24800/29304 (85%)]\tloss: 0.462523\n",
      "train epoch: 4 [24960/29304 (85%)]\tloss: 0.425104\n",
      "train epoch: 4 [25120/29304 (86%)]\tloss: 0.491133\n",
      "train epoch: 4 [25280/29304 (86%)]\tloss: 0.507976\n",
      "train epoch: 4 [25440/29304 (87%)]\tloss: 0.373772\n",
      "train epoch: 4 [25600/29304 (87%)]\tloss: 0.432741\n",
      "train epoch: 4 [25760/29304 (88%)]\tloss: 0.621906\n",
      "train epoch: 4 [25920/29304 (88%)]\tloss: 0.248215\n",
      "train epoch: 4 [26080/29304 (89%)]\tloss: 0.604709\n",
      "train epoch: 4 [26240/29304 (90%)]\tloss: 0.564729\n",
      "train epoch: 4 [26400/29304 (90%)]\tloss: 0.638021\n",
      "train epoch: 4 [26560/29304 (91%)]\tloss: 0.408941\n",
      "train epoch: 4 [26720/29304 (91%)]\tloss: 0.773010\n",
      "train epoch: 4 [26880/29304 (92%)]\tloss: 0.523900\n",
      "train epoch: 4 [27040/29304 (92%)]\tloss: 0.521802\n",
      "train epoch: 4 [27200/29304 (93%)]\tloss: 0.520389\n",
      "train epoch: 4 [27360/29304 (93%)]\tloss: 0.559009\n",
      "train epoch: 4 [27520/29304 (94%)]\tloss: 0.363528\n",
      "train epoch: 4 [27680/29304 (94%)]\tloss: 0.532605\n",
      "train epoch: 4 [27840/29304 (95%)]\tloss: 0.430318\n",
      "train epoch: 4 [28000/29304 (96%)]\tloss: 0.472903\n",
      "train epoch: 4 [28160/29304 (96%)]\tloss: 0.736047\n",
      "train epoch: 4 [28320/29304 (97%)]\tloss: 0.554560\n",
      "train epoch: 4 [28480/29304 (97%)]\tloss: 0.657955\n",
      "train epoch: 4 [28640/29304 (98%)]\tloss: 0.318059\n",
      "train epoch: 4 [28800/29304 (98%)]\tloss: 0.455466\n",
      "train epoch: 4 [28960/29304 (99%)]\tloss: 0.421093\n",
      "train epoch: 4 [29120/29304 (99%)]\tloss: 0.355964\n",
      "train epoch: 4 [29280/29304 (100%)]\tloss: 0.442264\n",
      "\n",
      "Epoch 4:\n",
      "Training loss: 0.5033\n",
      "Test loss: 0.4939\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 5 [    0/29304 (0%)]\tloss: 0.546292\n",
      "train epoch: 5 [  160/29304 (1%)]\tloss: 0.489437\n",
      "train epoch: 5 [  320/29304 (1%)]\tloss: 0.374523\n",
      "train epoch: 5 [  480/29304 (2%)]\tloss: 0.589588\n",
      "train epoch: 5 [  640/29304 (2%)]\tloss: 0.433867\n",
      "train epoch: 5 [  800/29304 (3%)]\tloss: 0.520661\n",
      "train epoch: 5 [  960/29304 (3%)]\tloss: 0.442914\n",
      "train epoch: 5 [ 1120/29304 (4%)]\tloss: 0.650591\n",
      "train epoch: 5 [ 1280/29304 (4%)]\tloss: 0.508760\n",
      "train epoch: 5 [ 1440/29304 (5%)]\tloss: 0.415322\n",
      "train epoch: 5 [ 1600/29304 (5%)]\tloss: 0.320116\n",
      "train epoch: 5 [ 1760/29304 (6%)]\tloss: 0.683805\n",
      "train epoch: 5 [ 1920/29304 (7%)]\tloss: 0.426692\n",
      "train epoch: 5 [ 2080/29304 (7%)]\tloss: 0.428202\n",
      "train epoch: 5 [ 2240/29304 (8%)]\tloss: 0.346722\n",
      "train epoch: 5 [ 2400/29304 (8%)]\tloss: 0.351047\n",
      "train epoch: 5 [ 2560/29304 (9%)]\tloss: 0.489081\n",
      "train epoch: 5 [ 2720/29304 (9%)]\tloss: 0.418797\n",
      "train epoch: 5 [ 2880/29304 (10%)]\tloss: 0.457504\n",
      "train epoch: 5 [ 3040/29304 (10%)]\tloss: 0.550263\n",
      "train epoch: 5 [ 3200/29304 (11%)]\tloss: 0.474561\n",
      "train epoch: 5 [ 3360/29304 (11%)]\tloss: 0.401568\n",
      "train epoch: 5 [ 3520/29304 (12%)]\tloss: 0.339369\n",
      "train epoch: 5 [ 3680/29304 (13%)]\tloss: 0.550868\n",
      "train epoch: 5 [ 3840/29304 (13%)]\tloss: 0.527193\n",
      "train epoch: 5 [ 4000/29304 (14%)]\tloss: 0.420842\n",
      "train epoch: 5 [ 4160/29304 (14%)]\tloss: 0.577485\n",
      "train epoch: 5 [ 4320/29304 (15%)]\tloss: 0.518678\n",
      "train epoch: 5 [ 4480/29304 (15%)]\tloss: 0.525719\n",
      "train epoch: 5 [ 4640/29304 (16%)]\tloss: 0.428613\n",
      "train epoch: 5 [ 4800/29304 (16%)]\tloss: 0.539444\n",
      "train epoch: 5 [ 4960/29304 (17%)]\tloss: 0.355083\n",
      "train epoch: 5 [ 5120/29304 (17%)]\tloss: 0.675723\n",
      "train epoch: 5 [ 5280/29304 (18%)]\tloss: 0.514853\n",
      "train epoch: 5 [ 5440/29304 (19%)]\tloss: 0.507596\n",
      "train epoch: 5 [ 5600/29304 (19%)]\tloss: 0.276632\n",
      "train epoch: 5 [ 5760/29304 (20%)]\tloss: 0.396765\n",
      "train epoch: 5 [ 5920/29304 (20%)]\tloss: 0.708735\n",
      "train epoch: 5 [ 6080/29304 (21%)]\tloss: 0.476915\n",
      "train epoch: 5 [ 6240/29304 (21%)]\tloss: 0.657686\n",
      "train epoch: 5 [ 6400/29304 (22%)]\tloss: 0.630350\n",
      "train epoch: 5 [ 6560/29304 (22%)]\tloss: 0.449749\n",
      "train epoch: 5 [ 6720/29304 (23%)]\tloss: 0.478358\n",
      "train epoch: 5 [ 6880/29304 (23%)]\tloss: 0.495806\n",
      "train epoch: 5 [ 7040/29304 (24%)]\tloss: 0.376884\n",
      "train epoch: 5 [ 7200/29304 (25%)]\tloss: 0.667538\n",
      "train epoch: 5 [ 7360/29304 (25%)]\tloss: 0.506808\n",
      "train epoch: 5 [ 7520/29304 (26%)]\tloss: 0.728573\n",
      "train epoch: 5 [ 7680/29304 (26%)]\tloss: 0.423537\n",
      "train epoch: 5 [ 7840/29304 (27%)]\tloss: 0.585293\n",
      "train epoch: 5 [ 8000/29304 (27%)]\tloss: 0.704701\n",
      "train epoch: 5 [ 8160/29304 (28%)]\tloss: 0.510377\n",
      "train epoch: 5 [ 8320/29304 (28%)]\tloss: 0.408671\n",
      "train epoch: 5 [ 8480/29304 (29%)]\tloss: 0.567168\n",
      "train epoch: 5 [ 8640/29304 (29%)]\tloss: 0.587987\n",
      "train epoch: 5 [ 8800/29304 (30%)]\tloss: 0.516865\n",
      "train epoch: 5 [ 8960/29304 (31%)]\tloss: 0.216629\n",
      "train epoch: 5 [ 9120/29304 (31%)]\tloss: 0.388484\n",
      "train epoch: 5 [ 9280/29304 (32%)]\tloss: 0.613177\n",
      "train epoch: 5 [ 9440/29304 (32%)]\tloss: 0.596412\n",
      "train epoch: 5 [ 9600/29304 (33%)]\tloss: 0.517845\n",
      "train epoch: 5 [ 9760/29304 (33%)]\tloss: 0.261271\n",
      "train epoch: 5 [ 9920/29304 (34%)]\tloss: 0.518243\n",
      "train epoch: 5 [10080/29304 (34%)]\tloss: 0.414638\n",
      "train epoch: 5 [10240/29304 (35%)]\tloss: 0.650808\n",
      "train epoch: 5 [10400/29304 (35%)]\tloss: 0.553862\n",
      "train epoch: 5 [10560/29304 (36%)]\tloss: 0.420861\n",
      "train epoch: 5 [10720/29304 (37%)]\tloss: 0.416400\n",
      "train epoch: 5 [10880/29304 (37%)]\tloss: 0.562135\n",
      "train epoch: 5 [11040/29304 (38%)]\tloss: 0.530341\n",
      "train epoch: 5 [11200/29304 (38%)]\tloss: 0.299776\n",
      "train epoch: 5 [11360/29304 (39%)]\tloss: 0.361315\n",
      "train epoch: 5 [11520/29304 (39%)]\tloss: 0.633409\n",
      "train epoch: 5 [11680/29304 (40%)]\tloss: 0.478711\n",
      "train epoch: 5 [11840/29304 (40%)]\tloss: 0.511626\n",
      "train epoch: 5 [12000/29304 (41%)]\tloss: 0.221894\n",
      "train epoch: 5 [12160/29304 (41%)]\tloss: 0.730272\n",
      "train epoch: 5 [12320/29304 (42%)]\tloss: 0.553846\n",
      "train epoch: 5 [12480/29304 (43%)]\tloss: 0.488467\n",
      "train epoch: 5 [12640/29304 (43%)]\tloss: 0.583755\n",
      "train epoch: 5 [12800/29304 (44%)]\tloss: 0.566150\n",
      "train epoch: 5 [12960/29304 (44%)]\tloss: 0.472322\n",
      "train epoch: 5 [13120/29304 (45%)]\tloss: 0.627793\n",
      "train epoch: 5 [13280/29304 (45%)]\tloss: 0.462290\n",
      "train epoch: 5 [13440/29304 (46%)]\tloss: 0.472931\n",
      "train epoch: 5 [13600/29304 (46%)]\tloss: 0.260755\n",
      "train epoch: 5 [13760/29304 (47%)]\tloss: 0.315494\n",
      "train epoch: 5 [13920/29304 (47%)]\tloss: 0.393144\n",
      "train epoch: 5 [14080/29304 (48%)]\tloss: 0.242521\n",
      "train epoch: 5 [14240/29304 (49%)]\tloss: 0.708791\n",
      "train epoch: 5 [14400/29304 (49%)]\tloss: 0.458597\n",
      "train epoch: 5 [14560/29304 (50%)]\tloss: 0.665480\n",
      "train epoch: 5 [14720/29304 (50%)]\tloss: 0.438619\n",
      "train epoch: 5 [14880/29304 (51%)]\tloss: 0.434636\n",
      "train epoch: 5 [15040/29304 (51%)]\tloss: 0.588964\n",
      "train epoch: 5 [15200/29304 (52%)]\tloss: 0.617246\n",
      "train epoch: 5 [15360/29304 (52%)]\tloss: 0.381169\n",
      "train epoch: 5 [15520/29304 (53%)]\tloss: 0.638204\n",
      "train epoch: 5 [15680/29304 (53%)]\tloss: 0.499038\n",
      "train epoch: 5 [15840/29304 (54%)]\tloss: 0.404762\n",
      "train epoch: 5 [16000/29304 (55%)]\tloss: 0.386569\n",
      "train epoch: 5 [16160/29304 (55%)]\tloss: 0.410818\n",
      "train epoch: 5 [16320/29304 (56%)]\tloss: 0.414208\n",
      "train epoch: 5 [16480/29304 (56%)]\tloss: 0.502421\n",
      "train epoch: 5 [16640/29304 (57%)]\tloss: 0.369560\n",
      "train epoch: 5 [16800/29304 (57%)]\tloss: 0.353836\n",
      "train epoch: 5 [16960/29304 (58%)]\tloss: 0.307016\n",
      "train epoch: 5 [17120/29304 (58%)]\tloss: 0.467490\n",
      "train epoch: 5 [17280/29304 (59%)]\tloss: 0.287251\n",
      "train epoch: 5 [17440/29304 (59%)]\tloss: 0.599609\n",
      "train epoch: 5 [17600/29304 (60%)]\tloss: 0.311255\n",
      "train epoch: 5 [17760/29304 (61%)]\tloss: 0.256568\n",
      "train epoch: 5 [17920/29304 (61%)]\tloss: 0.495845\n",
      "train epoch: 5 [18080/29304 (62%)]\tloss: 0.254068\n",
      "train epoch: 5 [18240/29304 (62%)]\tloss: 0.466233\n",
      "train epoch: 5 [18400/29304 (63%)]\tloss: 0.393393\n",
      "train epoch: 5 [18560/29304 (63%)]\tloss: 0.253716\n",
      "train epoch: 5 [18720/29304 (64%)]\tloss: 0.427703\n",
      "train epoch: 5 [18880/29304 (64%)]\tloss: 0.422290\n",
      "train epoch: 5 [19040/29304 (65%)]\tloss: 0.438753\n",
      "train epoch: 5 [19200/29304 (66%)]\tloss: 0.405535\n",
      "train epoch: 5 [19360/29304 (66%)]\tloss: 0.406182\n",
      "train epoch: 5 [19520/29304 (67%)]\tloss: 0.624240\n",
      "train epoch: 5 [19680/29304 (67%)]\tloss: 0.512688\n",
      "train epoch: 5 [19840/29304 (68%)]\tloss: 0.461188\n",
      "train epoch: 5 [20000/29304 (68%)]\tloss: 0.566953\n",
      "train epoch: 5 [20160/29304 (69%)]\tloss: 0.269212\n",
      "train epoch: 5 [20320/29304 (69%)]\tloss: 0.539046\n",
      "train epoch: 5 [20480/29304 (70%)]\tloss: 0.784062\n",
      "train epoch: 5 [20640/29304 (70%)]\tloss: 0.476284\n",
      "train epoch: 5 [20800/29304 (71%)]\tloss: 0.516834\n",
      "train epoch: 5 [20960/29304 (72%)]\tloss: 0.377534\n",
      "train epoch: 5 [21120/29304 (72%)]\tloss: 0.426644\n",
      "train epoch: 5 [21280/29304 (73%)]\tloss: 0.260532\n",
      "train epoch: 5 [21440/29304 (73%)]\tloss: 0.598424\n",
      "train epoch: 5 [21600/29304 (74%)]\tloss: 0.351082\n",
      "train epoch: 5 [21760/29304 (74%)]\tloss: 0.370308\n",
      "train epoch: 5 [21920/29304 (75%)]\tloss: 0.470940\n",
      "train epoch: 5 [22080/29304 (75%)]\tloss: 0.536636\n",
      "train epoch: 5 [22240/29304 (76%)]\tloss: 0.481482\n",
      "train epoch: 5 [22400/29304 (76%)]\tloss: 0.433158\n",
      "train epoch: 5 [22560/29304 (77%)]\tloss: 0.460200\n",
      "train epoch: 5 [22720/29304 (78%)]\tloss: 0.350908\n",
      "train epoch: 5 [22880/29304 (78%)]\tloss: 0.520418\n",
      "train epoch: 5 [23040/29304 (79%)]\tloss: 0.612161\n",
      "train epoch: 5 [23200/29304 (79%)]\tloss: 0.525794\n",
      "train epoch: 5 [23360/29304 (80%)]\tloss: 0.807719\n",
      "train epoch: 5 [23520/29304 (80%)]\tloss: 0.396906\n",
      "train epoch: 5 [23680/29304 (81%)]\tloss: 0.432037\n",
      "train epoch: 5 [23840/29304 (81%)]\tloss: 0.449377\n",
      "train epoch: 5 [24000/29304 (82%)]\tloss: 0.365934\n",
      "train epoch: 5 [24160/29304 (82%)]\tloss: 0.395491\n",
      "train epoch: 5 [24320/29304 (83%)]\tloss: 0.271533\n",
      "train epoch: 5 [24480/29304 (84%)]\tloss: 0.369501\n",
      "train epoch: 5 [24640/29304 (84%)]\tloss: 0.645652\n",
      "train epoch: 5 [24800/29304 (85%)]\tloss: 0.444279\n",
      "train epoch: 5 [24960/29304 (85%)]\tloss: 0.339984\n",
      "train epoch: 5 [25120/29304 (86%)]\tloss: 0.324938\n",
      "train epoch: 5 [25280/29304 (86%)]\tloss: 0.583029\n",
      "train epoch: 5 [25440/29304 (87%)]\tloss: 0.325897\n",
      "train epoch: 5 [25600/29304 (87%)]\tloss: 0.328901\n",
      "train epoch: 5 [25760/29304 (88%)]\tloss: 0.431670\n",
      "train epoch: 5 [25920/29304 (88%)]\tloss: 0.532038\n",
      "train epoch: 5 [26080/29304 (89%)]\tloss: 0.391408\n",
      "train epoch: 5 [26240/29304 (90%)]\tloss: 0.390946\n",
      "train epoch: 5 [26400/29304 (90%)]\tloss: 0.374338\n",
      "train epoch: 5 [26560/29304 (91%)]\tloss: 0.375866\n",
      "train epoch: 5 [26720/29304 (91%)]\tloss: 0.436594\n",
      "train epoch: 5 [26880/29304 (92%)]\tloss: 0.483007\n",
      "train epoch: 5 [27040/29304 (92%)]\tloss: 0.585664\n",
      "train epoch: 5 [27200/29304 (93%)]\tloss: 0.518045\n",
      "train epoch: 5 [27360/29304 (93%)]\tloss: 0.353396\n",
      "train epoch: 5 [27520/29304 (94%)]\tloss: 0.631738\n",
      "train epoch: 5 [27680/29304 (94%)]\tloss: 0.332381\n",
      "train epoch: 5 [27840/29304 (95%)]\tloss: 0.463996\n",
      "train epoch: 5 [28000/29304 (96%)]\tloss: 0.390228\n",
      "train epoch: 5 [28160/29304 (96%)]\tloss: 0.589174\n",
      "train epoch: 5 [28320/29304 (97%)]\tloss: 0.403125\n",
      "train epoch: 5 [28480/29304 (97%)]\tloss: 0.255545\n",
      "train epoch: 5 [28640/29304 (98%)]\tloss: 0.403979\n",
      "train epoch: 5 [28800/29304 (98%)]\tloss: 0.378395\n",
      "train epoch: 5 [28960/29304 (99%)]\tloss: 0.402486\n",
      "train epoch: 5 [29120/29304 (99%)]\tloss: 0.493397\n",
      "train epoch: 5 [29280/29304 (100%)]\tloss: 0.468536\n",
      "\n",
      "Epoch 5:\n",
      "Training loss: 0.4754\n",
      "Test loss: 0.4658\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 6 [    0/29304 (0%)]\tloss: 0.392124\n",
      "train epoch: 6 [  160/29304 (1%)]\tloss: 0.392327\n",
      "train epoch: 6 [  320/29304 (1%)]\tloss: 0.412069\n",
      "train epoch: 6 [  480/29304 (2%)]\tloss: 0.470441\n",
      "train epoch: 6 [  640/29304 (2%)]\tloss: 0.620047\n",
      "train epoch: 6 [  800/29304 (3%)]\tloss: 0.388391\n",
      "train epoch: 6 [  960/29304 (3%)]\tloss: 0.400864\n",
      "train epoch: 6 [ 1120/29304 (4%)]\tloss: 0.262819\n",
      "train epoch: 6 [ 1280/29304 (4%)]\tloss: 0.374575\n",
      "train epoch: 6 [ 1440/29304 (5%)]\tloss: 0.336369\n",
      "train epoch: 6 [ 1600/29304 (5%)]\tloss: 0.403620\n",
      "train epoch: 6 [ 1760/29304 (6%)]\tloss: 0.351067\n",
      "train epoch: 6 [ 1920/29304 (7%)]\tloss: 0.503307\n",
      "train epoch: 6 [ 2080/29304 (7%)]\tloss: 0.556789\n",
      "train epoch: 6 [ 2240/29304 (8%)]\tloss: 0.352607\n",
      "train epoch: 6 [ 2400/29304 (8%)]\tloss: 0.380361\n",
      "train epoch: 6 [ 2560/29304 (9%)]\tloss: 0.528737\n",
      "train epoch: 6 [ 2720/29304 (9%)]\tloss: 0.649406\n",
      "train epoch: 6 [ 2880/29304 (10%)]\tloss: 0.533483\n",
      "train epoch: 6 [ 3040/29304 (10%)]\tloss: 0.425757\n",
      "train epoch: 6 [ 3200/29304 (11%)]\tloss: 0.577231\n",
      "train epoch: 6 [ 3360/29304 (11%)]\tloss: 0.256560\n",
      "train epoch: 6 [ 3520/29304 (12%)]\tloss: 0.677001\n",
      "train epoch: 6 [ 3680/29304 (13%)]\tloss: 0.636152\n",
      "train epoch: 6 [ 3840/29304 (13%)]\tloss: 0.509026\n",
      "train epoch: 6 [ 4000/29304 (14%)]\tloss: 0.502963\n",
      "train epoch: 6 [ 4160/29304 (14%)]\tloss: 0.414075\n",
      "train epoch: 6 [ 4320/29304 (15%)]\tloss: 0.308911\n",
      "train epoch: 6 [ 4480/29304 (15%)]\tloss: 0.339337\n",
      "train epoch: 6 [ 4640/29304 (16%)]\tloss: 0.443031\n",
      "train epoch: 6 [ 4800/29304 (16%)]\tloss: 0.506838\n",
      "train epoch: 6 [ 4960/29304 (17%)]\tloss: 0.496444\n",
      "train epoch: 6 [ 5120/29304 (17%)]\tloss: 0.500157\n",
      "train epoch: 6 [ 5280/29304 (18%)]\tloss: 0.234334\n",
      "train epoch: 6 [ 5440/29304 (19%)]\tloss: 0.390509\n",
      "train epoch: 6 [ 5600/29304 (19%)]\tloss: 0.538951\n",
      "train epoch: 6 [ 5760/29304 (20%)]\tloss: 0.438361\n",
      "train epoch: 6 [ 5920/29304 (20%)]\tloss: 0.285471\n",
      "train epoch: 6 [ 6080/29304 (21%)]\tloss: 0.375456\n",
      "train epoch: 6 [ 6240/29304 (21%)]\tloss: 0.516648\n",
      "train epoch: 6 [ 6400/29304 (22%)]\tloss: 0.217771\n",
      "train epoch: 6 [ 6560/29304 (22%)]\tloss: 0.412580\n",
      "train epoch: 6 [ 6720/29304 (23%)]\tloss: 0.430120\n",
      "train epoch: 6 [ 6880/29304 (23%)]\tloss: 0.401424\n",
      "train epoch: 6 [ 7040/29304 (24%)]\tloss: 0.711779\n",
      "train epoch: 6 [ 7200/29304 (25%)]\tloss: 0.495960\n",
      "train epoch: 6 [ 7360/29304 (25%)]\tloss: 0.474144\n",
      "train epoch: 6 [ 7520/29304 (26%)]\tloss: 0.649328\n",
      "train epoch: 6 [ 7680/29304 (26%)]\tloss: 0.421091\n",
      "train epoch: 6 [ 7840/29304 (27%)]\tloss: 0.393200\n",
      "train epoch: 6 [ 8000/29304 (27%)]\tloss: 0.429526\n",
      "train epoch: 6 [ 8160/29304 (28%)]\tloss: 0.548706\n",
      "train epoch: 6 [ 8320/29304 (28%)]\tloss: 0.759098\n",
      "train epoch: 6 [ 8480/29304 (29%)]\tloss: 0.198536\n",
      "train epoch: 6 [ 8640/29304 (29%)]\tloss: 0.478308\n",
      "train epoch: 6 [ 8800/29304 (30%)]\tloss: 0.404524\n",
      "train epoch: 6 [ 8960/29304 (31%)]\tloss: 0.242168\n",
      "train epoch: 6 [ 9120/29304 (31%)]\tloss: 0.393065\n",
      "train epoch: 6 [ 9280/29304 (32%)]\tloss: 0.352412\n",
      "train epoch: 6 [ 9440/29304 (32%)]\tloss: 0.424394\n",
      "train epoch: 6 [ 9600/29304 (33%)]\tloss: 0.621354\n",
      "train epoch: 6 [ 9760/29304 (33%)]\tloss: 0.403221\n",
      "train epoch: 6 [ 9920/29304 (34%)]\tloss: 0.386363\n",
      "train epoch: 6 [10080/29304 (34%)]\tloss: 0.368023\n",
      "train epoch: 6 [10240/29304 (35%)]\tloss: 0.251515\n",
      "train epoch: 6 [10400/29304 (35%)]\tloss: 0.528575\n",
      "train epoch: 6 [10560/29304 (36%)]\tloss: 0.432478\n",
      "train epoch: 6 [10720/29304 (37%)]\tloss: 0.417602\n",
      "train epoch: 6 [10880/29304 (37%)]\tloss: 0.326832\n",
      "train epoch: 6 [11040/29304 (38%)]\tloss: 0.253876\n",
      "train epoch: 6 [11200/29304 (38%)]\tloss: 0.275310\n",
      "train epoch: 6 [11360/29304 (39%)]\tloss: 0.645649\n",
      "train epoch: 6 [11520/29304 (39%)]\tloss: 0.429902\n",
      "train epoch: 6 [11680/29304 (40%)]\tloss: 0.473820\n",
      "train epoch: 6 [11840/29304 (40%)]\tloss: 0.442253\n",
      "train epoch: 6 [12000/29304 (41%)]\tloss: 0.368784\n",
      "train epoch: 6 [12160/29304 (41%)]\tloss: 0.454072\n",
      "train epoch: 6 [12320/29304 (42%)]\tloss: 0.532021\n",
      "train epoch: 6 [12480/29304 (43%)]\tloss: 0.401602\n",
      "train epoch: 6 [12640/29304 (43%)]\tloss: 0.535026\n",
      "train epoch: 6 [12800/29304 (44%)]\tloss: 0.386173\n",
      "train epoch: 6 [12960/29304 (44%)]\tloss: 0.529760\n",
      "train epoch: 6 [13120/29304 (45%)]\tloss: 0.499528\n",
      "train epoch: 6 [13280/29304 (45%)]\tloss: 0.443448\n",
      "train epoch: 6 [13440/29304 (46%)]\tloss: 0.322206\n",
      "train epoch: 6 [13600/29304 (46%)]\tloss: 0.598122\n",
      "train epoch: 6 [13760/29304 (47%)]\tloss: 0.521991\n",
      "train epoch: 6 [13920/29304 (47%)]\tloss: 0.483775\n",
      "train epoch: 6 [14080/29304 (48%)]\tloss: 0.500749\n",
      "train epoch: 6 [14240/29304 (49%)]\tloss: 0.237989\n",
      "train epoch: 6 [14400/29304 (49%)]\tloss: 0.410271\n",
      "train epoch: 6 [14560/29304 (50%)]\tloss: 0.299965\n",
      "train epoch: 6 [14720/29304 (50%)]\tloss: 0.221311\n",
      "train epoch: 6 [14880/29304 (51%)]\tloss: 0.503419\n",
      "train epoch: 6 [15040/29304 (51%)]\tloss: 0.554573\n",
      "train epoch: 6 [15200/29304 (52%)]\tloss: 0.654894\n",
      "train epoch: 6 [15360/29304 (52%)]\tloss: 0.301642\n",
      "train epoch: 6 [15520/29304 (53%)]\tloss: 0.567854\n",
      "train epoch: 6 [15680/29304 (53%)]\tloss: 0.487243\n",
      "train epoch: 6 [15840/29304 (54%)]\tloss: 0.345756\n",
      "train epoch: 6 [16000/29304 (55%)]\tloss: 0.439400\n",
      "train epoch: 6 [16160/29304 (55%)]\tloss: 0.408659\n",
      "train epoch: 6 [16320/29304 (56%)]\tloss: 0.326898\n",
      "train epoch: 6 [16480/29304 (56%)]\tloss: 0.371470\n",
      "train epoch: 6 [16640/29304 (57%)]\tloss: 0.380984\n",
      "train epoch: 6 [16800/29304 (57%)]\tloss: 0.492544\n",
      "train epoch: 6 [16960/29304 (58%)]\tloss: 0.290763\n",
      "train epoch: 6 [17120/29304 (58%)]\tloss: 0.166435\n",
      "train epoch: 6 [17280/29304 (59%)]\tloss: 0.534191\n",
      "train epoch: 6 [17440/29304 (59%)]\tloss: 0.359381\n",
      "train epoch: 6 [17600/29304 (60%)]\tloss: 0.435644\n",
      "train epoch: 6 [17760/29304 (61%)]\tloss: 0.477049\n",
      "train epoch: 6 [17920/29304 (61%)]\tloss: 0.440459\n",
      "train epoch: 6 [18080/29304 (62%)]\tloss: 0.278949\n",
      "train epoch: 6 [18240/29304 (62%)]\tloss: 0.443099\n",
      "train epoch: 6 [18400/29304 (63%)]\tloss: 0.437676\n",
      "train epoch: 6 [18560/29304 (63%)]\tloss: 0.724623\n",
      "train epoch: 6 [18720/29304 (64%)]\tloss: 0.476084\n",
      "train epoch: 6 [18880/29304 (64%)]\tloss: 0.516287\n",
      "train epoch: 6 [19040/29304 (65%)]\tloss: 0.392312\n",
      "train epoch: 6 [19200/29304 (66%)]\tloss: 0.497508\n",
      "train epoch: 6 [19360/29304 (66%)]\tloss: 0.545314\n",
      "train epoch: 6 [19520/29304 (67%)]\tloss: 0.417196\n",
      "train epoch: 6 [19680/29304 (67%)]\tloss: 0.298446\n",
      "train epoch: 6 [19840/29304 (68%)]\tloss: 0.312460\n",
      "train epoch: 6 [20000/29304 (68%)]\tloss: 0.585987\n",
      "train epoch: 6 [20160/29304 (69%)]\tloss: 0.514206\n",
      "train epoch: 6 [20320/29304 (69%)]\tloss: 0.251901\n",
      "train epoch: 6 [20480/29304 (70%)]\tloss: 0.481298\n",
      "train epoch: 6 [20640/29304 (70%)]\tloss: 0.523975\n",
      "train epoch: 6 [20800/29304 (71%)]\tloss: 0.529495\n",
      "train epoch: 6 [20960/29304 (72%)]\tloss: 0.194507\n",
      "train epoch: 6 [21120/29304 (72%)]\tloss: 0.446167\n",
      "train epoch: 6 [21280/29304 (73%)]\tloss: 0.351103\n",
      "train epoch: 6 [21440/29304 (73%)]\tloss: 0.225345\n",
      "train epoch: 6 [21600/29304 (74%)]\tloss: 0.474923\n",
      "train epoch: 6 [21760/29304 (74%)]\tloss: 0.353200\n",
      "train epoch: 6 [21920/29304 (75%)]\tloss: 0.395768\n",
      "train epoch: 6 [22080/29304 (75%)]\tloss: 0.429191\n",
      "train epoch: 6 [22240/29304 (76%)]\tloss: 0.379300\n",
      "train epoch: 6 [22400/29304 (76%)]\tloss: 0.392697\n",
      "train epoch: 6 [22560/29304 (77%)]\tloss: 0.540974\n",
      "train epoch: 6 [22720/29304 (78%)]\tloss: 0.550761\n",
      "train epoch: 6 [22880/29304 (78%)]\tloss: 0.365213\n",
      "train epoch: 6 [23040/29304 (79%)]\tloss: 0.603392\n",
      "train epoch: 6 [23200/29304 (79%)]\tloss: 0.517449\n",
      "train epoch: 6 [23360/29304 (80%)]\tloss: 0.511090\n",
      "train epoch: 6 [23520/29304 (80%)]\tloss: 0.287379\n",
      "train epoch: 6 [23680/29304 (81%)]\tloss: 0.453707\n",
      "train epoch: 6 [23840/29304 (81%)]\tloss: 0.404021\n",
      "train epoch: 6 [24000/29304 (82%)]\tloss: 0.256747\n",
      "train epoch: 6 [24160/29304 (82%)]\tloss: 0.316295\n",
      "train epoch: 6 [24320/29304 (83%)]\tloss: 0.377736\n",
      "train epoch: 6 [24480/29304 (84%)]\tloss: 0.495052\n",
      "train epoch: 6 [24640/29304 (84%)]\tloss: 0.415964\n",
      "train epoch: 6 [24800/29304 (85%)]\tloss: 0.305111\n",
      "train epoch: 6 [24960/29304 (85%)]\tloss: 0.334408\n",
      "train epoch: 6 [25120/29304 (86%)]\tloss: 0.216759\n",
      "train epoch: 6 [25280/29304 (86%)]\tloss: 0.283496\n",
      "train epoch: 6 [25440/29304 (87%)]\tloss: 0.332327\n",
      "train epoch: 6 [25600/29304 (87%)]\tloss: 0.388556\n",
      "train epoch: 6 [25760/29304 (88%)]\tloss: 0.466723\n",
      "train epoch: 6 [25920/29304 (88%)]\tloss: 0.275274\n",
      "train epoch: 6 [26080/29304 (89%)]\tloss: 0.489256\n",
      "train epoch: 6 [26240/29304 (90%)]\tloss: 0.571978\n",
      "train epoch: 6 [26400/29304 (90%)]\tloss: 0.452033\n",
      "train epoch: 6 [26560/29304 (91%)]\tloss: 0.463593\n",
      "train epoch: 6 [26720/29304 (91%)]\tloss: 0.644066\n",
      "train epoch: 6 [26880/29304 (92%)]\tloss: 0.406275\n",
      "train epoch: 6 [27040/29304 (92%)]\tloss: 0.528506\n",
      "train epoch: 6 [27200/29304 (93%)]\tloss: 0.639000\n",
      "train epoch: 6 [27360/29304 (93%)]\tloss: 0.346825\n",
      "train epoch: 6 [27520/29304 (94%)]\tloss: 0.390479\n",
      "train epoch: 6 [27680/29304 (94%)]\tloss: 0.529242\n",
      "train epoch: 6 [27840/29304 (95%)]\tloss: 0.562672\n",
      "train epoch: 6 [28000/29304 (96%)]\tloss: 0.466147\n",
      "train epoch: 6 [28160/29304 (96%)]\tloss: 0.623672\n",
      "train epoch: 6 [28320/29304 (97%)]\tloss: 0.380349\n",
      "train epoch: 6 [28480/29304 (97%)]\tloss: 0.462429\n",
      "train epoch: 6 [28640/29304 (98%)]\tloss: 0.394745\n",
      "train epoch: 6 [28800/29304 (98%)]\tloss: 0.499137\n",
      "train epoch: 6 [28960/29304 (99%)]\tloss: 0.451105\n",
      "train epoch: 6 [29120/29304 (99%)]\tloss: 0.319994\n",
      "train epoch: 6 [29280/29304 (100%)]\tloss: 0.497719\n",
      "\n",
      "Epoch 6:\n",
      "Training loss: 0.4497\n",
      "Test loss: 0.4421\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 7 [    0/29304 (0%)]\tloss: 0.428757\n",
      "train epoch: 7 [  160/29304 (1%)]\tloss: 0.321096\n",
      "train epoch: 7 [  320/29304 (1%)]\tloss: 0.351649\n",
      "train epoch: 7 [  480/29304 (2%)]\tloss: 0.529647\n",
      "train epoch: 7 [  640/29304 (2%)]\tloss: 0.330132\n",
      "train epoch: 7 [  800/29304 (3%)]\tloss: 0.352880\n",
      "train epoch: 7 [  960/29304 (3%)]\tloss: 0.483949\n",
      "train epoch: 7 [ 1120/29304 (4%)]\tloss: 0.420450\n",
      "train epoch: 7 [ 1280/29304 (4%)]\tloss: 0.283033\n",
      "train epoch: 7 [ 1440/29304 (5%)]\tloss: 0.408796\n",
      "train epoch: 7 [ 1600/29304 (5%)]\tloss: 0.225888\n",
      "train epoch: 7 [ 1760/29304 (6%)]\tloss: 0.574938\n",
      "train epoch: 7 [ 1920/29304 (7%)]\tloss: 0.504512\n",
      "train epoch: 7 [ 2080/29304 (7%)]\tloss: 0.389016\n",
      "train epoch: 7 [ 2240/29304 (8%)]\tloss: 0.458391\n",
      "train epoch: 7 [ 2400/29304 (8%)]\tloss: 0.318578\n",
      "train epoch: 7 [ 2560/29304 (9%)]\tloss: 0.347026\n",
      "train epoch: 7 [ 2720/29304 (9%)]\tloss: 0.522387\n",
      "train epoch: 7 [ 2880/29304 (10%)]\tloss: 0.210212\n",
      "train epoch: 7 [ 3040/29304 (10%)]\tloss: 0.334259\n",
      "train epoch: 7 [ 3200/29304 (11%)]\tloss: 0.226480\n",
      "train epoch: 7 [ 3360/29304 (11%)]\tloss: 0.470201\n",
      "train epoch: 7 [ 3520/29304 (12%)]\tloss: 0.438001\n",
      "train epoch: 7 [ 3680/29304 (13%)]\tloss: 0.521856\n",
      "train epoch: 7 [ 3840/29304 (13%)]\tloss: 0.433190\n",
      "train epoch: 7 [ 4000/29304 (14%)]\tloss: 0.375908\n",
      "train epoch: 7 [ 4160/29304 (14%)]\tloss: 0.233627\n",
      "train epoch: 7 [ 4320/29304 (15%)]\tloss: 0.571321\n",
      "train epoch: 7 [ 4480/29304 (15%)]\tloss: 0.319621\n",
      "train epoch: 7 [ 4640/29304 (16%)]\tloss: 0.249336\n",
      "train epoch: 7 [ 4800/29304 (16%)]\tloss: 0.276028\n",
      "train epoch: 7 [ 4960/29304 (17%)]\tloss: 0.367723\n",
      "train epoch: 7 [ 5120/29304 (17%)]\tloss: 0.487141\n",
      "train epoch: 7 [ 5280/29304 (18%)]\tloss: 0.502370\n",
      "train epoch: 7 [ 5440/29304 (19%)]\tloss: 0.414804\n",
      "train epoch: 7 [ 5600/29304 (19%)]\tloss: 0.272602\n",
      "train epoch: 7 [ 5760/29304 (20%)]\tloss: 0.500533\n",
      "train epoch: 7 [ 5920/29304 (20%)]\tloss: 0.412639\n",
      "train epoch: 7 [ 6080/29304 (21%)]\tloss: 0.283947\n",
      "train epoch: 7 [ 6240/29304 (21%)]\tloss: 0.349848\n",
      "train epoch: 7 [ 6400/29304 (22%)]\tloss: 0.248598\n",
      "train epoch: 7 [ 6560/29304 (22%)]\tloss: 0.520067\n",
      "train epoch: 7 [ 6720/29304 (23%)]\tloss: 0.356332\n",
      "train epoch: 7 [ 6880/29304 (23%)]\tloss: 0.467132\n",
      "train epoch: 7 [ 7040/29304 (24%)]\tloss: 0.592649\n",
      "train epoch: 7 [ 7200/29304 (25%)]\tloss: 0.489061\n",
      "train epoch: 7 [ 7360/29304 (25%)]\tloss: 0.518706\n",
      "train epoch: 7 [ 7520/29304 (26%)]\tloss: 0.331346\n",
      "train epoch: 7 [ 7680/29304 (26%)]\tloss: 0.472693\n",
      "train epoch: 7 [ 7840/29304 (27%)]\tloss: 0.338689\n",
      "train epoch: 7 [ 8000/29304 (27%)]\tloss: 0.558120\n",
      "train epoch: 7 [ 8160/29304 (28%)]\tloss: 0.410370\n",
      "train epoch: 7 [ 8320/29304 (28%)]\tloss: 0.380425\n",
      "train epoch: 7 [ 8480/29304 (29%)]\tloss: 0.438463\n",
      "train epoch: 7 [ 8640/29304 (29%)]\tloss: 0.450208\n",
      "train epoch: 7 [ 8800/29304 (30%)]\tloss: 0.568540\n",
      "train epoch: 7 [ 8960/29304 (31%)]\tloss: 0.379357\n",
      "train epoch: 7 [ 9120/29304 (31%)]\tloss: 0.308512\n",
      "train epoch: 7 [ 9280/29304 (32%)]\tloss: 0.294950\n",
      "train epoch: 7 [ 9440/29304 (32%)]\tloss: 0.575957\n",
      "train epoch: 7 [ 9600/29304 (33%)]\tloss: 0.488253\n",
      "train epoch: 7 [ 9760/29304 (33%)]\tloss: 0.428938\n",
      "train epoch: 7 [ 9920/29304 (34%)]\tloss: 0.575247\n",
      "train epoch: 7 [10080/29304 (34%)]\tloss: 0.475657\n",
      "train epoch: 7 [10240/29304 (35%)]\tloss: 0.341502\n",
      "train epoch: 7 [10400/29304 (35%)]\tloss: 0.368256\n",
      "train epoch: 7 [10560/29304 (36%)]\tloss: 0.487237\n",
      "train epoch: 7 [10720/29304 (37%)]\tloss: 0.409897\n",
      "train epoch: 7 [10880/29304 (37%)]\tloss: 0.627434\n",
      "train epoch: 7 [11040/29304 (38%)]\tloss: 0.653252\n",
      "train epoch: 7 [11200/29304 (38%)]\tloss: 0.383901\n",
      "train epoch: 7 [11360/29304 (39%)]\tloss: 0.491735\n",
      "train epoch: 7 [11520/29304 (39%)]\tloss: 0.258151\n",
      "train epoch: 7 [11680/29304 (40%)]\tloss: 0.285789\n",
      "train epoch: 7 [11840/29304 (40%)]\tloss: 0.250496\n",
      "train epoch: 7 [12000/29304 (41%)]\tloss: 0.347882\n",
      "train epoch: 7 [12160/29304 (41%)]\tloss: 0.452133\n",
      "train epoch: 7 [12320/29304 (42%)]\tloss: 0.422343\n",
      "train epoch: 7 [12480/29304 (43%)]\tloss: 0.161868\n",
      "train epoch: 7 [12640/29304 (43%)]\tloss: 0.349807\n",
      "train epoch: 7 [12800/29304 (44%)]\tloss: 0.336903\n",
      "train epoch: 7 [12960/29304 (44%)]\tloss: 0.311566\n",
      "train epoch: 7 [13120/29304 (45%)]\tloss: 0.223561\n",
      "train epoch: 7 [13280/29304 (45%)]\tloss: 0.377912\n",
      "train epoch: 7 [13440/29304 (46%)]\tloss: 0.448679\n",
      "train epoch: 7 [13600/29304 (46%)]\tloss: 0.630607\n",
      "train epoch: 7 [13760/29304 (47%)]\tloss: 0.627478\n",
      "train epoch: 7 [13920/29304 (47%)]\tloss: 0.504676\n",
      "train epoch: 7 [14080/29304 (48%)]\tloss: 0.293507\n",
      "train epoch: 7 [14240/29304 (49%)]\tloss: 0.242702\n",
      "train epoch: 7 [14400/29304 (49%)]\tloss: 0.292214\n",
      "train epoch: 7 [14560/29304 (50%)]\tloss: 0.361244\n",
      "train epoch: 7 [14720/29304 (50%)]\tloss: 0.448436\n",
      "train epoch: 7 [14880/29304 (51%)]\tloss: 0.628090\n",
      "train epoch: 7 [15040/29304 (51%)]\tloss: 0.223653\n",
      "train epoch: 7 [15200/29304 (52%)]\tloss: 0.479339\n",
      "train epoch: 7 [15360/29304 (52%)]\tloss: 0.320968\n",
      "train epoch: 7 [15520/29304 (53%)]\tloss: 0.295300\n",
      "train epoch: 7 [15680/29304 (53%)]\tloss: 0.471114\n",
      "train epoch: 7 [15840/29304 (54%)]\tloss: 0.540787\n",
      "train epoch: 7 [16000/29304 (55%)]\tloss: 0.181131\n",
      "train epoch: 7 [16160/29304 (55%)]\tloss: 0.462909\n",
      "train epoch: 7 [16320/29304 (56%)]\tloss: 0.242353\n",
      "train epoch: 7 [16480/29304 (56%)]\tloss: 0.278053\n",
      "train epoch: 7 [16640/29304 (57%)]\tloss: 0.574769\n",
      "train epoch: 7 [16800/29304 (57%)]\tloss: 0.430083\n",
      "train epoch: 7 [16960/29304 (58%)]\tloss: 0.607048\n",
      "train epoch: 7 [17120/29304 (58%)]\tloss: 0.475156\n",
      "train epoch: 7 [17280/29304 (59%)]\tloss: 0.500636\n",
      "train epoch: 7 [17440/29304 (59%)]\tloss: 0.350987\n",
      "train epoch: 7 [17600/29304 (60%)]\tloss: 0.565884\n",
      "train epoch: 7 [17760/29304 (61%)]\tloss: 0.336331\n",
      "train epoch: 7 [17920/29304 (61%)]\tloss: 0.598358\n",
      "train epoch: 7 [18080/29304 (62%)]\tloss: 0.539469\n",
      "train epoch: 7 [18240/29304 (62%)]\tloss: 0.184368\n",
      "train epoch: 7 [18400/29304 (63%)]\tloss: 0.316084\n",
      "train epoch: 7 [18560/29304 (63%)]\tloss: 0.588225\n",
      "train epoch: 7 [18720/29304 (64%)]\tloss: 0.604960\n",
      "train epoch: 7 [18880/29304 (64%)]\tloss: 0.331966\n",
      "train epoch: 7 [19040/29304 (65%)]\tloss: 0.569108\n",
      "train epoch: 7 [19200/29304 (66%)]\tloss: 0.246183\n",
      "train epoch: 7 [19360/29304 (66%)]\tloss: 0.553668\n",
      "train epoch: 7 [19520/29304 (67%)]\tloss: 0.452483\n",
      "train epoch: 7 [19680/29304 (67%)]\tloss: 0.389020\n",
      "train epoch: 7 [19840/29304 (68%)]\tloss: 0.370995\n",
      "train epoch: 7 [20000/29304 (68%)]\tloss: 0.301437\n",
      "train epoch: 7 [20160/29304 (69%)]\tloss: 0.347890\n",
      "train epoch: 7 [20320/29304 (69%)]\tloss: 0.502263\n",
      "train epoch: 7 [20480/29304 (70%)]\tloss: 0.447818\n",
      "train epoch: 7 [20640/29304 (70%)]\tloss: 0.431416\n",
      "train epoch: 7 [20800/29304 (71%)]\tloss: 0.473164\n",
      "train epoch: 7 [20960/29304 (72%)]\tloss: 0.311997\n",
      "train epoch: 7 [21120/29304 (72%)]\tloss: 0.372693\n",
      "train epoch: 7 [21280/29304 (73%)]\tloss: 0.459571\n",
      "train epoch: 7 [21440/29304 (73%)]\tloss: 0.680223\n",
      "train epoch: 7 [21600/29304 (74%)]\tloss: 0.563963\n",
      "train epoch: 7 [21760/29304 (74%)]\tloss: 0.364631\n",
      "train epoch: 7 [21920/29304 (75%)]\tloss: 0.426000\n",
      "train epoch: 7 [22080/29304 (75%)]\tloss: 0.199869\n",
      "train epoch: 7 [22240/29304 (76%)]\tloss: 0.360200\n",
      "train epoch: 7 [22400/29304 (76%)]\tloss: 0.265741\n",
      "train epoch: 7 [22560/29304 (77%)]\tloss: 0.321679\n",
      "train epoch: 7 [22720/29304 (78%)]\tloss: 0.344336\n",
      "train epoch: 7 [22880/29304 (78%)]\tloss: 0.374489\n",
      "train epoch: 7 [23040/29304 (79%)]\tloss: 0.477848\n",
      "train epoch: 7 [23200/29304 (79%)]\tloss: 0.327476\n",
      "train epoch: 7 [23360/29304 (80%)]\tloss: 0.433556\n",
      "train epoch: 7 [23520/29304 (80%)]\tloss: 0.447533\n",
      "train epoch: 7 [23680/29304 (81%)]\tloss: 0.482066\n",
      "train epoch: 7 [23840/29304 (81%)]\tloss: 0.349875\n",
      "train epoch: 7 [24000/29304 (82%)]\tloss: 0.276383\n",
      "train epoch: 7 [24160/29304 (82%)]\tloss: 0.620558\n",
      "train epoch: 7 [24320/29304 (83%)]\tloss: 0.445439\n",
      "train epoch: 7 [24480/29304 (84%)]\tloss: 0.348527\n",
      "train epoch: 7 [24640/29304 (84%)]\tloss: 0.349238\n",
      "train epoch: 7 [24800/29304 (85%)]\tloss: 0.461844\n",
      "train epoch: 7 [24960/29304 (85%)]\tloss: 0.480787\n",
      "train epoch: 7 [25120/29304 (86%)]\tloss: 0.312986\n",
      "train epoch: 7 [25280/29304 (86%)]\tloss: 0.423889\n",
      "train epoch: 7 [25440/29304 (87%)]\tloss: 0.255695\n",
      "train epoch: 7 [25600/29304 (87%)]\tloss: 0.336175\n",
      "train epoch: 7 [25760/29304 (88%)]\tloss: 0.563314\n",
      "train epoch: 7 [25920/29304 (88%)]\tloss: 0.584208\n",
      "train epoch: 7 [26080/29304 (89%)]\tloss: 0.471050\n",
      "train epoch: 7 [26240/29304 (90%)]\tloss: 0.208588\n",
      "train epoch: 7 [26400/29304 (90%)]\tloss: 0.426684\n",
      "train epoch: 7 [26560/29304 (91%)]\tloss: 0.404378\n",
      "train epoch: 7 [26720/29304 (91%)]\tloss: 0.333011\n",
      "train epoch: 7 [26880/29304 (92%)]\tloss: 0.465991\n",
      "train epoch: 7 [27040/29304 (92%)]\tloss: 0.358936\n",
      "train epoch: 7 [27200/29304 (93%)]\tloss: 0.415817\n",
      "train epoch: 7 [27360/29304 (93%)]\tloss: 0.627437\n",
      "train epoch: 7 [27520/29304 (94%)]\tloss: 0.559672\n",
      "train epoch: 7 [27680/29304 (94%)]\tloss: 0.305405\n",
      "train epoch: 7 [27840/29304 (95%)]\tloss: 0.218695\n",
      "train epoch: 7 [28000/29304 (96%)]\tloss: 0.502382\n",
      "train epoch: 7 [28160/29304 (96%)]\tloss: 0.194591\n",
      "train epoch: 7 [28320/29304 (97%)]\tloss: 0.323479\n",
      "train epoch: 7 [28480/29304 (97%)]\tloss: 0.284611\n",
      "train epoch: 7 [28640/29304 (98%)]\tloss: 0.519798\n",
      "train epoch: 7 [28800/29304 (98%)]\tloss: 0.466809\n",
      "train epoch: 7 [28960/29304 (99%)]\tloss: 0.443247\n",
      "train epoch: 7 [29120/29304 (99%)]\tloss: 0.415193\n",
      "train epoch: 7 [29280/29304 (100%)]\tloss: 0.307403\n",
      "\n",
      "Epoch 7:\n",
      "Training loss: 0.4291\n",
      "Test loss: 0.4235\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 8 [    0/29304 (0%)]\tloss: 0.490898\n",
      "train epoch: 8 [  160/29304 (1%)]\tloss: 0.565224\n",
      "train epoch: 8 [  320/29304 (1%)]\tloss: 0.317618\n",
      "train epoch: 8 [  480/29304 (2%)]\tloss: 0.245519\n",
      "train epoch: 8 [  640/29304 (2%)]\tloss: 0.442961\n",
      "train epoch: 8 [  800/29304 (3%)]\tloss: 0.516122\n",
      "train epoch: 8 [  960/29304 (3%)]\tloss: 0.769419\n",
      "train epoch: 8 [ 1120/29304 (4%)]\tloss: 0.468586\n",
      "train epoch: 8 [ 1280/29304 (4%)]\tloss: 0.513187\n",
      "train epoch: 8 [ 1440/29304 (5%)]\tloss: 0.352605\n",
      "train epoch: 8 [ 1600/29304 (5%)]\tloss: 0.262265\n",
      "train epoch: 8 [ 1760/29304 (6%)]\tloss: 0.439628\n",
      "train epoch: 8 [ 1920/29304 (7%)]\tloss: 0.415887\n",
      "train epoch: 8 [ 2080/29304 (7%)]\tloss: 0.398804\n",
      "train epoch: 8 [ 2240/29304 (8%)]\tloss: 0.548984\n",
      "train epoch: 8 [ 2400/29304 (8%)]\tloss: 0.378037\n",
      "train epoch: 8 [ 2560/29304 (9%)]\tloss: 0.367926\n",
      "train epoch: 8 [ 2720/29304 (9%)]\tloss: 0.468714\n",
      "train epoch: 8 [ 2880/29304 (10%)]\tloss: 0.307783\n",
      "train epoch: 8 [ 3040/29304 (10%)]\tloss: 0.360316\n",
      "train epoch: 8 [ 3200/29304 (11%)]\tloss: 0.464063\n",
      "train epoch: 8 [ 3360/29304 (11%)]\tloss: 0.329921\n",
      "train epoch: 8 [ 3520/29304 (12%)]\tloss: 0.649012\n",
      "train epoch: 8 [ 3680/29304 (13%)]\tloss: 0.247701\n",
      "train epoch: 8 [ 3840/29304 (13%)]\tloss: 0.403884\n",
      "train epoch: 8 [ 4000/29304 (14%)]\tloss: 0.413315\n",
      "train epoch: 8 [ 4160/29304 (14%)]\tloss: 0.298827\n",
      "train epoch: 8 [ 4320/29304 (15%)]\tloss: 0.259217\n",
      "train epoch: 8 [ 4480/29304 (15%)]\tloss: 0.320595\n",
      "train epoch: 8 [ 4640/29304 (16%)]\tloss: 0.492112\n",
      "train epoch: 8 [ 4800/29304 (16%)]\tloss: 0.263730\n",
      "train epoch: 8 [ 4960/29304 (17%)]\tloss: 0.522219\n",
      "train epoch: 8 [ 5120/29304 (17%)]\tloss: 0.468172\n",
      "train epoch: 8 [ 5280/29304 (18%)]\tloss: 0.242233\n",
      "train epoch: 8 [ 5440/29304 (19%)]\tloss: 0.448478\n",
      "train epoch: 8 [ 5600/29304 (19%)]\tloss: 0.327674\n",
      "train epoch: 8 [ 5760/29304 (20%)]\tloss: 0.252607\n",
      "train epoch: 8 [ 5920/29304 (20%)]\tloss: 0.400678\n",
      "train epoch: 8 [ 6080/29304 (21%)]\tloss: 0.297672\n",
      "train epoch: 8 [ 6240/29304 (21%)]\tloss: 0.317334\n",
      "train epoch: 8 [ 6400/29304 (22%)]\tloss: 0.344996\n",
      "train epoch: 8 [ 6560/29304 (22%)]\tloss: 0.438343\n",
      "train epoch: 8 [ 6720/29304 (23%)]\tloss: 0.371500\n",
      "train epoch: 8 [ 6880/29304 (23%)]\tloss: 0.339953\n",
      "train epoch: 8 [ 7040/29304 (24%)]\tloss: 0.511814\n",
      "train epoch: 8 [ 7200/29304 (25%)]\tloss: 0.151749\n",
      "train epoch: 8 [ 7360/29304 (25%)]\tloss: 0.459702\n",
      "train epoch: 8 [ 7520/29304 (26%)]\tloss: 0.314553\n",
      "train epoch: 8 [ 7680/29304 (26%)]\tloss: 0.417777\n",
      "train epoch: 8 [ 7840/29304 (27%)]\tloss: 0.455989\n",
      "train epoch: 8 [ 8000/29304 (27%)]\tloss: 0.577283\n",
      "train epoch: 8 [ 8160/29304 (28%)]\tloss: 0.605187\n",
      "train epoch: 8 [ 8320/29304 (28%)]\tloss: 0.334342\n",
      "train epoch: 8 [ 8480/29304 (29%)]\tloss: 0.384698\n",
      "train epoch: 8 [ 8640/29304 (29%)]\tloss: 0.324345\n",
      "train epoch: 8 [ 8800/29304 (30%)]\tloss: 0.385775\n",
      "train epoch: 8 [ 8960/29304 (31%)]\tloss: 0.702019\n",
      "train epoch: 8 [ 9120/29304 (31%)]\tloss: 0.369372\n",
      "train epoch: 8 [ 9280/29304 (32%)]\tloss: 0.556921\n",
      "train epoch: 8 [ 9440/29304 (32%)]\tloss: 0.236671\n",
      "train epoch: 8 [ 9600/29304 (33%)]\tloss: 0.282878\n",
      "train epoch: 8 [ 9760/29304 (33%)]\tloss: 0.443605\n",
      "train epoch: 8 [ 9920/29304 (34%)]\tloss: 0.317990\n",
      "train epoch: 8 [10080/29304 (34%)]\tloss: 0.325467\n",
      "train epoch: 8 [10240/29304 (35%)]\tloss: 0.451704\n",
      "train epoch: 8 [10400/29304 (35%)]\tloss: 0.288547\n",
      "train epoch: 8 [10560/29304 (36%)]\tloss: 0.872096\n",
      "train epoch: 8 [10720/29304 (37%)]\tloss: 0.600138\n",
      "train epoch: 8 [10880/29304 (37%)]\tloss: 0.309371\n",
      "train epoch: 8 [11040/29304 (38%)]\tloss: 0.423824\n",
      "train epoch: 8 [11200/29304 (38%)]\tloss: 0.304775\n",
      "train epoch: 8 [11360/29304 (39%)]\tloss: 0.358681\n",
      "train epoch: 8 [11520/29304 (39%)]\tloss: 0.268888\n",
      "train epoch: 8 [11680/29304 (40%)]\tloss: 0.169069\n",
      "train epoch: 8 [11840/29304 (40%)]\tloss: 0.553611\n",
      "train epoch: 8 [12000/29304 (41%)]\tloss: 0.371531\n",
      "train epoch: 8 [12160/29304 (41%)]\tloss: 0.513923\n",
      "train epoch: 8 [12320/29304 (42%)]\tloss: 0.306581\n",
      "train epoch: 8 [12480/29304 (43%)]\tloss: 0.371992\n",
      "train epoch: 8 [12640/29304 (43%)]\tloss: 0.292148\n",
      "train epoch: 8 [12800/29304 (44%)]\tloss: 0.397860\n",
      "train epoch: 8 [12960/29304 (44%)]\tloss: 0.537586\n",
      "train epoch: 8 [13120/29304 (45%)]\tloss: 0.338692\n",
      "train epoch: 8 [13280/29304 (45%)]\tloss: 0.574317\n",
      "train epoch: 8 [13440/29304 (46%)]\tloss: 0.460540\n",
      "train epoch: 8 [13600/29304 (46%)]\tloss: 0.368735\n",
      "train epoch: 8 [13760/29304 (47%)]\tloss: 0.732856\n",
      "train epoch: 8 [13920/29304 (47%)]\tloss: 0.363564\n",
      "train epoch: 8 [14080/29304 (48%)]\tloss: 0.397086\n",
      "train epoch: 8 [14240/29304 (49%)]\tloss: 0.321227\n",
      "train epoch: 8 [14400/29304 (49%)]\tloss: 0.540219\n",
      "train epoch: 8 [14560/29304 (50%)]\tloss: 0.232026\n",
      "train epoch: 8 [14720/29304 (50%)]\tloss: 0.263044\n",
      "train epoch: 8 [14880/29304 (51%)]\tloss: 0.426181\n",
      "train epoch: 8 [15040/29304 (51%)]\tloss: 0.332366\n",
      "train epoch: 8 [15200/29304 (52%)]\tloss: 0.398869\n",
      "train epoch: 8 [15360/29304 (52%)]\tloss: 0.450041\n",
      "train epoch: 8 [15520/29304 (53%)]\tloss: 0.271381\n",
      "train epoch: 8 [15680/29304 (53%)]\tloss: 0.582390\n",
      "train epoch: 8 [15840/29304 (54%)]\tloss: 0.274733\n",
      "train epoch: 8 [16000/29304 (55%)]\tloss: 0.440999\n",
      "train epoch: 8 [16160/29304 (55%)]\tloss: 0.656201\n",
      "train epoch: 8 [16320/29304 (56%)]\tloss: 0.378848\n",
      "train epoch: 8 [16480/29304 (56%)]\tloss: 0.381657\n",
      "train epoch: 8 [16640/29304 (57%)]\tloss: 0.410906\n",
      "train epoch: 8 [16800/29304 (57%)]\tloss: 0.329035\n",
      "train epoch: 8 [16960/29304 (58%)]\tloss: 0.479386\n",
      "train epoch: 8 [17120/29304 (58%)]\tloss: 0.354577\n",
      "train epoch: 8 [17280/29304 (59%)]\tloss: 0.449665\n",
      "train epoch: 8 [17440/29304 (59%)]\tloss: 0.444002\n",
      "train epoch: 8 [17600/29304 (60%)]\tloss: 0.479415\n",
      "train epoch: 8 [17760/29304 (61%)]\tloss: 0.526448\n",
      "train epoch: 8 [17920/29304 (61%)]\tloss: 0.624888\n",
      "train epoch: 8 [18080/29304 (62%)]\tloss: 0.378886\n",
      "train epoch: 8 [18240/29304 (62%)]\tloss: 0.385345\n",
      "train epoch: 8 [18400/29304 (63%)]\tloss: 0.364635\n",
      "train epoch: 8 [18560/29304 (63%)]\tloss: 0.470394\n",
      "train epoch: 8 [18720/29304 (64%)]\tloss: 0.446835\n",
      "train epoch: 8 [18880/29304 (64%)]\tloss: 0.303307\n",
      "train epoch: 8 [19040/29304 (65%)]\tloss: 0.377349\n",
      "train epoch: 8 [19200/29304 (66%)]\tloss: 0.433227\n",
      "train epoch: 8 [19360/29304 (66%)]\tloss: 0.179970\n",
      "train epoch: 8 [19520/29304 (67%)]\tloss: 0.637688\n",
      "train epoch: 8 [19680/29304 (67%)]\tloss: 0.373712\n",
      "train epoch: 8 [19840/29304 (68%)]\tloss: 0.548329\n",
      "train epoch: 8 [20000/29304 (68%)]\tloss: 0.448576\n",
      "train epoch: 8 [20160/29304 (69%)]\tloss: 0.664287\n",
      "train epoch: 8 [20320/29304 (69%)]\tloss: 0.367643\n",
      "train epoch: 8 [20480/29304 (70%)]\tloss: 0.399897\n",
      "train epoch: 8 [20640/29304 (70%)]\tloss: 0.470591\n",
      "train epoch: 8 [20800/29304 (71%)]\tloss: 0.747974\n",
      "train epoch: 8 [20960/29304 (72%)]\tloss: 0.464552\n",
      "train epoch: 8 [21120/29304 (72%)]\tloss: 0.648432\n",
      "train epoch: 8 [21280/29304 (73%)]\tloss: 0.619091\n",
      "train epoch: 8 [21440/29304 (73%)]\tloss: 0.492597\n",
      "train epoch: 8 [21600/29304 (74%)]\tloss: 0.315805\n",
      "train epoch: 8 [21760/29304 (74%)]\tloss: 0.388199\n",
      "train epoch: 8 [21920/29304 (75%)]\tloss: 0.284561\n",
      "train epoch: 8 [22080/29304 (75%)]\tloss: 0.564404\n",
      "train epoch: 8 [22240/29304 (76%)]\tloss: 0.653104\n",
      "train epoch: 8 [22400/29304 (76%)]\tloss: 0.558137\n",
      "train epoch: 8 [22560/29304 (77%)]\tloss: 0.463242\n",
      "train epoch: 8 [22720/29304 (78%)]\tloss: 0.336049\n",
      "train epoch: 8 [22880/29304 (78%)]\tloss: 0.394347\n",
      "train epoch: 8 [23040/29304 (79%)]\tloss: 0.371186\n",
      "train epoch: 8 [23200/29304 (79%)]\tloss: 0.654328\n",
      "train epoch: 8 [23360/29304 (80%)]\tloss: 0.497790\n",
      "train epoch: 8 [23520/29304 (80%)]\tloss: 0.426768\n",
      "train epoch: 8 [23680/29304 (81%)]\tloss: 0.421821\n",
      "train epoch: 8 [23840/29304 (81%)]\tloss: 0.380445\n",
      "train epoch: 8 [24000/29304 (82%)]\tloss: 0.425511\n",
      "train epoch: 8 [24160/29304 (82%)]\tloss: 0.397025\n",
      "train epoch: 8 [24320/29304 (83%)]\tloss: 0.675706\n",
      "train epoch: 8 [24480/29304 (84%)]\tloss: 0.302949\n",
      "train epoch: 8 [24640/29304 (84%)]\tloss: 0.451161\n",
      "train epoch: 8 [24800/29304 (85%)]\tloss: 0.362893\n",
      "train epoch: 8 [24960/29304 (85%)]\tloss: 0.254469\n",
      "train epoch: 8 [25120/29304 (86%)]\tloss: 0.341695\n",
      "train epoch: 8 [25280/29304 (86%)]\tloss: 0.559104\n",
      "train epoch: 8 [25440/29304 (87%)]\tloss: 0.188657\n",
      "train epoch: 8 [25600/29304 (87%)]\tloss: 0.435489\n",
      "train epoch: 8 [25760/29304 (88%)]\tloss: 0.595346\n",
      "train epoch: 8 [25920/29304 (88%)]\tloss: 0.476825\n",
      "train epoch: 8 [26080/29304 (89%)]\tloss: 0.486110\n",
      "train epoch: 8 [26240/29304 (90%)]\tloss: 0.273734\n",
      "train epoch: 8 [26400/29304 (90%)]\tloss: 0.347754\n",
      "train epoch: 8 [26560/29304 (91%)]\tloss: 0.241380\n",
      "train epoch: 8 [26720/29304 (91%)]\tloss: 0.541254\n",
      "train epoch: 8 [26880/29304 (92%)]\tloss: 0.432569\n",
      "train epoch: 8 [27040/29304 (92%)]\tloss: 0.389626\n",
      "train epoch: 8 [27200/29304 (93%)]\tloss: 0.555978\n",
      "train epoch: 8 [27360/29304 (93%)]\tloss: 0.325353\n",
      "train epoch: 8 [27520/29304 (94%)]\tloss: 0.532311\n",
      "train epoch: 8 [27680/29304 (94%)]\tloss: 0.584302\n",
      "train epoch: 8 [27840/29304 (95%)]\tloss: 0.334795\n",
      "train epoch: 8 [28000/29304 (96%)]\tloss: 0.390228\n",
      "train epoch: 8 [28160/29304 (96%)]\tloss: 0.416504\n",
      "train epoch: 8 [28320/29304 (97%)]\tloss: 0.327314\n",
      "train epoch: 8 [28480/29304 (97%)]\tloss: 0.398323\n",
      "train epoch: 8 [28640/29304 (98%)]\tloss: 0.834085\n",
      "train epoch: 8 [28800/29304 (98%)]\tloss: 0.227024\n",
      "train epoch: 8 [28960/29304 (99%)]\tloss: 0.519219\n",
      "train epoch: 8 [29120/29304 (99%)]\tloss: 0.463971\n",
      "train epoch: 8 [29280/29304 (100%)]\tloss: 0.420926\n",
      "\n",
      "Epoch 8:\n",
      "Training loss: 0.4131\n",
      "Test loss: 0.4091\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 9 [    0/29304 (0%)]\tloss: 0.226756\n",
      "train epoch: 9 [  160/29304 (1%)]\tloss: 0.400910\n",
      "train epoch: 9 [  320/29304 (1%)]\tloss: 0.552486\n",
      "train epoch: 9 [  480/29304 (2%)]\tloss: 0.519091\n",
      "train epoch: 9 [  640/29304 (2%)]\tloss: 0.337931\n",
      "train epoch: 9 [  800/29304 (3%)]\tloss: 0.564035\n",
      "train epoch: 9 [  960/29304 (3%)]\tloss: 0.291088\n",
      "train epoch: 9 [ 1120/29304 (4%)]\tloss: 0.657898\n",
      "train epoch: 9 [ 1280/29304 (4%)]\tloss: 0.209916\n",
      "train epoch: 9 [ 1440/29304 (5%)]\tloss: 0.427396\n",
      "train epoch: 9 [ 1600/29304 (5%)]\tloss: 0.328567\n",
      "train epoch: 9 [ 1760/29304 (6%)]\tloss: 0.385716\n",
      "train epoch: 9 [ 1920/29304 (7%)]\tloss: 0.423594\n",
      "train epoch: 9 [ 2080/29304 (7%)]\tloss: 0.358854\n",
      "train epoch: 9 [ 2240/29304 (8%)]\tloss: 0.493284\n",
      "train epoch: 9 [ 2400/29304 (8%)]\tloss: 0.469224\n",
      "train epoch: 9 [ 2560/29304 (9%)]\tloss: 0.393692\n",
      "train epoch: 9 [ 2720/29304 (9%)]\tloss: 0.350781\n",
      "train epoch: 9 [ 2880/29304 (10%)]\tloss: 0.550834\n",
      "train epoch: 9 [ 3040/29304 (10%)]\tloss: 0.157776\n",
      "train epoch: 9 [ 3200/29304 (11%)]\tloss: 0.636313\n",
      "train epoch: 9 [ 3360/29304 (11%)]\tloss: 0.377696\n",
      "train epoch: 9 [ 3520/29304 (12%)]\tloss: 0.501297\n",
      "train epoch: 9 [ 3680/29304 (13%)]\tloss: 0.390541\n",
      "train epoch: 9 [ 3840/29304 (13%)]\tloss: 0.330644\n",
      "train epoch: 9 [ 4000/29304 (14%)]\tloss: 0.448444\n",
      "train epoch: 9 [ 4160/29304 (14%)]\tloss: 0.497678\n",
      "train epoch: 9 [ 4320/29304 (15%)]\tloss: 0.287959\n",
      "train epoch: 9 [ 4480/29304 (15%)]\tloss: 0.370828\n",
      "train epoch: 9 [ 4640/29304 (16%)]\tloss: 0.355649\n",
      "train epoch: 9 [ 4800/29304 (16%)]\tloss: 0.324159\n",
      "train epoch: 9 [ 4960/29304 (17%)]\tloss: 0.509556\n",
      "train epoch: 9 [ 5120/29304 (17%)]\tloss: 0.384905\n",
      "train epoch: 9 [ 5280/29304 (18%)]\tloss: 0.373724\n",
      "train epoch: 9 [ 5440/29304 (19%)]\tloss: 0.190681\n",
      "train epoch: 9 [ 5600/29304 (19%)]\tloss: 0.253123\n",
      "train epoch: 9 [ 5760/29304 (20%)]\tloss: 0.499181\n",
      "train epoch: 9 [ 5920/29304 (20%)]\tloss: 0.463674\n",
      "train epoch: 9 [ 6080/29304 (21%)]\tloss: 0.403547\n",
      "train epoch: 9 [ 6240/29304 (21%)]\tloss: 0.286822\n",
      "train epoch: 9 [ 6400/29304 (22%)]\tloss: 0.609997\n",
      "train epoch: 9 [ 6560/29304 (22%)]\tloss: 0.504718\n",
      "train epoch: 9 [ 6720/29304 (23%)]\tloss: 0.714014\n",
      "train epoch: 9 [ 6880/29304 (23%)]\tloss: 0.555762\n",
      "train epoch: 9 [ 7040/29304 (24%)]\tloss: 0.327928\n",
      "train epoch: 9 [ 7200/29304 (25%)]\tloss: 0.345586\n",
      "train epoch: 9 [ 7360/29304 (25%)]\tloss: 0.279559\n",
      "train epoch: 9 [ 7520/29304 (26%)]\tloss: 0.439144\n",
      "train epoch: 9 [ 7680/29304 (26%)]\tloss: 0.388943\n",
      "train epoch: 9 [ 7840/29304 (27%)]\tloss: 0.305706\n",
      "train epoch: 9 [ 8000/29304 (27%)]\tloss: 0.595442\n",
      "train epoch: 9 [ 8160/29304 (28%)]\tloss: 0.564716\n",
      "train epoch: 9 [ 8320/29304 (28%)]\tloss: 0.383301\n",
      "train epoch: 9 [ 8480/29304 (29%)]\tloss: 0.414206\n",
      "train epoch: 9 [ 8640/29304 (29%)]\tloss: 0.443948\n",
      "train epoch: 9 [ 8800/29304 (30%)]\tloss: 0.238649\n",
      "train epoch: 9 [ 8960/29304 (31%)]\tloss: 0.235762\n",
      "train epoch: 9 [ 9120/29304 (31%)]\tloss: 0.272220\n",
      "train epoch: 9 [ 9280/29304 (32%)]\tloss: 0.278678\n",
      "train epoch: 9 [ 9440/29304 (32%)]\tloss: 0.320991\n",
      "train epoch: 9 [ 9600/29304 (33%)]\tloss: 0.489272\n",
      "train epoch: 9 [ 9760/29304 (33%)]\tloss: 0.425598\n",
      "train epoch: 9 [ 9920/29304 (34%)]\tloss: 0.419412\n",
      "train epoch: 9 [10080/29304 (34%)]\tloss: 0.259919\n",
      "train epoch: 9 [10240/29304 (35%)]\tloss: 0.286315\n",
      "train epoch: 9 [10400/29304 (35%)]\tloss: 0.135331\n",
      "train epoch: 9 [10560/29304 (36%)]\tloss: 0.449106\n",
      "train epoch: 9 [10720/29304 (37%)]\tloss: 0.548461\n",
      "train epoch: 9 [10880/29304 (37%)]\tloss: 0.519205\n",
      "train epoch: 9 [11040/29304 (38%)]\tloss: 0.490586\n",
      "train epoch: 9 [11200/29304 (38%)]\tloss: 0.337494\n",
      "train epoch: 9 [11360/29304 (39%)]\tloss: 0.491409\n",
      "train epoch: 9 [11520/29304 (39%)]\tloss: 0.562551\n",
      "train epoch: 9 [11680/29304 (40%)]\tloss: 0.479309\n",
      "train epoch: 9 [11840/29304 (40%)]\tloss: 0.501180\n",
      "train epoch: 9 [12000/29304 (41%)]\tloss: 0.306081\n",
      "train epoch: 9 [12160/29304 (41%)]\tloss: 0.378959\n",
      "train epoch: 9 [12320/29304 (42%)]\tloss: 0.467695\n",
      "train epoch: 9 [12480/29304 (43%)]\tloss: 0.501760\n",
      "train epoch: 9 [12640/29304 (43%)]\tloss: 0.442510\n",
      "train epoch: 9 [12800/29304 (44%)]\tloss: 0.462478\n",
      "train epoch: 9 [12960/29304 (44%)]\tloss: 0.364787\n",
      "train epoch: 9 [13120/29304 (45%)]\tloss: 0.375718\n",
      "train epoch: 9 [13280/29304 (45%)]\tloss: 0.405774\n",
      "train epoch: 9 [13440/29304 (46%)]\tloss: 0.302411\n",
      "train epoch: 9 [13600/29304 (46%)]\tloss: 0.525666\n",
      "train epoch: 9 [13760/29304 (47%)]\tloss: 0.390987\n",
      "train epoch: 9 [13920/29304 (47%)]\tloss: 0.245667\n",
      "train epoch: 9 [14080/29304 (48%)]\tloss: 0.267051\n",
      "train epoch: 9 [14240/29304 (49%)]\tloss: 0.470664\n",
      "train epoch: 9 [14400/29304 (49%)]\tloss: 0.330673\n",
      "train epoch: 9 [14560/29304 (50%)]\tloss: 0.471875\n",
      "train epoch: 9 [14720/29304 (50%)]\tloss: 0.352759\n",
      "train epoch: 9 [14880/29304 (51%)]\tloss: 0.301554\n",
      "train epoch: 9 [15040/29304 (51%)]\tloss: 0.362456\n",
      "train epoch: 9 [15200/29304 (52%)]\tloss: 0.260020\n",
      "train epoch: 9 [15360/29304 (52%)]\tloss: 0.723465\n",
      "train epoch: 9 [15520/29304 (53%)]\tloss: 0.344185\n",
      "train epoch: 9 [15680/29304 (53%)]\tloss: 0.357594\n",
      "train epoch: 9 [15840/29304 (54%)]\tloss: 0.329526\n",
      "train epoch: 9 [16000/29304 (55%)]\tloss: 0.357655\n",
      "train epoch: 9 [16160/29304 (55%)]\tloss: 0.189503\n",
      "train epoch: 9 [16320/29304 (56%)]\tloss: 0.492929\n",
      "train epoch: 9 [16480/29304 (56%)]\tloss: 0.367014\n",
      "train epoch: 9 [16640/29304 (57%)]\tloss: 0.381968\n",
      "train epoch: 9 [16800/29304 (57%)]\tloss: 0.219900\n",
      "train epoch: 9 [16960/29304 (58%)]\tloss: 0.461575\n",
      "train epoch: 9 [17120/29304 (58%)]\tloss: 0.301179\n",
      "train epoch: 9 [17280/29304 (59%)]\tloss: 0.331591\n",
      "train epoch: 9 [17440/29304 (59%)]\tloss: 0.515977\n",
      "train epoch: 9 [17600/29304 (60%)]\tloss: 0.299781\n",
      "train epoch: 9 [17760/29304 (61%)]\tloss: 0.370308\n",
      "train epoch: 9 [17920/29304 (61%)]\tloss: 0.549302\n",
      "train epoch: 9 [18080/29304 (62%)]\tloss: 0.406357\n",
      "train epoch: 9 [18240/29304 (62%)]\tloss: 0.536351\n",
      "train epoch: 9 [18400/29304 (63%)]\tloss: 0.420811\n",
      "train epoch: 9 [18560/29304 (63%)]\tloss: 0.301346\n",
      "train epoch: 9 [18720/29304 (64%)]\tloss: 0.397189\n",
      "train epoch: 9 [18880/29304 (64%)]\tloss: 0.264849\n",
      "train epoch: 9 [19040/29304 (65%)]\tloss: 0.568560\n",
      "train epoch: 9 [19200/29304 (66%)]\tloss: 0.368834\n",
      "train epoch: 9 [19360/29304 (66%)]\tloss: 0.193076\n",
      "train epoch: 9 [19520/29304 (67%)]\tloss: 0.324696\n",
      "train epoch: 9 [19680/29304 (67%)]\tloss: 0.249007\n",
      "train epoch: 9 [19840/29304 (68%)]\tloss: 0.537293\n",
      "train epoch: 9 [20000/29304 (68%)]\tloss: 0.434056\n",
      "train epoch: 9 [20160/29304 (69%)]\tloss: 0.318467\n",
      "train epoch: 9 [20320/29304 (69%)]\tloss: 0.415138\n",
      "train epoch: 9 [20480/29304 (70%)]\tloss: 0.261620\n",
      "train epoch: 9 [20640/29304 (70%)]\tloss: 0.162931\n",
      "train epoch: 9 [20800/29304 (71%)]\tloss: 0.333873\n",
      "train epoch: 9 [20960/29304 (72%)]\tloss: 0.382834\n",
      "train epoch: 9 [21120/29304 (72%)]\tloss: 0.390070\n",
      "train epoch: 9 [21280/29304 (73%)]\tloss: 0.594754\n",
      "train epoch: 9 [21440/29304 (73%)]\tloss: 0.396000\n",
      "train epoch: 9 [21600/29304 (74%)]\tloss: 0.497572\n",
      "train epoch: 9 [21760/29304 (74%)]\tloss: 0.331167\n",
      "train epoch: 9 [21920/29304 (75%)]\tloss: 0.421387\n",
      "train epoch: 9 [22080/29304 (75%)]\tloss: 0.604705\n",
      "train epoch: 9 [22240/29304 (76%)]\tloss: 0.478606\n",
      "train epoch: 9 [22400/29304 (76%)]\tloss: 0.306415\n",
      "train epoch: 9 [22560/29304 (77%)]\tloss: 0.463738\n",
      "train epoch: 9 [22720/29304 (78%)]\tloss: 0.473360\n",
      "train epoch: 9 [22880/29304 (78%)]\tloss: 0.465990\n",
      "train epoch: 9 [23040/29304 (79%)]\tloss: 0.358679\n",
      "train epoch: 9 [23200/29304 (79%)]\tloss: 0.246011\n",
      "train epoch: 9 [23360/29304 (80%)]\tloss: 0.397590\n",
      "train epoch: 9 [23520/29304 (80%)]\tloss: 0.418891\n",
      "train epoch: 9 [23680/29304 (81%)]\tloss: 0.355579\n",
      "train epoch: 9 [23840/29304 (81%)]\tloss: 0.404694\n",
      "train epoch: 9 [24000/29304 (82%)]\tloss: 0.452462\n",
      "train epoch: 9 [24160/29304 (82%)]\tloss: 0.413031\n",
      "train epoch: 9 [24320/29304 (83%)]\tloss: 0.519130\n",
      "train epoch: 9 [24480/29304 (84%)]\tloss: 0.400280\n",
      "train epoch: 9 [24640/29304 (84%)]\tloss: 0.234628\n",
      "train epoch: 9 [24800/29304 (85%)]\tloss: 0.525259\n",
      "train epoch: 9 [24960/29304 (85%)]\tloss: 0.237848\n",
      "train epoch: 9 [25120/29304 (86%)]\tloss: 0.465524\n",
      "train epoch: 9 [25280/29304 (86%)]\tloss: 0.414443\n",
      "train epoch: 9 [25440/29304 (87%)]\tloss: 0.267749\n",
      "train epoch: 9 [25600/29304 (87%)]\tloss: 0.329907\n",
      "train epoch: 9 [25760/29304 (88%)]\tloss: 0.416662\n",
      "train epoch: 9 [25920/29304 (88%)]\tloss: 0.258091\n",
      "train epoch: 9 [26080/29304 (89%)]\tloss: 0.306094\n",
      "train epoch: 9 [26240/29304 (90%)]\tloss: 0.447289\n",
      "train epoch: 9 [26400/29304 (90%)]\tloss: 0.521471\n",
      "train epoch: 9 [26560/29304 (91%)]\tloss: 0.490332\n",
      "train epoch: 9 [26720/29304 (91%)]\tloss: 0.394668\n",
      "train epoch: 9 [26880/29304 (92%)]\tloss: 0.475394\n",
      "train epoch: 9 [27040/29304 (92%)]\tloss: 0.417147\n",
      "train epoch: 9 [27200/29304 (93%)]\tloss: 0.332974\n",
      "train epoch: 9 [27360/29304 (93%)]\tloss: 0.600187\n",
      "train epoch: 9 [27520/29304 (94%)]\tloss: 0.565698\n",
      "train epoch: 9 [27680/29304 (94%)]\tloss: 0.401399\n",
      "train epoch: 9 [27840/29304 (95%)]\tloss: 0.467275\n",
      "train epoch: 9 [28000/29304 (96%)]\tloss: 0.512071\n",
      "train epoch: 9 [28160/29304 (96%)]\tloss: 0.508576\n",
      "train epoch: 9 [28320/29304 (97%)]\tloss: 0.422026\n",
      "train epoch: 9 [28480/29304 (97%)]\tloss: 0.550003\n",
      "train epoch: 9 [28640/29304 (98%)]\tloss: 0.635741\n",
      "train epoch: 9 [28800/29304 (98%)]\tloss: 0.392493\n",
      "train epoch: 9 [28960/29304 (99%)]\tloss: 0.330919\n",
      "train epoch: 9 [29120/29304 (99%)]\tloss: 0.467596\n",
      "train epoch: 9 [29280/29304 (100%)]\tloss: 0.330762\n",
      "\n",
      "Epoch 9:\n",
      "Training loss: 0.4006\n",
      "Test loss: 0.3977\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 10 [    0/29304 (0%)]\tloss: 0.513402\n",
      "train epoch: 10 [  160/29304 (1%)]\tloss: 0.488340\n",
      "train epoch: 10 [  320/29304 (1%)]\tloss: 0.427350\n",
      "train epoch: 10 [  480/29304 (2%)]\tloss: 0.376357\n",
      "train epoch: 10 [  640/29304 (2%)]\tloss: 0.392561\n",
      "train epoch: 10 [  800/29304 (3%)]\tloss: 0.285135\n",
      "train epoch: 10 [  960/29304 (3%)]\tloss: 0.271969\n",
      "train epoch: 10 [ 1120/29304 (4%)]\tloss: 0.655620\n",
      "train epoch: 10 [ 1280/29304 (4%)]\tloss: 0.511764\n",
      "train epoch: 10 [ 1440/29304 (5%)]\tloss: 0.452434\n",
      "train epoch: 10 [ 1600/29304 (5%)]\tloss: 0.525304\n",
      "train epoch: 10 [ 1760/29304 (6%)]\tloss: 0.345809\n",
      "train epoch: 10 [ 1920/29304 (7%)]\tloss: 0.723281\n",
      "train epoch: 10 [ 2080/29304 (7%)]\tloss: 0.288055\n",
      "train epoch: 10 [ 2240/29304 (8%)]\tloss: 0.387514\n",
      "train epoch: 10 [ 2400/29304 (8%)]\tloss: 0.332603\n",
      "train epoch: 10 [ 2560/29304 (9%)]\tloss: 0.467291\n",
      "train epoch: 10 [ 2720/29304 (9%)]\tloss: 0.209079\n",
      "train epoch: 10 [ 2880/29304 (10%)]\tloss: 0.621671\n",
      "train epoch: 10 [ 3040/29304 (10%)]\tloss: 0.183747\n",
      "train epoch: 10 [ 3200/29304 (11%)]\tloss: 0.290693\n",
      "train epoch: 10 [ 3360/29304 (11%)]\tloss: 0.276882\n",
      "train epoch: 10 [ 3520/29304 (12%)]\tloss: 0.471883\n",
      "train epoch: 10 [ 3680/29304 (13%)]\tloss: 0.435793\n",
      "train epoch: 10 [ 3840/29304 (13%)]\tloss: 0.213865\n",
      "train epoch: 10 [ 4000/29304 (14%)]\tloss: 0.356946\n",
      "train epoch: 10 [ 4160/29304 (14%)]\tloss: 0.468369\n",
      "train epoch: 10 [ 4320/29304 (15%)]\tloss: 0.275513\n",
      "train epoch: 10 [ 4480/29304 (15%)]\tloss: 0.470666\n",
      "train epoch: 10 [ 4640/29304 (16%)]\tloss: 0.204436\n",
      "train epoch: 10 [ 4800/29304 (16%)]\tloss: 0.644223\n",
      "train epoch: 10 [ 4960/29304 (17%)]\tloss: 0.384359\n",
      "train epoch: 10 [ 5120/29304 (17%)]\tloss: 0.481997\n",
      "train epoch: 10 [ 5280/29304 (18%)]\tloss: 0.453511\n",
      "train epoch: 10 [ 5440/29304 (19%)]\tloss: 0.361841\n",
      "train epoch: 10 [ 5600/29304 (19%)]\tloss: 0.285812\n",
      "train epoch: 10 [ 5760/29304 (20%)]\tloss: 0.355646\n",
      "train epoch: 10 [ 5920/29304 (20%)]\tloss: 0.631470\n",
      "train epoch: 10 [ 6080/29304 (21%)]\tloss: 0.284599\n",
      "train epoch: 10 [ 6240/29304 (21%)]\tloss: 0.214663\n",
      "train epoch: 10 [ 6400/29304 (22%)]\tloss: 0.526781\n",
      "train epoch: 10 [ 6560/29304 (22%)]\tloss: 0.352703\n",
      "train epoch: 10 [ 6720/29304 (23%)]\tloss: 0.490694\n",
      "train epoch: 10 [ 6880/29304 (23%)]\tloss: 0.529315\n",
      "train epoch: 10 [ 7040/29304 (24%)]\tloss: 0.366285\n",
      "train epoch: 10 [ 7200/29304 (25%)]\tloss: 0.434988\n",
      "train epoch: 10 [ 7360/29304 (25%)]\tloss: 0.309879\n",
      "train epoch: 10 [ 7520/29304 (26%)]\tloss: 0.452877\n",
      "train epoch: 10 [ 7680/29304 (26%)]\tloss: 0.420182\n",
      "train epoch: 10 [ 7840/29304 (27%)]\tloss: 0.499539\n",
      "train epoch: 10 [ 8000/29304 (27%)]\tloss: 0.352642\n",
      "train epoch: 10 [ 8160/29304 (28%)]\tloss: 0.405016\n",
      "train epoch: 10 [ 8320/29304 (28%)]\tloss: 0.452262\n",
      "train epoch: 10 [ 8480/29304 (29%)]\tloss: 0.244123\n",
      "train epoch: 10 [ 8640/29304 (29%)]\tloss: 0.381145\n",
      "train epoch: 10 [ 8800/29304 (30%)]\tloss: 0.415300\n",
      "train epoch: 10 [ 8960/29304 (31%)]\tloss: 0.462754\n",
      "train epoch: 10 [ 9120/29304 (31%)]\tloss: 0.570222\n",
      "train epoch: 10 [ 9280/29304 (32%)]\tloss: 0.288690\n",
      "train epoch: 10 [ 9440/29304 (32%)]\tloss: 0.337982\n",
      "train epoch: 10 [ 9600/29304 (33%)]\tloss: 0.341492\n",
      "train epoch: 10 [ 9760/29304 (33%)]\tloss: 0.344617\n",
      "train epoch: 10 [ 9920/29304 (34%)]\tloss: 0.567232\n",
      "train epoch: 10 [10080/29304 (34%)]\tloss: 0.410732\n",
      "train epoch: 10 [10240/29304 (35%)]\tloss: 0.497563\n",
      "train epoch: 10 [10400/29304 (35%)]\tloss: 0.389343\n",
      "train epoch: 10 [10560/29304 (36%)]\tloss: 0.404472\n",
      "train epoch: 10 [10720/29304 (37%)]\tloss: 0.645802\n",
      "train epoch: 10 [10880/29304 (37%)]\tloss: 0.453178\n",
      "train epoch: 10 [11040/29304 (38%)]\tloss: 0.236410\n",
      "train epoch: 10 [11200/29304 (38%)]\tloss: 0.289784\n",
      "train epoch: 10 [11360/29304 (39%)]\tloss: 0.370855\n",
      "train epoch: 10 [11520/29304 (39%)]\tloss: 0.409443\n",
      "train epoch: 10 [11680/29304 (40%)]\tloss: 0.288277\n",
      "train epoch: 10 [11840/29304 (40%)]\tloss: 0.308391\n",
      "train epoch: 10 [12000/29304 (41%)]\tloss: 0.374272\n",
      "train epoch: 10 [12160/29304 (41%)]\tloss: 0.544101\n",
      "train epoch: 10 [12320/29304 (42%)]\tloss: 0.277021\n",
      "train epoch: 10 [12480/29304 (43%)]\tloss: 0.598128\n",
      "train epoch: 10 [12640/29304 (43%)]\tloss: 0.689918\n",
      "train epoch: 10 [12800/29304 (44%)]\tloss: 0.513487\n",
      "train epoch: 10 [12960/29304 (44%)]\tloss: 0.312761\n",
      "train epoch: 10 [13120/29304 (45%)]\tloss: 0.435076\n",
      "train epoch: 10 [13280/29304 (45%)]\tloss: 0.273245\n",
      "train epoch: 10 [13440/29304 (46%)]\tloss: 0.279234\n",
      "train epoch: 10 [13600/29304 (46%)]\tloss: 0.138787\n",
      "train epoch: 10 [13760/29304 (47%)]\tloss: 0.363672\n",
      "train epoch: 10 [13920/29304 (47%)]\tloss: 0.426238\n",
      "train epoch: 10 [14080/29304 (48%)]\tloss: 0.490567\n",
      "train epoch: 10 [14240/29304 (49%)]\tloss: 0.591548\n",
      "train epoch: 10 [14400/29304 (49%)]\tloss: 0.284130\n",
      "train epoch: 10 [14560/29304 (50%)]\tloss: 0.248987\n",
      "train epoch: 10 [14720/29304 (50%)]\tloss: 0.401456\n",
      "train epoch: 10 [14880/29304 (51%)]\tloss: 0.299387\n",
      "train epoch: 10 [15040/29304 (51%)]\tloss: 0.345855\n",
      "train epoch: 10 [15200/29304 (52%)]\tloss: 0.285921\n",
      "train epoch: 10 [15360/29304 (52%)]\tloss: 0.305203\n",
      "train epoch: 10 [15520/29304 (53%)]\tloss: 0.417301\n",
      "train epoch: 10 [15680/29304 (53%)]\tloss: 0.271120\n",
      "train epoch: 10 [15840/29304 (54%)]\tloss: 0.628374\n",
      "train epoch: 10 [16000/29304 (55%)]\tloss: 0.357325\n",
      "train epoch: 10 [16160/29304 (55%)]\tloss: 0.478048\n",
      "train epoch: 10 [16320/29304 (56%)]\tloss: 0.438052\n",
      "train epoch: 10 [16480/29304 (56%)]\tloss: 0.427054\n",
      "train epoch: 10 [16640/29304 (57%)]\tloss: 0.325625\n",
      "train epoch: 10 [16800/29304 (57%)]\tloss: 0.339869\n",
      "train epoch: 10 [16960/29304 (58%)]\tloss: 0.344691\n",
      "train epoch: 10 [17120/29304 (58%)]\tloss: 0.472388\n",
      "train epoch: 10 [17280/29304 (59%)]\tloss: 0.338760\n",
      "train epoch: 10 [17440/29304 (59%)]\tloss: 0.471420\n",
      "train epoch: 10 [17600/29304 (60%)]\tloss: 0.408151\n",
      "train epoch: 10 [17760/29304 (61%)]\tloss: 0.384696\n",
      "train epoch: 10 [17920/29304 (61%)]\tloss: 0.605808\n",
      "train epoch: 10 [18080/29304 (62%)]\tloss: 0.279176\n",
      "train epoch: 10 [18240/29304 (62%)]\tloss: 0.310835\n",
      "train epoch: 10 [18400/29304 (63%)]\tloss: 0.431511\n",
      "train epoch: 10 [18560/29304 (63%)]\tloss: 0.193176\n",
      "train epoch: 10 [18720/29304 (64%)]\tloss: 0.283445\n",
      "train epoch: 10 [18880/29304 (64%)]\tloss: 0.316356\n",
      "train epoch: 10 [19040/29304 (65%)]\tloss: 0.310485\n",
      "train epoch: 10 [19200/29304 (66%)]\tloss: 0.600706\n",
      "train epoch: 10 [19360/29304 (66%)]\tloss: 0.308253\n",
      "train epoch: 10 [19520/29304 (67%)]\tloss: 0.432362\n",
      "train epoch: 10 [19680/29304 (67%)]\tloss: 0.491379\n",
      "train epoch: 10 [19840/29304 (68%)]\tloss: 0.327702\n",
      "train epoch: 10 [20000/29304 (68%)]\tloss: 0.313365\n",
      "train epoch: 10 [20160/29304 (69%)]\tloss: 0.273949\n",
      "train epoch: 10 [20320/29304 (69%)]\tloss: 0.342945\n",
      "train epoch: 10 [20480/29304 (70%)]\tloss: 0.793310\n",
      "train epoch: 10 [20640/29304 (70%)]\tloss: 0.310012\n",
      "train epoch: 10 [20800/29304 (71%)]\tloss: 0.538147\n",
      "train epoch: 10 [20960/29304 (72%)]\tloss: 0.422512\n",
      "train epoch: 10 [21120/29304 (72%)]\tloss: 0.403651\n",
      "train epoch: 10 [21280/29304 (73%)]\tloss: 0.388528\n",
      "train epoch: 10 [21440/29304 (73%)]\tloss: 0.525302\n",
      "train epoch: 10 [21600/29304 (74%)]\tloss: 0.459325\n",
      "train epoch: 10 [21760/29304 (74%)]\tloss: 0.266127\n",
      "train epoch: 10 [21920/29304 (75%)]\tloss: 0.415753\n",
      "train epoch: 10 [22080/29304 (75%)]\tloss: 0.418045\n",
      "train epoch: 10 [22240/29304 (76%)]\tloss: 0.357325\n",
      "train epoch: 10 [22400/29304 (76%)]\tloss: 0.230958\n",
      "train epoch: 10 [22560/29304 (77%)]\tloss: 0.350583\n",
      "train epoch: 10 [22720/29304 (78%)]\tloss: 0.387472\n",
      "train epoch: 10 [22880/29304 (78%)]\tloss: 0.562082\n",
      "train epoch: 10 [23040/29304 (79%)]\tloss: 0.367867\n",
      "train epoch: 10 [23200/29304 (79%)]\tloss: 0.674506\n",
      "train epoch: 10 [23360/29304 (80%)]\tloss: 0.523199\n",
      "train epoch: 10 [23520/29304 (80%)]\tloss: 0.454954\n",
      "train epoch: 10 [23680/29304 (81%)]\tloss: 0.349648\n",
      "train epoch: 10 [23840/29304 (81%)]\tloss: 0.358332\n",
      "train epoch: 10 [24000/29304 (82%)]\tloss: 0.235110\n",
      "train epoch: 10 [24160/29304 (82%)]\tloss: 0.717438\n",
      "train epoch: 10 [24320/29304 (83%)]\tloss: 0.448764\n",
      "train epoch: 10 [24480/29304 (84%)]\tloss: 0.352227\n",
      "train epoch: 10 [24640/29304 (84%)]\tloss: 0.663924\n",
      "train epoch: 10 [24800/29304 (85%)]\tloss: 0.378421\n",
      "train epoch: 10 [24960/29304 (85%)]\tloss: 0.323326\n",
      "train epoch: 10 [25120/29304 (86%)]\tloss: 0.580321\n",
      "train epoch: 10 [25280/29304 (86%)]\tloss: 0.335356\n",
      "train epoch: 10 [25440/29304 (87%)]\tloss: 0.408162\n",
      "train epoch: 10 [25600/29304 (87%)]\tloss: 0.265781\n",
      "train epoch: 10 [25760/29304 (88%)]\tloss: 0.320335\n",
      "train epoch: 10 [25920/29304 (88%)]\tloss: 0.363450\n",
      "train epoch: 10 [26080/29304 (89%)]\tloss: 0.517873\n",
      "train epoch: 10 [26240/29304 (90%)]\tloss: 0.128617\n",
      "train epoch: 10 [26400/29304 (90%)]\tloss: 0.363671\n",
      "train epoch: 10 [26560/29304 (91%)]\tloss: 0.291467\n",
      "train epoch: 10 [26720/29304 (91%)]\tloss: 0.490223\n",
      "train epoch: 10 [26880/29304 (92%)]\tloss: 0.403829\n",
      "train epoch: 10 [27040/29304 (92%)]\tloss: 0.503914\n",
      "train epoch: 10 [27200/29304 (93%)]\tloss: 0.399934\n",
      "train epoch: 10 [27360/29304 (93%)]\tloss: 0.356510\n",
      "train epoch: 10 [27520/29304 (94%)]\tloss: 0.227891\n",
      "train epoch: 10 [27680/29304 (94%)]\tloss: 0.236787\n",
      "train epoch: 10 [27840/29304 (95%)]\tloss: 0.358064\n",
      "train epoch: 10 [28000/29304 (96%)]\tloss: 0.235180\n",
      "train epoch: 10 [28160/29304 (96%)]\tloss: 0.362292\n",
      "train epoch: 10 [28320/29304 (97%)]\tloss: 0.389489\n",
      "train epoch: 10 [28480/29304 (97%)]\tloss: 0.442650\n",
      "train epoch: 10 [28640/29304 (98%)]\tloss: 0.357779\n",
      "train epoch: 10 [28800/29304 (98%)]\tloss: 0.319773\n",
      "train epoch: 10 [28960/29304 (99%)]\tloss: 0.528020\n",
      "train epoch: 10 [29120/29304 (99%)]\tloss: 0.399077\n",
      "train epoch: 10 [29280/29304 (100%)]\tloss: 0.365519\n",
      "\n",
      "Epoch 10:\n",
      "Training loss: 0.3908\n",
      "Test loss: 0.3885\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 11 [    0/29304 (0%)]\tloss: 0.321298\n",
      "train epoch: 11 [  160/29304 (1%)]\tloss: 0.307832\n",
      "train epoch: 11 [  320/29304 (1%)]\tloss: 0.401721\n",
      "train epoch: 11 [  480/29304 (2%)]\tloss: 0.498724\n",
      "train epoch: 11 [  640/29304 (2%)]\tloss: 0.393641\n",
      "train epoch: 11 [  800/29304 (3%)]\tloss: 0.266734\n",
      "train epoch: 11 [  960/29304 (3%)]\tloss: 0.335012\n",
      "train epoch: 11 [ 1120/29304 (4%)]\tloss: 0.622457\n",
      "train epoch: 11 [ 1280/29304 (4%)]\tloss: 0.291470\n",
      "train epoch: 11 [ 1440/29304 (5%)]\tloss: 0.333029\n",
      "train epoch: 11 [ 1600/29304 (5%)]\tloss: 0.466397\n",
      "train epoch: 11 [ 1760/29304 (6%)]\tloss: 0.312147\n",
      "train epoch: 11 [ 1920/29304 (7%)]\tloss: 0.304447\n",
      "train epoch: 11 [ 2080/29304 (7%)]\tloss: 0.450558\n",
      "train epoch: 11 [ 2240/29304 (8%)]\tloss: 0.383415\n",
      "train epoch: 11 [ 2400/29304 (8%)]\tloss: 0.800965\n",
      "train epoch: 11 [ 2560/29304 (9%)]\tloss: 0.577771\n",
      "train epoch: 11 [ 2720/29304 (9%)]\tloss: 0.338035\n",
      "train epoch: 11 [ 2880/29304 (10%)]\tloss: 0.278837\n",
      "train epoch: 11 [ 3040/29304 (10%)]\tloss: 0.343107\n",
      "train epoch: 11 [ 3200/29304 (11%)]\tloss: 0.445185\n",
      "train epoch: 11 [ 3360/29304 (11%)]\tloss: 0.225558\n",
      "train epoch: 11 [ 3520/29304 (12%)]\tloss: 0.402972\n",
      "train epoch: 11 [ 3680/29304 (13%)]\tloss: 0.391593\n",
      "train epoch: 11 [ 3840/29304 (13%)]\tloss: 0.362839\n",
      "train epoch: 11 [ 4000/29304 (14%)]\tloss: 0.220025\n",
      "train epoch: 11 [ 4160/29304 (14%)]\tloss: 0.351589\n",
      "train epoch: 11 [ 4320/29304 (15%)]\tloss: 0.264792\n",
      "train epoch: 11 [ 4480/29304 (15%)]\tloss: 0.350341\n",
      "train epoch: 11 [ 4640/29304 (16%)]\tloss: 0.482308\n",
      "train epoch: 11 [ 4800/29304 (16%)]\tloss: 0.442930\n",
      "train epoch: 11 [ 4960/29304 (17%)]\tloss: 0.342365\n",
      "train epoch: 11 [ 5120/29304 (17%)]\tloss: 0.248381\n",
      "train epoch: 11 [ 5280/29304 (18%)]\tloss: 0.329514\n",
      "train epoch: 11 [ 5440/29304 (19%)]\tloss: 0.513887\n",
      "train epoch: 11 [ 5600/29304 (19%)]\tloss: 0.264128\n",
      "train epoch: 11 [ 5760/29304 (20%)]\tloss: 0.364724\n",
      "train epoch: 11 [ 5920/29304 (20%)]\tloss: 0.156181\n",
      "train epoch: 11 [ 6080/29304 (21%)]\tloss: 0.222608\n",
      "train epoch: 11 [ 6240/29304 (21%)]\tloss: 0.278428\n",
      "train epoch: 11 [ 6400/29304 (22%)]\tloss: 0.469859\n",
      "train epoch: 11 [ 6560/29304 (22%)]\tloss: 0.513509\n",
      "train epoch: 11 [ 6720/29304 (23%)]\tloss: 0.494796\n",
      "train epoch: 11 [ 6880/29304 (23%)]\tloss: 0.264980\n",
      "train epoch: 11 [ 7040/29304 (24%)]\tloss: 0.375695\n",
      "train epoch: 11 [ 7200/29304 (25%)]\tloss: 0.358341\n",
      "train epoch: 11 [ 7360/29304 (25%)]\tloss: 0.249549\n",
      "train epoch: 11 [ 7520/29304 (26%)]\tloss: 0.487636\n",
      "train epoch: 11 [ 7680/29304 (26%)]\tloss: 0.338241\n",
      "train epoch: 11 [ 7840/29304 (27%)]\tloss: 0.420624\n",
      "train epoch: 11 [ 8000/29304 (27%)]\tloss: 0.291929\n",
      "train epoch: 11 [ 8160/29304 (28%)]\tloss: 0.236893\n",
      "train epoch: 11 [ 8320/29304 (28%)]\tloss: 0.332832\n",
      "train epoch: 11 [ 8480/29304 (29%)]\tloss: 0.358476\n",
      "train epoch: 11 [ 8640/29304 (29%)]\tloss: 0.448402\n",
      "train epoch: 11 [ 8800/29304 (30%)]\tloss: 0.389771\n",
      "train epoch: 11 [ 8960/29304 (31%)]\tloss: 0.280670\n",
      "train epoch: 11 [ 9120/29304 (31%)]\tloss: 0.562907\n",
      "train epoch: 11 [ 9280/29304 (32%)]\tloss: 0.392882\n",
      "train epoch: 11 [ 9440/29304 (32%)]\tloss: 0.395333\n",
      "train epoch: 11 [ 9600/29304 (33%)]\tloss: 0.375935\n",
      "train epoch: 11 [ 9760/29304 (33%)]\tloss: 0.437043\n",
      "train epoch: 11 [ 9920/29304 (34%)]\tloss: 0.426328\n",
      "train epoch: 11 [10080/29304 (34%)]\tloss: 0.508260\n",
      "train epoch: 11 [10240/29304 (35%)]\tloss: 0.509674\n",
      "train epoch: 11 [10400/29304 (35%)]\tloss: 0.572412\n",
      "train epoch: 11 [10560/29304 (36%)]\tloss: 0.393938\n",
      "train epoch: 11 [10720/29304 (37%)]\tloss: 0.538064\n",
      "train epoch: 11 [10880/29304 (37%)]\tloss: 0.264601\n",
      "train epoch: 11 [11040/29304 (38%)]\tloss: 0.377429\n",
      "train epoch: 11 [11200/29304 (38%)]\tloss: 0.311307\n",
      "train epoch: 11 [11360/29304 (39%)]\tloss: 0.403011\n",
      "train epoch: 11 [11520/29304 (39%)]\tloss: 0.355925\n",
      "train epoch: 11 [11680/29304 (40%)]\tloss: 0.487504\n",
      "train epoch: 11 [11840/29304 (40%)]\tloss: 0.362960\n",
      "train epoch: 11 [12000/29304 (41%)]\tloss: 0.305313\n",
      "train epoch: 11 [12160/29304 (41%)]\tloss: 0.315926\n",
      "train epoch: 11 [12320/29304 (42%)]\tloss: 0.472742\n",
      "train epoch: 11 [12480/29304 (43%)]\tloss: 0.303680\n",
      "train epoch: 11 [12640/29304 (43%)]\tloss: 0.337889\n",
      "train epoch: 11 [12800/29304 (44%)]\tloss: 0.248611\n",
      "train epoch: 11 [12960/29304 (44%)]\tloss: 0.459223\n",
      "train epoch: 11 [13120/29304 (45%)]\tloss: 0.324062\n",
      "train epoch: 11 [13280/29304 (45%)]\tloss: 0.429086\n",
      "train epoch: 11 [13440/29304 (46%)]\tloss: 0.450401\n",
      "train epoch: 11 [13600/29304 (46%)]\tloss: 0.275291\n",
      "train epoch: 11 [13760/29304 (47%)]\tloss: 0.387212\n",
      "train epoch: 11 [13920/29304 (47%)]\tloss: 0.358861\n",
      "train epoch: 11 [14080/29304 (48%)]\tloss: 0.380647\n",
      "train epoch: 11 [14240/29304 (49%)]\tloss: 0.526195\n",
      "train epoch: 11 [14400/29304 (49%)]\tloss: 0.209036\n",
      "train epoch: 11 [14560/29304 (50%)]\tloss: 0.244755\n",
      "train epoch: 11 [14720/29304 (50%)]\tloss: 0.331538\n",
      "train epoch: 11 [14880/29304 (51%)]\tloss: 0.362012\n",
      "train epoch: 11 [15040/29304 (51%)]\tloss: 0.282242\n",
      "train epoch: 11 [15200/29304 (52%)]\tloss: 0.451870\n",
      "train epoch: 11 [15360/29304 (52%)]\tloss: 0.241818\n",
      "train epoch: 11 [15520/29304 (53%)]\tloss: 0.300875\n",
      "train epoch: 11 [15680/29304 (53%)]\tloss: 0.440766\n",
      "train epoch: 11 [15840/29304 (54%)]\tloss: 0.485105\n",
      "train epoch: 11 [16000/29304 (55%)]\tloss: 0.581068\n",
      "train epoch: 11 [16160/29304 (55%)]\tloss: 0.336843\n",
      "train epoch: 11 [16320/29304 (56%)]\tloss: 0.367054\n",
      "train epoch: 11 [16480/29304 (56%)]\tloss: 0.356965\n",
      "train epoch: 11 [16640/29304 (57%)]\tloss: 0.361019\n",
      "train epoch: 11 [16800/29304 (57%)]\tloss: 0.352391\n",
      "train epoch: 11 [16960/29304 (58%)]\tloss: 0.508438\n",
      "train epoch: 11 [17120/29304 (58%)]\tloss: 0.340806\n",
      "train epoch: 11 [17280/29304 (59%)]\tloss: 0.376984\n",
      "train epoch: 11 [17440/29304 (59%)]\tloss: 0.282249\n",
      "train epoch: 11 [17600/29304 (60%)]\tloss: 0.375165\n",
      "train epoch: 11 [17760/29304 (61%)]\tloss: 0.375764\n",
      "train epoch: 11 [17920/29304 (61%)]\tloss: 0.432784\n",
      "train epoch: 11 [18080/29304 (62%)]\tloss: 0.295815\n",
      "train epoch: 11 [18240/29304 (62%)]\tloss: 0.459547\n",
      "train epoch: 11 [18400/29304 (63%)]\tloss: 0.481958\n",
      "train epoch: 11 [18560/29304 (63%)]\tloss: 0.492473\n",
      "train epoch: 11 [18720/29304 (64%)]\tloss: 0.414153\n",
      "train epoch: 11 [18880/29304 (64%)]\tloss: 0.441888\n",
      "train epoch: 11 [19040/29304 (65%)]\tloss: 0.385849\n",
      "train epoch: 11 [19200/29304 (66%)]\tloss: 0.482830\n",
      "train epoch: 11 [19360/29304 (66%)]\tloss: 0.401931\n",
      "train epoch: 11 [19520/29304 (67%)]\tloss: 0.347596\n",
      "train epoch: 11 [19680/29304 (67%)]\tloss: 0.147217\n",
      "train epoch: 11 [19840/29304 (68%)]\tloss: 0.541863\n",
      "train epoch: 11 [20000/29304 (68%)]\tloss: 0.452848\n",
      "train epoch: 11 [20160/29304 (69%)]\tloss: 0.545790\n",
      "train epoch: 11 [20320/29304 (69%)]\tloss: 0.369537\n",
      "train epoch: 11 [20480/29304 (70%)]\tloss: 0.506477\n",
      "train epoch: 11 [20640/29304 (70%)]\tloss: 0.410231\n",
      "train epoch: 11 [20800/29304 (71%)]\tloss: 0.493018\n",
      "train epoch: 11 [20960/29304 (72%)]\tloss: 0.632862\n",
      "train epoch: 11 [21120/29304 (72%)]\tloss: 0.291443\n",
      "train epoch: 11 [21280/29304 (73%)]\tloss: 0.434843\n",
      "train epoch: 11 [21440/29304 (73%)]\tloss: 0.327592\n",
      "train epoch: 11 [21600/29304 (74%)]\tloss: 0.258566\n",
      "train epoch: 11 [21760/29304 (74%)]\tloss: 0.409766\n",
      "train epoch: 11 [21920/29304 (75%)]\tloss: 0.328122\n",
      "train epoch: 11 [22080/29304 (75%)]\tloss: 0.438466\n",
      "train epoch: 11 [22240/29304 (76%)]\tloss: 0.228857\n",
      "train epoch: 11 [22400/29304 (76%)]\tloss: 0.565391\n",
      "train epoch: 11 [22560/29304 (77%)]\tloss: 0.365584\n",
      "train epoch: 11 [22720/29304 (78%)]\tloss: 0.269805\n",
      "train epoch: 11 [22880/29304 (78%)]\tloss: 0.429006\n",
      "train epoch: 11 [23040/29304 (79%)]\tloss: 0.361195\n",
      "train epoch: 11 [23200/29304 (79%)]\tloss: 0.446919\n",
      "train epoch: 11 [23360/29304 (80%)]\tloss: 0.682335\n",
      "train epoch: 11 [23520/29304 (80%)]\tloss: 0.398932\n",
      "train epoch: 11 [23680/29304 (81%)]\tloss: 0.469172\n",
      "train epoch: 11 [23840/29304 (81%)]\tloss: 0.669458\n",
      "train epoch: 11 [24000/29304 (82%)]\tloss: 0.384720\n",
      "train epoch: 11 [24160/29304 (82%)]\tloss: 0.264786\n",
      "train epoch: 11 [24320/29304 (83%)]\tloss: 0.361184\n",
      "train epoch: 11 [24480/29304 (84%)]\tloss: 0.246597\n",
      "train epoch: 11 [24640/29304 (84%)]\tloss: 0.215038\n",
      "train epoch: 11 [24800/29304 (85%)]\tloss: 0.438419\n",
      "train epoch: 11 [24960/29304 (85%)]\tloss: 0.469406\n",
      "train epoch: 11 [25120/29304 (86%)]\tloss: 0.486682\n",
      "train epoch: 11 [25280/29304 (86%)]\tloss: 0.586800\n",
      "train epoch: 11 [25440/29304 (87%)]\tloss: 0.706246\n",
      "train epoch: 11 [25600/29304 (87%)]\tloss: 0.357970\n",
      "train epoch: 11 [25760/29304 (88%)]\tloss: 0.416083\n",
      "train epoch: 11 [25920/29304 (88%)]\tloss: 0.521426\n",
      "train epoch: 11 [26080/29304 (89%)]\tloss: 0.509871\n",
      "train epoch: 11 [26240/29304 (90%)]\tloss: 0.281846\n",
      "train epoch: 11 [26400/29304 (90%)]\tloss: 0.585333\n",
      "train epoch: 11 [26560/29304 (91%)]\tloss: 0.337455\n",
      "train epoch: 11 [26720/29304 (91%)]\tloss: 0.280674\n",
      "train epoch: 11 [26880/29304 (92%)]\tloss: 0.275377\n",
      "train epoch: 11 [27040/29304 (92%)]\tloss: 0.295194\n",
      "train epoch: 11 [27200/29304 (93%)]\tloss: 0.329345\n",
      "train epoch: 11 [27360/29304 (93%)]\tloss: 0.345671\n",
      "train epoch: 11 [27520/29304 (94%)]\tloss: 0.353622\n",
      "train epoch: 11 [27680/29304 (94%)]\tloss: 0.157352\n",
      "train epoch: 11 [27840/29304 (95%)]\tloss: 0.352576\n",
      "train epoch: 11 [28000/29304 (96%)]\tloss: 0.662831\n",
      "train epoch: 11 [28160/29304 (96%)]\tloss: 0.327559\n",
      "train epoch: 11 [28320/29304 (97%)]\tloss: 0.239765\n",
      "train epoch: 11 [28480/29304 (97%)]\tloss: 0.507933\n",
      "train epoch: 11 [28640/29304 (98%)]\tloss: 0.369514\n",
      "train epoch: 11 [28800/29304 (98%)]\tloss: 0.388550\n",
      "train epoch: 11 [28960/29304 (99%)]\tloss: 0.295171\n",
      "train epoch: 11 [29120/29304 (99%)]\tloss: 0.271623\n",
      "train epoch: 11 [29280/29304 (100%)]\tloss: 0.381430\n",
      "\n",
      "Epoch 11:\n",
      "Training loss: 0.3826\n",
      "Test loss: 0.3808\n",
      "Accuracy: 2456/3257 (75%)\n",
      "\n",
      "train epoch: 12 [    0/29304 (0%)]\tloss: 0.240895\n",
      "train epoch: 12 [  160/29304 (1%)]\tloss: 0.335412\n",
      "train epoch: 12 [  320/29304 (1%)]\tloss: 0.303313\n",
      "train epoch: 12 [  480/29304 (2%)]\tloss: 0.243401\n",
      "train epoch: 12 [  640/29304 (2%)]\tloss: 0.244334\n",
      "train epoch: 12 [  800/29304 (3%)]\tloss: 0.572844\n",
      "train epoch: 12 [  960/29304 (3%)]\tloss: 0.384071\n",
      "train epoch: 12 [ 1120/29304 (4%)]\tloss: 0.261652\n",
      "train epoch: 12 [ 1280/29304 (4%)]\tloss: 0.347578\n",
      "train epoch: 12 [ 1440/29304 (5%)]\tloss: 0.481334\n",
      "train epoch: 12 [ 1600/29304 (5%)]\tloss: 0.317707\n",
      "train epoch: 12 [ 1760/29304 (6%)]\tloss: 0.556764\n",
      "train epoch: 12 [ 1920/29304 (7%)]\tloss: 0.480302\n",
      "train epoch: 12 [ 2080/29304 (7%)]\tloss: 0.429025\n",
      "train epoch: 12 [ 2240/29304 (8%)]\tloss: 0.385708\n",
      "train epoch: 12 [ 2400/29304 (8%)]\tloss: 0.249809\n",
      "train epoch: 12 [ 2560/29304 (9%)]\tloss: 0.392269\n",
      "train epoch: 12 [ 2720/29304 (9%)]\tloss: 0.430149\n",
      "train epoch: 12 [ 2880/29304 (10%)]\tloss: 0.292810\n",
      "train epoch: 12 [ 3040/29304 (10%)]\tloss: 0.133665\n",
      "train epoch: 12 [ 3200/29304 (11%)]\tloss: 0.306666\n",
      "train epoch: 12 [ 3360/29304 (11%)]\tloss: 0.372367\n",
      "train epoch: 12 [ 3520/29304 (12%)]\tloss: 0.398610\n",
      "train epoch: 12 [ 3680/29304 (13%)]\tloss: 0.256238\n",
      "train epoch: 12 [ 3840/29304 (13%)]\tloss: 0.230656\n",
      "train epoch: 12 [ 4000/29304 (14%)]\tloss: 0.374484\n",
      "train epoch: 12 [ 4160/29304 (14%)]\tloss: 0.302185\n",
      "train epoch: 12 [ 4320/29304 (15%)]\tloss: 0.239004\n",
      "train epoch: 12 [ 4480/29304 (15%)]\tloss: 0.223544\n",
      "train epoch: 12 [ 4640/29304 (16%)]\tloss: 0.417478\n",
      "train epoch: 12 [ 4800/29304 (16%)]\tloss: 0.508943\n",
      "train epoch: 12 [ 4960/29304 (17%)]\tloss: 0.132163\n",
      "train epoch: 12 [ 5120/29304 (17%)]\tloss: 0.236163\n",
      "train epoch: 12 [ 5280/29304 (18%)]\tloss: 0.464549\n",
      "train epoch: 12 [ 5440/29304 (19%)]\tloss: 0.246977\n",
      "train epoch: 12 [ 5600/29304 (19%)]\tloss: 0.689404\n",
      "train epoch: 12 [ 5760/29304 (20%)]\tloss: 0.372569\n",
      "train epoch: 12 [ 5920/29304 (20%)]\tloss: 0.277357\n",
      "train epoch: 12 [ 6080/29304 (21%)]\tloss: 0.182117\n",
      "train epoch: 12 [ 6240/29304 (21%)]\tloss: 0.393664\n",
      "train epoch: 12 [ 6400/29304 (22%)]\tloss: 0.249342\n",
      "train epoch: 12 [ 6560/29304 (22%)]\tloss: 0.348848\n",
      "train epoch: 12 [ 6720/29304 (23%)]\tloss: 0.188995\n",
      "train epoch: 12 [ 6880/29304 (23%)]\tloss: 0.237504\n",
      "train epoch: 12 [ 7040/29304 (24%)]\tloss: 0.559446\n",
      "train epoch: 12 [ 7200/29304 (25%)]\tloss: 0.197535\n",
      "train epoch: 12 [ 7360/29304 (25%)]\tloss: 0.295142\n",
      "train epoch: 12 [ 7520/29304 (26%)]\tloss: 0.182421\n",
      "train epoch: 12 [ 7680/29304 (26%)]\tloss: 0.564399\n",
      "train epoch: 12 [ 7840/29304 (27%)]\tloss: 0.364280\n",
      "train epoch: 12 [ 8000/29304 (27%)]\tloss: 0.391597\n",
      "train epoch: 12 [ 8160/29304 (28%)]\tloss: 0.273805\n",
      "train epoch: 12 [ 8320/29304 (28%)]\tloss: 0.446355\n",
      "train epoch: 12 [ 8480/29304 (29%)]\tloss: 0.417771\n",
      "train epoch: 12 [ 8640/29304 (29%)]\tloss: 0.287729\n",
      "train epoch: 12 [ 8800/29304 (30%)]\tloss: 0.442145\n",
      "train epoch: 12 [ 8960/29304 (31%)]\tloss: 0.527554\n",
      "train epoch: 12 [ 9120/29304 (31%)]\tloss: 0.322668\n",
      "train epoch: 12 [ 9280/29304 (32%)]\tloss: 0.432340\n",
      "train epoch: 12 [ 9440/29304 (32%)]\tloss: 0.466555\n",
      "train epoch: 12 [ 9600/29304 (33%)]\tloss: 0.344915\n",
      "train epoch: 12 [ 9760/29304 (33%)]\tloss: 0.248377\n",
      "train epoch: 12 [ 9920/29304 (34%)]\tloss: 0.349484\n",
      "train epoch: 12 [10080/29304 (34%)]\tloss: 0.507997\n",
      "train epoch: 12 [10240/29304 (35%)]\tloss: 0.474130\n",
      "train epoch: 12 [10400/29304 (35%)]\tloss: 0.480800\n",
      "train epoch: 12 [10560/29304 (36%)]\tloss: 0.534435\n",
      "train epoch: 12 [10720/29304 (37%)]\tloss: 0.408542\n",
      "train epoch: 12 [10880/29304 (37%)]\tloss: 0.339567\n",
      "train epoch: 12 [11040/29304 (38%)]\tloss: 0.435114\n",
      "train epoch: 12 [11200/29304 (38%)]\tloss: 0.299128\n",
      "train epoch: 12 [11360/29304 (39%)]\tloss: 0.240019\n",
      "train epoch: 12 [11520/29304 (39%)]\tloss: 0.402797\n",
      "train epoch: 12 [11680/29304 (40%)]\tloss: 0.276006\n",
      "train epoch: 12 [11840/29304 (40%)]\tloss: 0.490627\n",
      "train epoch: 12 [12000/29304 (41%)]\tloss: 0.346637\n",
      "train epoch: 12 [12160/29304 (41%)]\tloss: 0.346826\n",
      "train epoch: 12 [12320/29304 (42%)]\tloss: 0.344049\n",
      "train epoch: 12 [12480/29304 (43%)]\tloss: 0.270735\n",
      "train epoch: 12 [12640/29304 (43%)]\tloss: 0.380122\n",
      "train epoch: 12 [12800/29304 (44%)]\tloss: 0.431426\n",
      "train epoch: 12 [12960/29304 (44%)]\tloss: 0.363254\n",
      "train epoch: 12 [13120/29304 (45%)]\tloss: 0.386747\n",
      "train epoch: 12 [13280/29304 (45%)]\tloss: 0.342584\n",
      "train epoch: 12 [13440/29304 (46%)]\tloss: 0.523492\n",
      "train epoch: 12 [13600/29304 (46%)]\tloss: 0.454564\n",
      "train epoch: 12 [13760/29304 (47%)]\tloss: 0.258870\n",
      "train epoch: 12 [13920/29304 (47%)]\tloss: 0.312208\n",
      "train epoch: 12 [14080/29304 (48%)]\tloss: 0.603215\n",
      "train epoch: 12 [14240/29304 (49%)]\tloss: 0.287822\n",
      "train epoch: 12 [14400/29304 (49%)]\tloss: 0.496093\n",
      "train epoch: 12 [14560/29304 (50%)]\tloss: 0.185273\n",
      "train epoch: 12 [14720/29304 (50%)]\tloss: 0.434915\n",
      "train epoch: 12 [14880/29304 (51%)]\tloss: 0.286908\n",
      "train epoch: 12 [15040/29304 (51%)]\tloss: 0.326635\n",
      "train epoch: 12 [15200/29304 (52%)]\tloss: 0.249000\n",
      "train epoch: 12 [15360/29304 (52%)]\tloss: 0.445615\n",
      "train epoch: 12 [15520/29304 (53%)]\tloss: 0.270534\n",
      "train epoch: 12 [15680/29304 (53%)]\tloss: 0.332131\n",
      "train epoch: 12 [15840/29304 (54%)]\tloss: 0.176764\n",
      "train epoch: 12 [16000/29304 (55%)]\tloss: 0.437856\n",
      "train epoch: 12 [16160/29304 (55%)]\tloss: 0.516023\n",
      "train epoch: 12 [16320/29304 (56%)]\tloss: 0.288850\n",
      "train epoch: 12 [16480/29304 (56%)]\tloss: 0.522087\n",
      "train epoch: 12 [16640/29304 (57%)]\tloss: 0.158365\n",
      "train epoch: 12 [16800/29304 (57%)]\tloss: 0.515034\n",
      "train epoch: 12 [16960/29304 (58%)]\tloss: 0.415946\n",
      "train epoch: 12 [17120/29304 (58%)]\tloss: 0.257089\n",
      "train epoch: 12 [17280/29304 (59%)]\tloss: 0.470962\n",
      "train epoch: 12 [17440/29304 (59%)]\tloss: 0.357412\n",
      "train epoch: 12 [17600/29304 (60%)]\tloss: 0.515895\n",
      "train epoch: 12 [17760/29304 (61%)]\tloss: 0.435337\n",
      "train epoch: 12 [17920/29304 (61%)]\tloss: 0.283678\n",
      "train epoch: 12 [18080/29304 (62%)]\tloss: 0.565597\n",
      "train epoch: 12 [18240/29304 (62%)]\tloss: 0.343195\n",
      "train epoch: 12 [18400/29304 (63%)]\tloss: 0.447575\n",
      "train epoch: 12 [18560/29304 (63%)]\tloss: 0.434200\n",
      "train epoch: 12 [18720/29304 (64%)]\tloss: 0.427564\n",
      "train epoch: 12 [18880/29304 (64%)]\tloss: 0.285055\n",
      "train epoch: 12 [19040/29304 (65%)]\tloss: 0.292063\n",
      "train epoch: 12 [19200/29304 (66%)]\tloss: 0.423871\n",
      "train epoch: 12 [19360/29304 (66%)]\tloss: 0.287941\n",
      "train epoch: 12 [19520/29304 (67%)]\tloss: 0.405309\n",
      "train epoch: 12 [19680/29304 (67%)]\tloss: 0.415587\n",
      "train epoch: 12 [19840/29304 (68%)]\tloss: 0.337119\n",
      "train epoch: 12 [20000/29304 (68%)]\tloss: 0.423690\n",
      "train epoch: 12 [20160/29304 (69%)]\tloss: 0.427366\n",
      "train epoch: 12 [20320/29304 (69%)]\tloss: 0.357815\n",
      "train epoch: 12 [20480/29304 (70%)]\tloss: 0.564816\n",
      "train epoch: 12 [20640/29304 (70%)]\tloss: 0.135829\n",
      "train epoch: 12 [20800/29304 (71%)]\tloss: 0.503316\n",
      "train epoch: 12 [20960/29304 (72%)]\tloss: 0.302057\n",
      "train epoch: 12 [21120/29304 (72%)]\tloss: 0.268106\n",
      "train epoch: 12 [21280/29304 (73%)]\tloss: 0.296633\n",
      "train epoch: 12 [21440/29304 (73%)]\tloss: 0.212513\n",
      "train epoch: 12 [21600/29304 (74%)]\tloss: 0.285521\n",
      "train epoch: 12 [21760/29304 (74%)]\tloss: 0.578090\n",
      "train epoch: 12 [21920/29304 (75%)]\tloss: 0.230261\n",
      "train epoch: 12 [22080/29304 (75%)]\tloss: 0.697685\n",
      "train epoch: 12 [22240/29304 (76%)]\tloss: 0.318054\n",
      "train epoch: 12 [22400/29304 (76%)]\tloss: 0.678174\n",
      "train epoch: 12 [22560/29304 (77%)]\tloss: 0.354764\n",
      "train epoch: 12 [22720/29304 (78%)]\tloss: 0.391643\n",
      "train epoch: 12 [22880/29304 (78%)]\tloss: 0.513757\n",
      "train epoch: 12 [23040/29304 (79%)]\tloss: 0.168878\n",
      "train epoch: 12 [23200/29304 (79%)]\tloss: 0.402280\n",
      "train epoch: 12 [23360/29304 (80%)]\tloss: 0.156134\n",
      "train epoch: 12 [23520/29304 (80%)]\tloss: 0.491269\n",
      "train epoch: 12 [23680/29304 (81%)]\tloss: 0.257563\n",
      "train epoch: 12 [23840/29304 (81%)]\tloss: 0.220798\n",
      "train epoch: 12 [24000/29304 (82%)]\tloss: 0.269465\n",
      "train epoch: 12 [24160/29304 (82%)]\tloss: 0.130249\n",
      "train epoch: 12 [24320/29304 (83%)]\tloss: 0.364534\n",
      "train epoch: 12 [24480/29304 (84%)]\tloss: 0.401562\n",
      "train epoch: 12 [24640/29304 (84%)]\tloss: 0.311506\n",
      "train epoch: 12 [24800/29304 (85%)]\tloss: 0.277447\n",
      "train epoch: 12 [24960/29304 (85%)]\tloss: 0.342512\n",
      "train epoch: 12 [25120/29304 (86%)]\tloss: 0.331386\n",
      "train epoch: 12 [25280/29304 (86%)]\tloss: 0.317125\n",
      "train epoch: 12 [25440/29304 (87%)]\tloss: 0.395466\n",
      "train epoch: 12 [25600/29304 (87%)]\tloss: 0.509177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\n\u001b[1;32m     21\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_set[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     22\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[9], line 115\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self, training_set, test_set, num_epochs, gamma)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# training loop\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, end_epoch):\n\u001b[0;32m--> 115\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(epoch)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_epoch \u001b[38;5;241m=\u001b[39m epoch\n",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m, in \u001b[0;36mModelTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     57\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data)\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(output, target)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/projects/deeplearning/venv/lib/python3.10/site-packages/torch/_tensor.py:571\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    527\u001b[0m ):\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_torch_function_unary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    572\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m             Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m             (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m             inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m         )\n\u001b[1;32m    581\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Simple Pyramid structure, gradually decrease width\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# initialize trainer\n",
    "trainer = ModelTrainer(\n",
    "    input_size=len(training_set[0][0]),\n",
    "    device=device,\n",
    "    batch_size=16,\n",
    "    learning_rate=0.001,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# train model\n",
    "trained_model = trainer.train(\n",
    "    training_set=training_set,\n",
    "    test_set=test_set,\n",
    "    num_epochs=200\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
