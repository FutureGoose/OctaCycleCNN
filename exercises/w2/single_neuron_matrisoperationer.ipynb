{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Neuron calculations using proper matrix operations\n",
    "\n",
    "In this lecture, we explore how a single neuron processes data using matrix multiplication. \n",
    "\n",
    "---\n",
    "\n",
    "### A Single Neuron with Single and Multiple Training Samples\n",
    "In this lecture, we explore how a single neuron processes data using matrix multiplication. \n",
    "\n",
    "We'll start with a single training sample and later expand to multiple samples, explicitly including the bias as a vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Part 1: Inputs, Weights, and Bias for a single training sample**\n",
    "\n",
    "1. *Inputs*: Denoted as $ x_1, x_2, \\dots, x_n $ for $ n $ features, represented as a vector $ \\mathbf{x} \\in \\mathbb{R}^n $:\n",
    "   $$\n",
    "   \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. *Weights*: Denoted as $ w_1, w_2, \\dots, w_n $, represented as a vector $ \\mathbf{w} \\in \\mathbb{R}^n $:\n",
    "   $$\n",
    "   \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. *Bias*: A single scalar $ b \\in \\mathbb{R} $:\n",
    "   $$\n",
    "   b\n",
    "   $$\n",
    "\n",
    "The neuron then computes the **sum**:\n",
    "$$\n",
    "z = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "\n",
    "Finally, an **activation function** $ \\sigma $ is applied:\n",
    "$$\n",
    "a = \\sigma(z)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum (z): -3.5\n",
      "Activated output (a): 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Single training sample\n",
    "\n",
    "x = np.array([2.0, 3.0])  # inputs with n=2 features\n",
    "w = np.array([0.5, -0.2]) # weights\n",
    "b = -3.9                   # bias\n",
    "\n",
    "# Sum calculation\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "print(\"sum (z):\", z)\n",
    "\n",
    "# ReLU activation function\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Activated output\n",
    "\n",
    "a = relu(z)\n",
    "print(\"Activated output (a):\", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary for Single Training Sample\n",
    "1. The neuron computes:\n",
    "   - sum: $ z = \\mathbf{w}^\\top \\mathbf{x} + b $,\n",
    "   - Activation: $ a = \\sigma(z) $.\n",
    "2. Weights $ \\mathbf{w} $ and inputs $ \\mathbf{x} $ are vectors, while the bias b is a scalar represented as a 1-dimensional vector.\n",
    "3. Next, we generalize to **multiple training samples**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Multiple Training Samples**\n",
    "\n",
    "When there are $ m $ training samples, each with $ n $ features:\n",
    "1. **Inputs**: Represented as a matrix $ \\mathbf{X} \\in \\mathbb{R}^{m \\times n} $:\n",
    "   $$\n",
    "   \\mathbf{X} = \\begin{bmatrix}\n",
    "   x_{1,1} & x_{1,2} & \\dots & x_{1,n} \\\\\n",
    "   x_{2,1} & x_{2,2} & \\dots & x_{2,n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   x_{m,1} & x_{m,2} & \\dots & x_{m,n}\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **Weights**: Same as before, $ \\mathbf{w} \\in \\mathbb{R}^n $:\n",
    "   $$\n",
    "   \\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Bias**: A vector $ \\mathbf{b} \\in \\mathbb{R}^m $, where each entry is equal to the bias scalar:\n",
    "   $$\n",
    "   \\mathbf{b} = \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "The neuron computes the sum:\n",
    "$$\n",
    "\\mathbf{z} = \\mathbf{X} \\mathbf{w} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Finally, apply the activation function element-wise:\n",
    "$$\n",
    "\\mathbf{a} = \\sigma(\\mathbf{z})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear output (z)     : [1.1 1.7 2.3]\n",
      "Activated output (a)  : [0.75026011 0.84553473 0.90887704]\n"
     ]
    }
   ],
   "source": [
    "# Multiple training samples (m=3, n=2)\n",
    "\n",
    "X = np.array([[1.0, 2.0],      # First sample\n",
    "              [3.0, 4.0],      # Second sample\n",
    "              [5.0, 6.0]])     # Third sample (m=3, n=2)\n",
    "\n",
    "w = np.array([0.5, -0.2])      # Weights \n",
    "\n",
    "b = np.array([1.0, 1.0, 1.0])  # Bias vector \n",
    "\n",
    "# Sigmoid activation function\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Linear output\n",
    "\n",
    "z = np.dot(X, w) + b\n",
    "print(\"Linear output (z)     :\", z)\n",
    "\n",
    "# Activated output\n",
    "\n",
    "a = sigmoid(z)\n",
    "print(\"Activated output (a)  :\", a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary for Multiple Training Samples\n",
    "1. The neuron computes:\n",
    "   - The sums: $ \\mathbf{z} = \\mathbf{X} \\mathbf{w} + \\mathbf{b} $,\n",
    "   - Activation: $ \\mathbf{a} = \\sigma(\\mathbf{z}) $.\n",
    "2. **Key Difference**:\n",
    "   - Inputs $ \\mathbf{X} $: A matrix with all training samples.\n",
    "   - Bias $ \\mathbf{b} $: Explicitly represented as a vector, matching the number of training samples $ m $.\n",
    "3. This efficient matrix formulation allows processing of multiple samples in parallel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "1. For a **single training sample**, the neuron performs:\n",
    "   - Weighted sum: $ z = \\mathbf{w}^\\top \\mathbf{x} + b $,\n",
    "   - Activation: $ a = \\sigma(z) $.\n",
    "\n",
    "2. For **multiple training samples**, the computation scales to matrix operations:\n",
    "   - Linear output: $ \\mathbf{z} = \\mathbf{X} \\mathbf{w} + \\mathbf{b} $,\n",
    "   - Activation: $ \\mathbf{a} = \\sigma(\\mathbf{z}) $.\n",
    "\n",
    "This framework is the foundation of neural network operations and scales naturally to larger architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
